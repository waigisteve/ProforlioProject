{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCgSJL6kg65GR9zkx8Jxhq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waigisteve/ProforlioProject/blob/main/python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"its that time again\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QspLkQsquURo",
        "outputId": "cd608f10-aeca-43ce-d451-7f23cc8b1801"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "its that time again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mhW10otguI_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x=14"
      ],
      "metadata": {
        "id": "lxz9YapBvThF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CA8iFk0suDeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCO3-i2mveEO",
        "outputId": "64339aa1-d43d-4374-a337-59e3bae32c86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y=34\n",
        "sum =x+y\n",
        "print(sum)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQL25I_xv5lF",
        "outputId": "01da5ebb-2b9e-4bcb-e431-010bbf730167"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: A script that prints the average of three subjects  scores Maths, Science , Geography  add an extra 5 marks for each subject and generate the answer\n",
        "\n",
        "# Scores for each subject\n",
        "maths = 70\n",
        "science = 80\n",
        "geography = 90\n",
        "\n",
        "# Adding bonus marks\n",
        "maths += 5\n",
        "science += 5\n",
        "geography += 5\n",
        "\n",
        "# Calculate the total score\n",
        "total_score = maths + science + geography\n",
        "\n",
        "# Calculate the average score\n",
        "average_score = total_score / 3\n",
        "\n",
        "# Print the average score\n",
        "print(\"The average score is:\", average_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXklWEZ4wYtl",
        "outputId": "84a02704-4de2-4b87-eeff-25756eb9f906"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "its that time again\n",
            "14\n",
            "48\n",
            "The average score is: 82.33333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maths = \"good marks\"\n",
        "science =\"excellent\"\n",
        "total =maths + science\n",
        "print(total)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYNYOB021k2q",
        "outputId": "13d6bee7-2e26-4b4c-d606-02520bfac142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "good marksexcellent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "science =100\n",
        "maths =\"120\"\n",
        "total =int(maths) + science\n",
        "print(total)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIizEu6k5jxy",
        "outputId": "5a502cbe-387a-465b-8d52-bbdc8d4544ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "220\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maths = int(input(\"enter maths marks\"))\n",
        "science = int(input(\"enter science marks\"))\n",
        "total =maths + science\n",
        "total_marks = maths + science\n",
        "print(total/200*100)\n",
        "\n",
        "if total >89:\n",
        "  print(\"A\")\n",
        "elif total >79:\n",
        "  print(\"B\")\n",
        "elif total >69:\n",
        "  print(\"C\")\n",
        "elif total >59:\n",
        "  print(\"D\")\n",
        "else:\n",
        "  print(\"F\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9hhTD2k8GX-",
        "outputId": "183c3249-626e-4697-a02f-c0b25f982eaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter maths marks43\n",
            "enter science marks23\n",
            "33.0\n",
            "D\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "from os import PRIO_PROCESS\n",
        "Movie Ticket Pricing\n",
        "Scenario: a cinema has different  ticket prices based on age.\n",
        "Task: Write a program that asks for the customers age adn prints the ticket PRIO_PROCESS\n",
        "\n",
        "child (0-12): 200\n",
        "Teen (13-17): 400\n",
        "Adult (18-64): 600\n",
        "Senior (65 and above): 800"
      ],
      "metadata": {
        "id": "mN8lG5uiCGtx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LYsy5jLHCJ3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "age = int(input(\"Enter your age: \"))\n",
        "\n",
        "if age <= 12:\n",
        "    price = 200\n",
        "    category = \"Child\"\n",
        "elif age <= 17:\n",
        "    price = 400\n",
        "    category = \"Teen\"\n",
        "elif age <= 64:\n",
        "    price = 600\n",
        "    category = \"Adult\"\n",
        "else:\n",
        "    price = 800\n",
        "    category = \"Senior\"\n",
        "\n",
        "print(f\"Ticket price for {category}: {price}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WA8I9FfpCLFa",
        "outputId": "06a5aa21-c562-4900-d1ca-32c338a32c04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your age: 8\n",
            "Ticket price for Child: 200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "a country has a progressive tax system based on annual income. create a program that calculates tax based on the following brackets up to 350000 no tax 351000 to 500000 5%\n",
        "501,000 to 1000000 10% above 1000000 20% tax"
      ],
      "metadata": {
        "id": "YO72BmxxGQ2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_tax(income):\n",
        "    if income <= 350000:\n",
        "        tax = 0\n",
        "        tax_percent = 0\n",
        "    elif income <= 500000:\n",
        "        tax = (income - 350000) * 0.05\n",
        "        tax_percent = 5\n",
        "    elif income <= 1000000:\n",
        "        tax = (150000 * 0.05) + (income - 500000) * 0.10\n",
        "        tax_percent = (tax / income) * 100  # Calculate effective tax percentage\n",
        "    else:\n",
        "        tax = (150000 * 0.05) + (500000 * 0.10) + (income - 1000000) * 0.20\n",
        "        tax_percent = (tax / income) * 100  # Calculate effective tax percentage\n",
        "\n",
        "    return tax, tax_percent\n",
        "\n",
        "income = float(input(\"Enter your annual income: \"))\n",
        "tax_amount, tax_percentage = calculate_tax(income)\n",
        "\n",
        "print(f\"Tax payable on income of ${income:,.2f} is:\")\n",
        "print(f\"- Amount: ${tax_amount:,.2f}\")\n",
        "print(f\"- Effective Tax Percentage: {tax_percentage:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVAGYH_XGZvW",
        "outputId": "c95ec28f-5b88-477e-898f-67e14cff4d0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your annual income: 1300000\n",
            "Tax payable on income of $1,300,000.00 is:\n",
            "- Amount: $117,500.00\n",
            "- Effective Tax Percentage: 9.04%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YXaFwkTzKpLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "546PymiJKsi8",
        "outputId": "87fae08c-469a-4b6d-a3ec-b669a43a7a6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your exam score: 45\n",
            "Admission unlikely\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Advanced Loan Approval System\n",
        "\n",
        "Scenario: A bank has a complex loan approval process based on multiple factors.\n",
        "Task: Create a program that determines loan approval and interest rate based on:\n",
        "\n",
        "Credit Score (Excellent: 800+, Good: 700-799, Fair: 600-699, Poor: below 600)\n",
        "Annual Income (High: 1,000,000+, Medium: 500,000-999,999, Low: below 500,000)\n",
        "Debt-to-Income Ratio (Low: below 20%, Medium: 20-36%, High: above 36%)\n",
        "Loan Amount (Small: below 1,000,000, Medium: 1,000,000-5,000,000, Large: above 5,000,000)\n",
        "\n",
        "Rules:\n",
        "\n",
        "Approve loan if Credit Score is Good or Excellent, and Debt-to-Income Ratio is Low or Medium\n",
        "Reject loan if Credit Score is Poor, or if Debt-to-Income Ratio is High\n",
        "For Fair Credit Score, approve only if Income is High and Debt-to-Income Ratio is Low\n",
        "Interest rates:\n",
        "\n",
        "8% for Excellent Credit, Low Debt-to-Income, and High Income\n",
        "10% for Good Credit or (Fair Credit with High Income and Low Debt-to-Income)\n",
        "12% for all other approved loans"
      ],
      "metadata": {
        "id": "iPId-LNyLNK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def assess_loan(credit_score, income, dti):\n",
        "    approved = False\n",
        "    interest_rate = None\n",
        "\n",
        "    if credit_score >= 800:\n",
        "        if income >= 1000000 and dti < 0.20:\n",
        "            approved = True\n",
        "            interest_rate = 0.03  # Excellent credit, high income, low DTI\n",
        "        elif income >= 500000 and dti < 0.36:\n",
        "            approved = True\n",
        "            interest_rate = 0.04  # Excellent credit, medium income, reasonable DTI\n",
        "        elif dti < 0.20:\n",
        "            approved = True\n",
        "            interest_rate = 0.05  # Excellent credit, but lower income or higher DTI\n",
        "    elif credit_score >= 700:\n",
        "        if income >= 500000 and dti < 0.20:\n",
        "            approved = True\n",
        "            interest_rate = 0.06  # Good credit, medium income, low DTI\n",
        "        elif dti < 0.30:\n",
        "            approved = True\n",
        "            interest_rate = 0.07  # Good credit, but lower income or higher DTI\n",
        "    elif credit_score >= 600:\n",
        "        if income >= 750000 and dti < 0.20:\n",
        "            approved = True\n",
        "            interest_rate = 0.08  # Fair credit, high income, low DTI\n",
        "    # No approval for poor credit (below 600)\n",
        "\n",
        "    return approved, interest_rate\n",
        "\n",
        "# Get user input\n",
        "credit_score = int(input(\"Enter your credit score: \"))\n",
        "income = float(input(\"Enter your annual income: \"))\n",
        "dti = float(input(\"Enter your debt-to-income ratio (e.g., 0.3 for 30%): \"))\n",
        "\n",
        "# Assess loan application\n",
        "approved, interest_rate = assess_loan(credit_score, income, dti)\n",
        "\n",
        "if approved:\n",
        "    print(\"Congratulations! Your loan is approved.\")\n",
        "    print(f\"Your interest rate is: {interest_rate * 100:.2f}%\")\n",
        "else:\n",
        "    print(\"Unfortunately, your loan application is not approved at this time.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36yrE5wZLTnZ",
        "outputId": "78b13cf3-436b-423b-ab11-12502ae784f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your credit score: 500\n",
            "Enter your annual income: 1200000\n",
            "Enter your debt-to-income ratio (e.g., 0.3 for 30%): 40\n",
            "Unfortunately, your loan application is not approved at this time.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyodbc\n",
        "\n",
        "# Database connection details\n",
        "server = 'STEPHENWAIGI'\n",
        "database = 'Roster'\n",
        "\n",
        "try:\n",
        "    # Connect to the database using Windows Authentication\n",
        "    conn = pyodbc.connect('Driver={SQL Server};'\n",
        "                          'Server=' + server + ';'\n",
        "                          'Database=' + database + ';'\n",
        "                          'Trusted_Connection=yes;')\n",
        "\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Execute the query to fetch data from the Revenue table\n",
        "    cursor.execute(\"SELECT * FROM Revenue\")\n",
        "    rows = cursor.fetchall()\n",
        "\n",
        "    # Display the data\n",
        "    for row in rows:\n",
        "        print(row)\n",
        "\n",
        "except pyodbc.Error as ex:\n",
        "    print(\"Error connecting to database or fetching data:\", ex)\n",
        "\n",
        "finally:\n",
        "    if conn:\n",
        "        conn.close()\n",
        "        print(\"Database connection closed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "mFmJoGQ2P7gP",
        "outputId": "8017b96f-1921-4d74-b9eb-872b233c781f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error connecting to database or fetching data: ('01000', \"[01000] [unixODBC][Driver Manager]Can't open lib 'SQL Server' : file not found (0) (SQLDriverConnect)\")\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'conn' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-31133ea8f978>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Database connection closed.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'conn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyodbc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0EthSO1QMDR",
        "outputId": "ec08d060-042c-4a26-e9f0-c7136a4a2ae3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyodbc\n",
            "  Downloading pyodbc-5.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (334 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m334.7/334.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyodbc\n",
            "Successfully installed pyodbc-5.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install unixodbc unixodbc-dev"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGIrh_A5QyuE",
        "outputId": "0c6c94ef-acb1-4c0c-f18c-ed934e1903fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "unixodbc-dev is already the newest version (2.3.9-5ubuntu0.1).\n",
            "unixodbc-dev set to manually installed.\n",
            "The following NEW packages will be installed:\n",
            "  unixodbc\n",
            "0 upgraded, 1 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 26.7 kB of archives.\n",
            "After this operation, 111 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 unixodbc amd64 2.3.9-5ubuntu0.1 [26.7 kB]\n",
            "Fetched 26.7 kB in 3s (9,726 B/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package unixodbc.\n",
            "(Reading database ... 123586 files and directories currently installed.)\n",
            "Preparing to unpack .../unixodbc_2.3.9-5ubuntu0.1_amd64.deb ...\n",
            "Unpacking unixodbc (2.3.9-5ubuntu0.1) ...\n",
            "Setting up unixodbc (2.3.9-5ubuntu0.1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "odbcinst -j"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "NjOPPI7WRZbR",
        "outputId": "30c3a0c7-f0f3-447d-bb09-f67bef2c7691"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'odbcinst' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-dab819a1a072>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0modbcinst\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'odbcinst' is not defined"
          ]
        }
      ]
    },
    {
      "source": [
        "sudo apt-get install unixodbc unixodbc-dev"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "oUB_ADstReht",
        "outputId": "4c9c19df-e092-4b9f-db92-3720c017b66b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-40-3f4ec9a7584e>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-40-3f4ec9a7584e>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    sudo apt-get install unixodbc unixodbc-dev\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!sudo apt-get install unixodbc unixodbc-dev"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIVK3YxQRzJj",
        "outputId": "a33c8c6f-5343-4eae-f25d-8c501d21ba63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "unixodbc-dev is already the newest version (2.3.9-5ubuntu0.1).\n",
            "unixodbc is already the newest version (2.3.9-5ubuntu0.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FtCFbZbkTKUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comprehensive Health Insurance Premium Calculator\n",
        "\n",
        "Scenario: A health insurance company determines premiums based on multiple health factors.\n",
        "Task: Create a program that calculates the insurance premium based on:\n",
        "\n",
        "Age (Child: 0-18, Young Adult: 19-35, Adult: 36-55, Senior: 56+)\n",
        "BMI Category (Underweight, Normal, Overweight, Obese)\n",
        "Smoking Status (Non-smoker, Occasional, Regular)\n",
        "Pre-existing Conditions (None, Minor, Major)\n",
        "Family History (No risks, Minor risks, Major risks)\n",
        "\n",
        "Rules:\n",
        "\n",
        "Base premium: 5000 for Child, 10000 for Young Adult, 15000 for Adult, 20000 for Senior\n",
        "Increase premium by:\n",
        "\n",
        "20% for Overweight, 50% for Obese\n",
        "30% for Occasional Smoker, 50% for Regular Smoker\n",
        "20% for Minor pre-existing conditions, 50% for Major\n",
        "10% for Minor family history risks, 25% for Major\n",
        "\n",
        "\n",
        "Decrease premium by 10% for Underweight\n",
        "Maximum increase: 150% of base premium\n",
        "Add flat 1000 for each year above 30"
      ],
      "metadata": {
        "id": "S6Tlwv-7UFpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_final_fare(base_fare, peak_season=False, holiday_season=False, weekend=False, booking_time='regular', passenger_type='adult'):\n",
        "    # Start with the base fare\n",
        "    fare = base_fare\n",
        "\n",
        "    # Apply seasonal adjustments\n",
        "    if peak_season:\n",
        "        fare *= 1.20\n",
        "    elif holiday_season:\n",
        "        fare *= 1.10\n",
        "\n",
        "    # Apply weekend adjustment\n",
        "    if weekend:\n",
        "        fare *= 1.10\n",
        "\n",
        "    # Apply booking time adjustments\n",
        "    if booking_time == 'late':\n",
        "        fare *= 1.15\n",
        "    elif booking_time == 'early':\n",
        "        fare *= 0.90\n",
        "\n",
        "    # Apply passenger type adjustments\n",
        "    if passenger_type == 'child':\n",
        "        fare *= 0.75\n",
        "    elif passenger_type == 'senior':\n",
        "        fare *= 0.90\n",
        "\n",
        "    # Ensure maximum combined discount and increase limits\n",
        "    if fare < base_fare * 0.70:\n",
        "        fare = base_fare * 0.70\n",
        "    if fare > base_fare * 1.50:\n",
        "        fare = base_fare * 1.50\n",
        "\n",
        "    return fare\n",
        "\n",
        "# Prompt the user for inputs\n",
        "base_fare = float(input(\"Enter the base fare: \"))\n",
        "peak_season = input(\"Is it peak season? (yes/no): \").lower() == 'yes'\n",
        "holiday_season = input(\"Is it holiday season? (yes/no): \").lower() == 'yes'\n",
        "weekend = input(\"Is the travel on a weekend? (yes/no): \").lower() == 'yes'\n",
        "booking_time = input(\"Enter the booking time (early, regular, late): \").lower()\n",
        "passenger_type = input(\"Enter the passenger type (adult, child, senior): \").lower()\n",
        "\n",
        "# Calculate the final fare\n",
        "final_fare = calculate_final_fare(base_fare, peak_season, holiday_season, weekend, booking_time, passenger_type)\n",
        "\n",
        "print(f\"The final fare is: ${final_fare:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKT1MIhkVMSL",
        "outputId": "8f2a1f86-60ce-476d-f0a4-69f13df02537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the base fare: 5300\n",
            "Is it peak season? (yes/no): yes\n",
            "Is it holiday season? (yes/no): yes\n",
            "Is the travel on a weekend? (yes/no): no\n",
            "Enter the booking time (early, regular, late): late\n",
            "Enter the passenger type (adult, child, senior): senior\n",
            "The final fare is: $6582.60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comprehensive Health Insurance Premium Calculator\n",
        "\n",
        "Scenario: A health insurance company determines premiums based on multiple health factors.\n",
        "Task: Create a program that calculates the insurance premium based on:\n",
        "\n",
        "Age (Child: 0-18, Young Adult: 19-35, Adult: 36-55, Senior: 56+)\n",
        "BMI Category (Underweight, Normal, Overweight, Obese)\n",
        "Smoking Status (Non-smoker, Occasional, Regular)\n",
        "Pre-existing Conditions (None, Minor, Major)\n",
        "Family History (No risks, Minor risks, Major risks)\n",
        "\n",
        "Rules:\n",
        "\n",
        "Base premium: 5000 for Child, 10000 for Young Adult, 15000 for Adult, 20000 for Senior\n",
        "Increase premium by:\n",
        "\n",
        "20% for Overweight, 50% for Obese\n",
        "30% for Occasional Smoker, 50% for Regular Smoker\n",
        "20% for Minor pre-existing conditions, 50% for Major\n",
        "10% for Minor family history risks, 25% for Major\n",
        "\n",
        "\n",
        "Decrease premium by 10% for Underweight\n",
        "Maximum increase: 150% of base premium\n",
        "Add flat 1000 for each year above 30\n"
      ],
      "metadata": {
        "id": "d1iGDCs8ZXG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_premium(age, bmi_category, smoking_status, pre_existing_conditions, family_history):\n",
        "    # Determine base premium based on age\n",
        "    if age <= 18:\n",
        "        base_premium = 5000\n",
        "    elif 19 <= age <= 35:\n",
        "        base_premium = 10000\n",
        "    elif 36 <= age <= 55:\n",
        "        base_premium = 15000\n",
        "    else:\n",
        "        base_premium = 20000\n",
        "\n",
        "    premium = base_premium\n",
        "\n",
        "    # Adjust premium based on BMI category\n",
        "    if bmi_category == 'underweight':\n",
        "        premium *= 0.90\n",
        "    elif bmi_category == 'overweight':\n",
        "        premium *= 1.20\n",
        "    elif bmi_category == 'obese':\n",
        "        premium *= 1.50\n",
        "\n",
        "    # Adjust premium based on smoking status\n",
        "    if smoking_status == 'occasional':\n",
        "        premium *= 1.30\n",
        "    elif smoking_status == 'regular':\n",
        "        premium *= 1.50\n",
        "\n",
        "    # Adjust premium based on pre-existing conditions\n",
        "    if pre_existing_conditions == 'minor':\n",
        "        premium *= 1.20\n",
        "    elif pre_existing_conditions == 'major':\n",
        "        premium *= 1.50\n",
        "\n",
        "    # Adjust premium based on family history\n",
        "    if family_history == 'minor':\n",
        "        premium *= 1.10\n",
        "    elif family_history == 'major':\n",
        "        premium *= 1.25\n",
        "\n",
        "    # Cap the maximum increase\n",
        "    if premium > base_premium * 2.50:\n",
        "        premium = base_premium * 2.50\n",
        "\n",
        "    # Add flat 1000 for each year above 30\n",
        "    if age > 30:\n",
        "        premium += (age - 30) * 1000\n",
        "\n",
        "    return premium\n",
        "\n",
        "# Prompt the user for inputs\n",
        "age = int(input(\"Enter your age: \"))\n",
        "bmi_category = input(\"Enter your BMI category (underweight, normal, overweight, obese): \").lower()\n",
        "smoking_status = input(\"Enter your smoking status (non-smoker, occasional, regular): \").lower()\n",
        "pre_existing_conditions = input(\"Enter your pre-existing conditions status (none, minor, major): \").lower()\n",
        "family_history = input(\"Enter your family history risk (no risks, minor risks, major risks): \").lower()\n",
        "\n",
        "# Calculate the premium\n",
        "final_premium = calculate_premium(age, bmi_category, smoking_status, pre_existing_conditions, family_history)\n",
        "\n",
        "print(f\"The calculated insurance premium is: ${final_premium:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkcqDBK7ZhKl",
        "outputId": "65b59713-00c0-4922-cc45-9da6297fa049"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your age: 56\n",
            "Enter your BMI category (underweight, normal, overweight, obese): overweight\n",
            "Enter your smoking status (non-smoker, occasional, regular): occasional\n",
            "Enter your pre-existing conditions status (none, minor, major): minor\n",
            "Enter your family history risk (no risks, minor risks, major risks): minor risks\n",
            "The calculated insurance premium is: $63440.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Advanced Academic Performance Evaluator\n",
        "\n",
        "Scenario: A school uses a comprehensive system to evaluate student performance and provide tailored advice.\n",
        "Task: Create a program that assesses a student's performance and provides recommendations based on:\n",
        "\n",
        "Grades in core subjects (Math, Science, Language, Social Studies)\n",
        "Extracurricular activities participation (None, Low, Medium, High)\n",
        "Attendance rate (Excellent: >95%, Good: 90-95%, Fair: 80-89%, Poor: <80%)\n",
        "Behavioral record (Excellent, Good, Needs Improvement, Poor)\n",
        "Learning style (Visual, Auditory, Kinesthetic)\n",
        "\n",
        "Rules:\n",
        "\n",
        "Calculate GPA (4.0 scale) from core subject grades\n",
        "Determine overall academic standing:\n",
        "\n",
        "Outstanding: GPA > 3.5, Good attendance, Good or Excellent behavior\n",
        "Good: GPA 3.0-3.5, At least Fair attendance, At least Needs Improvement behavior\n",
        "Average: GPA 2.0-2.9, At least Fair attendance\n",
        "Needs Improvement: GPA < 2.0 or Poor attendance"
      ],
      "metadata": {
        "id": "k5EFwtqNj00V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_gpa(grades):\n",
        "    return sum(grades) / len(grades)\n",
        "\n",
        "def determine_academic_standing(gpa, attendance_rate, behavioral_record):\n",
        "    if gpa > 3.5 and attendance_rate in ['excellent', 'good'] and behavioral_record in ['excellent', 'good']:\n",
        "        return 'Outstanding'\n",
        "    elif 3.0 <= gpa <= 3.5 and attendance_rate in ['excellent', 'good', 'fair'] and behavioral_record in ['excellent', 'good', 'needs improvement']:\n",
        "        return 'Good'\n",
        "    elif 2.0 <= gpa <= 2.9 and attendance_rate in ['excellent', 'good', 'fair']:\n",
        "        return 'Average'\n",
        "    else:\n",
        "        return 'Needs Improvement'\n",
        "\n",
        "def provide_recommendations(academic_standing, learning_style):\n",
        "    recommendations = {\n",
        "        'Outstanding': 'Keep up the great work! Continue to challenge yourself with advanced courses and leadership roles in extracurricular activities.',\n",
        "        'Good': 'You are doing well! Consider seeking additional help in subjects you find challenging and stay involved in extracurricular activities.',\n",
        "        'Average': 'Focus on improving your study habits and consider tutoring for subjects where you struggle. Maintain consistent attendance.',\n",
        "        'Needs Improvement': 'Seek help from teachers and counselors to address academic and attendance issues. Develop a study plan and stick to it.',\n",
        "    }\n",
        "\n",
        "    learning_style_advice = {\n",
        "        'visual': 'Use diagrams, charts, and written notes to enhance your learning.',\n",
        "        'auditory': 'Participate in discussions and use audio recordings to reinforce learning.',\n",
        "        'kinesthetic': 'Engage in hands-on activities and practical exercises to understand concepts better.'\n",
        "    }\n",
        "\n",
        "    return f\"{recommendations[academic_standing]} Additionally, since you are a {learning_style} learner, {learning_style_advice[learning_style]}\"\n",
        "\n",
        "# Prompt the user for inputs\n",
        "grades = []\n",
        "subjects = ['Math', 'Science', 'Language', 'Social Studies']\n",
        "for subject in subjects:\n",
        "    grade = float(input(f\"Enter your grade for {subject} (0.0 - 4.0 scale): \"))\n",
        "    grades.append(grade)\n",
        "\n",
        "extracurricular_activities = input(\"Enter your level of participation in extracurricular activities (none, low, medium, high): \").lower()\n",
        "attendance_rate = input(\"Enter your attendance rate (excellent: >95%, good: 90-95%, fair: 80-89%, poor: <80%): \").lower()\n",
        "behavioral_record = input(\"Enter your behavioral record (excellent, good, needs improvement, poor): \").lower()\n",
        "learning_style = input(\"Enter your learning style (visual, auditory, kinesthetic): \").lower()\n",
        "\n",
        "# Calculate GPA\n",
        "gpa = calculate_gpa(grades)\n",
        "\n",
        "# Determine academic standing\n",
        "academic_standing = determine_academic_standing(gpa, attendance_rate, behavioral_record)\n",
        "\n",
        "# Provide recommendations\n",
        "recommendations = provide_recommendations(academic_standing, learning_style)\n",
        "\n",
        "print(f\"\\nYour GPA is: {gpa:.2f}\")\n",
        "print(f\"Your academic standing is: {academic_standing}\")\n",
        "print(f\"Recommendations: {recommendations}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MoH_BVXlY4N",
        "outputId": "6125ad47-5acc-4233-ccfc-0d2f01edcfd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your grade for Math (0.0 - 4.0 scale): 3.6\n",
            "Enter your grade for Science (0.0 - 4.0 scale): 3.1\n",
            "Enter your grade for Language (0.0 - 4.0 scale): 4\n",
            "Enter your grade for Social Studies (0.0 - 4.0 scale): 3.3\n",
            "Enter your level of participation in extracurricular activities (none, low, medium, high): medium\n",
            "Enter your attendance rate (excellent: >95%, good: 90-95%, fair: 80-89%, poor: <80%): good\n",
            "Enter your behavioral record (excellent, good, needs improvement, poor): poor\n",
            "Enter your learning style (visual, auditory, kinesthetic): visual\n",
            "\n",
            "Your GPA is: 3.50\n",
            "Your academic standing is: Needs Improvement\n",
            "Recommendations: Seek help from teachers and counselors to address academic and attendance issues. Develop a study plan and stick to it. Additionally, since you are a visual learner, Use diagrams, charts, and written notes to enhance your learning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "loops\n"
      ],
      "metadata": {
        "id": "jOpiDf3houZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i=1\n",
        "while i<=5:\n",
        "  print(\"Jump\")\n",
        "  i=i+1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lMe0s1gow5s",
        "outputId": "eb15d829-9e20-48b1-b208-1cfc8877b37f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jump\n",
            "Jump\n",
            "Jump\n",
            "Jump\n",
            "Jump\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "script that calculates the average percentage based on the marks entered for six subjects (Math, English, Physics, Chemistry, Geography, Business):\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AQ4zFXYNtfoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate average percentage and determine result\n",
        "def calculate_average_and_result(marks):\n",
        "    total_marks = sum(marks)\n",
        "    average_percentage = total_marks / len(marks)\n",
        "\n",
        "    if average_percentage < 50:\n",
        "        result = \"Fail\"\n",
        "    elif 50 <= average_percentage < 60:\n",
        "        result = \"Pass\"\n",
        "    elif 60 <= average_percentage < 70:\n",
        "        result = \"Credit\"\n",
        "    else:\n",
        "        result = \"Distinction\"\n",
        "\n",
        "    return average_percentage, result\n",
        "\n",
        "# List of subjects\n",
        "subjects = [\"Maths\", \"English\", \"Physics\", \"Chemistry\", \"Business\"]\n",
        "\n",
        "# List to store marks\n",
        "marks = []\n",
        "\n",
        "# Loop to get marks for each subject\n",
        "for subject in subjects:\n",
        "    while True:\n",
        "        try:\n",
        "            mark = float(input(f\"Enter marks for {subject}: \"))\n",
        "            if 0 <= mark <= 100:\n",
        "                marks.append(mark)\n",
        "                break\n",
        "            else:\n",
        "                print(\"Please enter a valid mark between 0 and 100.\")\n",
        "        except ValueError:\n",
        "            print(\"Please enter a valid number.\")\n",
        "\n",
        "# Calculate average percentage and result\n",
        "average_percentage, result = calculate_average_and_result(marks)\n",
        "\n",
        "# Print the results\n",
        "print(f\"\\nYour average percentage is: {average_percentage:.2f}%\")\n",
        "print(f\"Your result is: {result}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuEpX8pvthk3",
        "outputId": "e1f1952c-9b9c-44e6-bb88-844b8981118d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter marks for Maths: 40\n",
            "Enter marks for English: 60\n",
            "Enter marks for Physics: 57\n",
            "Enter marks for Chemistry: 68\n",
            "Enter marks for Business: 80\n",
            "\n",
            "Your average percentage is: 61.00%\n",
            "Your result is: Credit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "code to use use loop to confirm password three times after which user is prompted to reset password."
      ],
      "metadata": {
        "id": "21vUX_i3yU4I"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S-wRLkSE1HAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actualPass = 'admin'\n",
        "i = 1\n",
        "\n",
        "while i < 4:\n",
        "    password = input(\"Enter the password: \")\n",
        "    if password == actualPass:\n",
        "        print(\"Login Successful\")\n",
        "        break\n",
        "    else:\n",
        "        print(\"Retry\")\n",
        "    i = i + 1\n",
        "\n",
        "if i == 4:\n",
        "    change_password = input(\"You have exhausted all retries. Would you like to change the password? (yes/no): \").lower()\n",
        "    if change_password == 'yes':\n",
        "        new_password = input(\"Enter the new password: \")\n",
        "        confirm_password = input(\"Confirm the new password: \")\n",
        "        if new_password == confirm_password:\n",
        "            actualPass = new_password\n",
        "            print(\"Password changed successfully.\")\n",
        "        else:\n",
        "            print(\"Passwords do not match. Password change failed.\")\n",
        "    else:\n",
        "        print(\"Password change not initiated.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6epEw6yyfqH",
        "outputId": "26e34956-73cc-4932-9009-791b315037db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the password: sdfdsf\n",
            "Retry\n",
            "Enter the password: sdfds\n",
            "Retry\n",
            "Enter the password: sdfsd\n",
            "Retry\n",
            "You have exhausted all retries. Would you like to change the password? (yes/no): yes\n",
            "Enter the new password: admin\n",
            "Confirm the new password: admin\n",
            "Password changed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GUfcJCcn3WEF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Countdown Timer\n",
        "Create a program that simulates a countdown timer.\n",
        "Start with a variable seconds  set to 10.\n",
        "Use a while loop to count down from 10 to 1, printing each number.\n",
        "After the loop ends, print \"Time's up!\".\n",
        "\n"
      ],
      "metadata": {
        "id": "X6kcr3O23Wsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Set the initial countdown time in seconds\n",
        "seconds = 10\n",
        "\n",
        "# Start the countdown\n",
        "while seconds > 0:\n",
        "    print(seconds)\n",
        "    time.sleep(1)  # Wait for 1 second\n",
        "    seconds -= 1\n",
        "\n",
        "# Print the final message\n",
        "print(\"Time's up!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22PAYGfR37Cl",
        "outputId": "ed2effea-1913-4a14-d936-0c5b7b0d69d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "9\n",
            "8\n",
            "7\n",
            "6\n",
            "5\n",
            "4\n",
            "3\n",
            "2\n",
            "1\n",
            "Time's up!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple Interest Calculator\n",
        "Develop a program that calculates simple interest over a period of years.\n",
        "Set initial variables: principal = 1000, rate = 0.05 (5%), years = 5.\n",
        "Use a while loop to calculate and print the balance for each year.\n",
        "For each year, calculate the interest and add it to the principal."
      ],
      "metadata": {
        "id": "UFXNr3c94Jn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set initial variables\n",
        "principal = 1000\n",
        "rate = 0.05  # 5% interest rate\n",
        "years = 5\n",
        "\n",
        "# Initialize a variable to track the current year\n",
        "current_year = 1\n",
        "\n",
        "# Calculate and print the balance for each year\n",
        "while current_year <= years:\n",
        "    # Calculate interest for the current year\n",
        "    interest = principal * rate\n",
        "    # Add the interest to the principal\n",
        "    principal += interest\n",
        "    # Print the balance for the current year\n",
        "    print(f\"Year {current_year}: Balance = ${principal:.2f}\")\n",
        "    # Move to the next year\n",
        "    current_year += 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XlENOip4Xl_",
        "outputId": "cafc88a2-561f-490c-fd93-134494c9d0af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Year 1: Balance = $1050.00\n",
            "Year 2: Balance = $1102.50\n",
            "Year 3: Balance = $1157.62\n",
            "Year 4: Balance = $1215.51\n",
            "Year 5: Balance = $1276.28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set initial variables\n",
        "principal = 1000\n",
        "rate = 0.05  # 5% interest rate\n",
        "\n",
        "# Prompt the user for the number of years\n",
        "years = int(input(\"Enter the number of years: \"))\n",
        "\n",
        "# Initialize a variable to track the current year\n",
        "current_year = 1\n",
        "\n",
        "# Calculate and print the balance for each year\n",
        "while current_year <= years:\n",
        "    # Calculate interest for the current year\n",
        "    interest = principal * rate\n",
        "    # Add the interest to the principal\n",
        "    principal += interest\n",
        "    # Print the balance for the current year\n",
        "    print(f\"Year {current_year}: Balance = ${principal:.2f}\")\n",
        "    # Move to the next year\n",
        "    current_year += 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiTxj9t74rBf",
        "outputId": "8e21ab8f-e0f2-43be-c1e2-210ea84cb695"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the number of years: 3\n",
            "Year 1: Balance = $1050.00\n",
            "Year 2: Balance = $1102.50\n",
            "Year 3: Balance = $1157.62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**list**   ordered changable and allows duplicates"
      ],
      "metadata": {
        "id": "3UoGCxUt8nda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "abc = [1,2,3,4,5]\n",
        "xyz = ['Stephen','Jacinta','Ayden']\n",
        "newlist =[1,'Stephen', 4.5]\n",
        "\n"
      ],
      "metadata": {
        "id": "iVraXRIf8pZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(abc)\n",
        "print(type(abc))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRcIU2-_9WqF",
        "outputId": "54836b77-999b-4c82-9a2e-f333559e7544"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5]\n",
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (abc[0])\n",
        "print (abc[3])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYZ_6Orw9gXV",
        "outputId": "fbdce778-8b49-4755-ea8d-a57cf4aa7220"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xyz[1] = 'Clement'\n",
        "print(xyz[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3AOBWmA-xoM",
        "outputId": "3f14f079-91b9-439f-f198-b8a553d7ee1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clement\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "abc =[1,2,3,2,1,6,5,7]\n",
        "print(len(abc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-lTQznV_62o",
        "outputId": "592b524f-db7f-49e0-c0a5-2231b1b19ea1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (abc[-4])  #index in python starts from zero"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iwq_x16kAeQc",
        "outputId": "ac43bedd-f9e8-45bb-b925-53366b18eb25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(abc[1:4])   # fisrt index and last index excluded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pY2VKr7SAxkG",
        "outputId": "56bbe4d2-248f-4d59-e33d-3e2b9afbbbe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 3, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "abc.append(30)\n",
        "print(abc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5EOE012CR2L",
        "outputId": "f6900e36-4bc9-4327-c489-04a1512d7a53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 2, 1, 6, 5, 7, 30]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(abc)\n",
        "print(xyz)\n",
        "abc.extend(xyz)\n",
        "print(abc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDbgOMIxDgdM",
        "outputId": "218cdfe2-1d26-44c9-9ccf-174b24ef981c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 2, 1, 6, 5, 7, 30, 'Stephen', 'Clement', 'Ayden']\n",
            "['Stephen', 'Clement', 'Ayden']\n",
            "[1, 2, 3, 2, 1, 6, 5, 7, 30, 'Stephen', 'Clement', 'Ayden', 'Stephen', 'Clement', 'Ayden']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "abc.remove(5)  # remove based on item\n",
        "print (abc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBoCI9yDDzxN",
        "outputId": "57906c47-305b-4724-8266-07e70571cef9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 2, 1, 6, 7, 30, 'Stephen', 'Clement', 'Ayden', 'Stephen', 'Clement', 'Ayden']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "abc.pop(7)  # remove based on index\n",
        "print(abc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVdstdvmEgFX",
        "outputId": "58db85a3-bc12-40ab-e2e4-abd34c4b9ab6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 2, 1, 6, 7, 'Clement', 'Ayden', 'Stephen', 'Clement', 'Ayden']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del abc[3]\n",
        "print(abc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NiOUOoCE5_q",
        "outputId": "c21d9877-7554-4311-c875-de6161e8814d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 1, 6, 7, 'Clement', 'Ayden', 'Stephen', 'Clement', 'Ayden']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in abc:   #iterate\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uo8OGR5vFtiv",
        "outputId": "40051fde-4b83-4974-fdc6-aacaafeb7ef9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "1\n",
            "6\n",
            "7\n",
            "Clement\n",
            "Ayden\n",
            "Stephen\n",
            "Clement\n",
            "Ayden\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "abc.clear()\n",
        "print(abc)\n",
        "print(len(abc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zRMmXrwIx0b",
        "outputId": "d4282b60-00a2-40bb-81f3-996642ae4fe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tuple**  ordered, not changeable, uses curved brackets, use square brackets to access"
      ],
      "metadata": {
        "id": "F2waU_fBLcZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customer = ('Stephen','Waigi', 'Data and Cloud')\n",
        "print(customer)\n",
        "print(type(customer))\n",
        "print(len(customer))\n",
        "print(customer[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1R_XT-SuMKpL",
        "outputId": "c87daff2-370b-40ee-f696-8a41271ba7b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Stephen', 'Waigi', 'Data and Cloud')\n",
            "<class 'tuple'>\n",
            "3\n",
            "Data and Cloud\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "customerList = list(customer)\n",
        "customerList.append('Data engineering')\n",
        "customer = tuple(customerList)\n",
        "print(customer)\n"
      ],
      "metadata": {
        "id": "vrlaLDuGOJxx",
        "outputId": "027bb775-ecae-4622-8892-6a94f0c94608",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Stephen', 'Waigi', 'Data and Cloud', 'Data engineering')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**set** uses curly brackets, can't allow duplicate, unordered, addition and delition allowed but update not allowed"
      ],
      "metadata": {
        "id": "wUCN9jiCQ4N-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "countries = {'Rwanda','Rwanda','Tanzania','Kenya'}\n",
        "print(countries)\n",
        "print(type(countries))\n",
        "print(len(countries))"
      ],
      "metadata": {
        "id": "LvrDVKgARtxl",
        "outputId": "bf606e85-68e4-4945-97b8-61901ee964e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Rwanda', 'Kenya', 'Tanzania'}\n",
            "<class 'set'>\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "countries.remove('Rwanda')\n",
        "print(countries)"
      ],
      "metadata": {
        "id": "GovuigxjSgar",
        "outputId": "056abc27-fc39-4659-ab7e-bd3840c14069",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Kenya', 'Tanzania'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dictionary**  does not allow duplicate and uses key Value Pair and is ordered and is editable"
      ],
      "metadata": {
        "id": "n21SA3xMUAJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "employee ={\"Name\": \"Stephen\", \"Age\":40, \"City\": \"Nairobi\", \"Country\": \"Kenya\"}\n",
        "print(employee)\n",
        "print(type(employee))\n",
        "print(len(employee))"
      ],
      "metadata": {
        "id": "nS42mJLkUO6F",
        "outputId": "f5a22dc0-8e27-4527-c94d-0ae54a4f8b92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Name': 'Stephen', 'Age': 40, 'City': 'Nairobi', 'Country': 'Kenya'}\n",
            "<class 'dict'>\n",
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(employee['Name'])\n",
        "print(employee['Age'])\n",
        "print(employee['City'])\n",
        "print(employee['Country'])\n",
        "print(employee.keys())\n",
        "print(employee.values())"
      ],
      "metadata": {
        "id": "dyvIWlb6V0t4",
        "outputId": "fea440a5-b9df-4d8b-d79b-81ba3a4bb478",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stephen\n",
            "40\n",
            "Nairobi\n",
            "Kenya\n",
            "dict_keys(['Name', 'Age', 'City', 'Country'])\n",
            "dict_values(['Stephen', 40, 'Nairobi', 'Kenya'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(employee.keys())\n",
        "keys = employee.keys()\n",
        "employee[\"salary\"] = 100000\n",
        "\n",
        "for i in keys:\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "9STjtw8yXNR4",
        "outputId": "82ad7c91-32c3-49b1-bb37-2dec6fa12f9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['Name', 'Age', 'City', 'Country'])\n",
            "Name\n",
            "Age\n",
            "City\n",
            "Country\n",
            "salary\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "imagine that you have a to do list. Take the 5 items from the user to add to the todo list. Ask user which item is finished, then remove it from the list. print the remaining items in the todo list."
      ],
      "metadata": {
        "id": "w4ve7fw1BxTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# File to store the to-do list\n",
        "todo_file = \"todo_list.json\"\n",
        "\n",
        "# Default to-do list\n",
        "default_todo_list = [\n",
        "    \"update stocks\",\n",
        "    \"call account customers\",\n",
        "    \"submit daily standup report\",\n",
        "    \"forward pending deliveries\",\n",
        "    \"compile weekly casuals wages\"\n",
        "]\n",
        "\n",
        "def load_todo_list():\n",
        "    if os.path.exists(todo_file):\n",
        "        with open(todo_file, \"r\") as file:\n",
        "            return json.load(file)\n",
        "    else:\n",
        "        return default_todo_list\n",
        "\n",
        "def save_todo_list(todo_list):\n",
        "    with open(todo_file, \"w\") as file:\n",
        "        json.dump(todo_list, file)\n",
        "\n",
        "# Load the to-do list\n",
        "todo_list = load_todo_list()\n",
        "\n",
        "if todo_list:\n",
        "    # Display the to-do list\n",
        "    print(\"To-do list:\")\n",
        "    for index, item in enumerate(todo_list, start=1):\n",
        "        print(f\"{index}. {item}\")\n",
        "\n",
        "    # Ask user which item is finished\n",
        "    finished_item = int(input(\"\\nWhich item is finished (enter the number): \"))\n",
        "\n",
        "    # Remove the finished item from the list\n",
        "    if 1 <= finished_item <= len(todo_list):\n",
        "        finished_task = todo_list.pop(finished_item - 1)\n",
        "        print(f\"\\nRemoved finished item: {finished_task}\")\n",
        "        save_todo_list(todo_list)\n",
        "    else:\n",
        "        print(\"\\nInvalid item number.\")\n",
        "else:\n",
        "    print(\"\\nAll items done, please create a new to-do list.\")\n",
        "\n",
        "# Display the remaining to-do list\n",
        "if todo_list:\n",
        "    print(\"\\nRemaining to-do list:\")\n",
        "    for index, item in enumerate(todo_list, start=1):\n",
        "        print(f\"{index}. {item}\")\n",
        "else:\n",
        "    print(\"\\nAll items done, please create a new to-do list.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k69Nhh-WFerW",
        "outputId": "188cb756-cafa-4d58-cbe9-5e2cfb55c5b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "All items done, please create a new to-do list.\n",
            "\n",
            "All items done, please create a new to-do list.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# File to store the to-do list\n",
        "todo_file = \"todo_list.json\"\n",
        "\n",
        "def load_todo_list():\n",
        "    if os.path.exists(todo_file):\n",
        "        with open(todo_file, \"r\") as file:\n",
        "            return json.load(file)\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "def save_todo_list(todo_list):\n",
        "    with open(todo_file, \"w\") as file:\n",
        "        json.dump(todo_list, file)\n",
        "\n",
        "def add_items_to_todo_list(todo_list):\n",
        "    print(\"Enter your to-do items (type 'done' when finished):\")\n",
        "    while len(todo_list) < 5:\n",
        "        item = input(\"Add item: \")\n",
        "        if item.lower() == 'done':\n",
        "            break\n",
        "        if len(todo_list) < 5:\n",
        "            todo_list.append(item)\n",
        "            print(f\"Item added. You can add {5 - len(todo_list)} more items.\")\n",
        "        else:\n",
        "            print(\"To-do list is full. You cannot add more than 5 items.\")\n",
        "    save_todo_list(todo_list)\n",
        "\n",
        "def main():\n",
        "    # Load the to-do list\n",
        "    todo_list = load_todo_list()\n",
        "\n",
        "    # If the list is empty, prompt the user to add items\n",
        "    if not todo_list:\n",
        "        add_items_to_todo_list(todo_list)\n",
        "\n",
        "    while todo_list:\n",
        "        # Display the to-do list\n",
        "        print(\"\\nTo-do list:\")\n",
        "        for index, item in enumerate(todo_list, start=1):\n",
        "            print(f\"{index}. {item}\")\n",
        "\n",
        "        # Ask user which item is finished\n",
        "        try:\n",
        "            finished_item = int(input(\"\\nWhich item is finished (enter the number): \"))\n",
        "            if 1 <= finished_item <= len(todo_list):\n",
        "                finished_task = todo_list.pop(finished_item - 1)\n",
        "                print(f\"\\nRemoved finished item: {finished_task}\")\n",
        "                save_todo_list(todo_list)\n",
        "            else:\n",
        "                print(\"\\nInvalid item number.\")\n",
        "        except ValueError:\n",
        "            print(\"\\nPlease enter a valid number.\")\n",
        "\n",
        "        # If all items are done, notify the user and ask for next action\n",
        "        if not todo_list:\n",
        "            print(\"\\nAll items done.\")\n",
        "            next_action = input(\"Do you want to create a new to-do list? (yes/no): \").strip().lower()\n",
        "            if next_action == 'yes':\n",
        "                add_items_to_todo_list(todo_list)\n",
        "            else:\n",
        "                print(\"Exiting the to-do list manager. Goodbye!\")\n",
        "                break\n",
        "\n",
        "# Final message if all items are done\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsLaOXYLJtU5",
        "outputId": "43ded5c9-e1ba-497a-d97a-38c310cda4c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your to-do items (type 'done' when finished):\n",
            "Add item: create database\n",
            "Item added. You can add 4 more items.\n",
            "Add item: create table schema\n",
            "Item added. You can add 3 more items.\n",
            "Add item: import data into table\n",
            "Item added. You can add 2 more items.\n",
            "Add item: create indexes\n",
            "Item added. You can add 1 more items.\n",
            "Add item: backup database\n",
            "Item added. You can add 0 more items.\n",
            "\n",
            "To-do list:\n",
            "1. create database\n",
            "2. create table schema\n",
            "3. import data into table\n",
            "4. create indexes\n",
            "5. backup database\n",
            "\n",
            "Which item is finished (enter the number): 1\n",
            "\n",
            "Removed finished item: create database\n",
            "\n",
            "To-do list:\n",
            "1. create table schema\n",
            "2. import data into table\n",
            "3. create indexes\n",
            "4. backup database\n",
            "\n",
            "Which item is finished (enter the number): 4\n",
            "\n",
            "Removed finished item: backup database\n",
            "\n",
            "To-do list:\n",
            "1. create table schema\n",
            "2. import data into table\n",
            "3. create indexes\n",
            "\n",
            "Which item is finished (enter the number): 2\n",
            "\n",
            "Removed finished item: import data into table\n",
            "\n",
            "To-do list:\n",
            "1. create table schema\n",
            "2. create indexes\n",
            "\n",
            "Which item is finished (enter the number): 3\n",
            "\n",
            "Invalid item number.\n",
            "\n",
            "To-do list:\n",
            "1. create table schema\n",
            "2. create indexes\n",
            "\n",
            "Which item is finished (enter the number): 2\n",
            "\n",
            "Removed finished item: create indexes\n",
            "\n",
            "To-do list:\n",
            "1. create table schema\n",
            "\n",
            "Which item is finished (enter the number): 1\n",
            "\n",
            "Removed finished item: create table schema\n",
            "\n",
            "All items done.\n",
            "Do you want to create a new to-do list? (yes/no): no\n",
            "Exiting the to-do list manager. Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**user defined functions**"
      ],
      "metadata": {
        "id": "Oriy8T0UPJkM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display(message):\n",
        "  print(message)\n",
        "\n",
        "x=20\n",
        "y=35\n",
        "z=x+y\n",
        "display(z)\n",
        "display(\"Hello all\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfG8Hc5mPQZ_",
        "outputId": "182fd496-5f18-469d-f91b-ee789a48ce0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "55\n",
            "Hello all\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add (x,y):\n",
        "  return x+y\n",
        "\n",
        "def substract(x,y):\n",
        "  return x-y\n",
        "\n",
        "def multiply(x,y):\n",
        "  return x*y\n",
        "\n",
        "def divide(x,y):\n",
        "  return x/y\n",
        "\n",
        "print(add(10,20))\n",
        "print(substract(10,20))\n",
        "print(multiply(10,20))\n",
        "print(divide(10,20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZV4JSPkQuzq",
        "outputId": "74a24d44-c763-47d8-bb37-e3bf9adca1d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30\n",
            "-10\n",
            "200\n",
            "0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Module /library"
      ],
      "metadata": {
        "id": "oFIi7D4XSU6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "print(dir(math))\n",
        "def calculate_average_and_result(marks):\n",
        "    total_marks = sum(marks)\n",
        "    total_subjects = len(marks)\n",
        "    average_percentage = (total_marks / (total_subjects * 100)) * 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dleu-eohS9aK",
        "outputId": "aa9ec985-a162-474f-fd2a-8bd72ca412a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['__doc__', '__loader__', '__name__', '__package__', '__spec__', 'acos', 'acosh', 'asin', 'asinh', 'atan', 'atan2', 'atanh', 'ceil', 'comb', 'copysign', 'cos', 'cosh', 'degrees', 'dist', 'e', 'erf', 'erfc', 'exp', 'expm1', 'fabs', 'factorial', 'floor', 'fmod', 'frexp', 'fsum', 'gamma', 'gcd', 'hypot', 'inf', 'isclose', 'isfinite', 'isinf', 'isnan', 'isqrt', 'lcm', 'ldexp', 'lgamma', 'log', 'log10', 'log1p', 'log2', 'modf', 'nan', 'nextafter', 'perm', 'pi', 'pow', 'prod', 'radians', 'remainder', 'sin', 'sinh', 'sqrt', 'tan', 'tanh', 'tau', 'trunc', 'ulp']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "class and objects"
      ],
      "metadata": {
        "id": "6WWPqbU01OWd"
      }
    },
    {
      "source": [
        "class Vehicle:\n",
        "  name = ''\n",
        "  color = ''\n",
        "\n",
        "# Create the instance outside the class definition\n",
        "car = Vehicle()\n",
        "car.name = 'BMW'\n",
        "car.color = 'red'\n",
        "print(car.name)\n",
        "print(car.color)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHhab-q51-9K",
        "outputId": "eab567bd-83cc-4458-c5ad-e880db83a458"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BMW\n",
            "red\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Car:\n",
        "  def __init__(self, name, color):\n",
        "    self.name = name\n",
        "    self.color = color"
      ],
      "metadata": {
        "id": "v18ZIvnc2iHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Employee:\n",
        "    def __init__(self, name, age, salary):\n",
        "        self.name = name\n",
        "        self.age = age\n",
        "        self.salary = salary\n",
        "        print(\"Inside the Employee init function:\")\n",
        "\n",
        "# Create instances of Employee\n",
        "c1 = Employee('Stephen', 40, 50000)\n",
        "c2 = Employee('Jacinta', 36, 45000)\n",
        "\n",
        "# Print the attributes for Stephen\n",
        "print(c1.name)\n",
        "print(c1.age)\n",
        "print(c1.salary)\n",
        "\n",
        "# Print the attributes for Jacinta\n",
        "print(c2.name)\n",
        "print(c2.age)\n",
        "print(c2.salary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmnMWUwe2xyM",
        "outputId": "d2f6abe2-be95-4c12-d6ec-0487c8490266"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inside the Employee init function:\n",
            "Inside the Employee init function:\n",
            "Stephen\n",
            "40\n",
            "50000\n",
            "Jacinta\n",
            "36\n",
            "45000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the attributes for Jacinta\n",
        "print(c2.name)\n",
        "print(c2.age)\n",
        "print(c2.salary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAVN_KXq6KDt",
        "outputId": "ebe8807e-168f-4b85-a94c-a2598d09c5a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jacinta\n",
            "36\n",
            "45000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "exception handling"
      ],
      "metadata": {
        "id": "vk0-EnT87kYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CF-dQmR6OHeJ",
        "outputId": "1e9c15c8-dfc1-45cb-cfb9-429c8629828b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JgivKegR3SB",
        "outputId": "eb891db2-2d3e-464d-a0f6-a182b983e7c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=528e428682a2fd9ef4e775c80692164843bda07bbb60b72afa317c09b4612a77\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "pyspark"
      ],
      "metadata": {
        "id": "flISvtVBdWrd"
      }
    },
    {
      "source": [
        "# Import the necessary PySpark module\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"MySparkApp\").getOrCreate()\n",
        "\n",
        "# Sample data\n",
        "data = [(\"Alice\", 25), (\"Bob\", 30)]\n",
        "# Define the schema\n",
        "schema = [\"name\", \"age\"]\n",
        "\n",
        "# Now you can use the spark object to create a DataFrame\n",
        "df = spark.createDataFrame(data, schema)\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRpCCkbbbfzc",
        "outputId": "ee42851d-8420-4414-e5cf-8c02e2ff6bed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+\n",
            "| name|age|\n",
            "+-----+---+\n",
            "|Alice| 25|\n",
            "|  Bob| 30|\n",
            "+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField  # Import both StructType and StructField\n",
        "\n",
        "help(StructType)\n",
        "help(StructField)  # Now StructField is accessible\n",
        "\n",
        "data = [(1, 'Stephen'), (2, 'Ayden')]\n",
        "schema = [\"id\", \"name\"]\n",
        "df = spark.createDataFrame(data, schema)\n",
        "df.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_k-E02A2e4To",
        "outputId": "367d5201-8662-4fc3-a5d9-5639fb62f922"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class StructType in module pyspark.sql.types:\n",
            "\n",
            "class StructType(DataType)\n",
            " |  StructType(fields: Optional[List[pyspark.sql.types.StructField]] = None)\n",
            " |  \n",
            " |  Struct type, consisting of a list of :class:`StructField`.\n",
            " |  \n",
            " |  This is the data type representing a :class:`Row`.\n",
            " |  \n",
            " |  Iterating a :class:`StructType` will iterate over its :class:`StructField`\\s.\n",
            " |  A contained :class:`StructField` can be accessed by its name or position.\n",
            " |  \n",
            " |  Examples\n",
            " |  --------\n",
            " |  >>> from pyspark.sql.types import *\n",
            " |  >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n",
            " |  >>> struct1[\"f1\"]\n",
            " |  StructField('f1', StringType(), True)\n",
            " |  >>> struct1[0]\n",
            " |  StructField('f1', StringType(), True)\n",
            " |  \n",
            " |  >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n",
            " |  >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n",
            " |  >>> struct1 == struct2\n",
            " |  True\n",
            " |  >>> struct1 = StructType([StructField(\"f1\", CharType(10), True)])\n",
            " |  >>> struct2 = StructType([StructField(\"f1\", CharType(10), True)])\n",
            " |  >>> struct1 == struct2\n",
            " |  True\n",
            " |  >>> struct1 = StructType([StructField(\"f1\", VarcharType(10), True)])\n",
            " |  >>> struct2 = StructType([StructField(\"f1\", VarcharType(10), True)])\n",
            " |  >>> struct1 == struct2\n",
            " |  True\n",
            " |  >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n",
            " |  >>> struct2 = StructType([StructField(\"f1\", StringType(), True),\n",
            " |  ...     StructField(\"f2\", IntegerType(), False)])\n",
            " |  >>> struct1 == struct2\n",
            " |  False\n",
            " |  \n",
            " |  The below example demonstrates how to create a DataFrame based on a struct created\n",
            " |  using class:`StructType` and class:`StructField`:\n",
            " |  \n",
            " |  >>> data = [(\"Alice\", [\"Java\", \"Scala\"]), (\"Bob\", [\"Python\", \"Scala\"])]\n",
            " |  >>> schema = StructType([\n",
            " |  ...     StructField(\"name\", StringType()),\n",
            " |  ...     StructField(\"languagesSkills\", ArrayType(StringType())),\n",
            " |  ... ])\n",
            " |  >>> df = spark.createDataFrame(data=data, schema=schema)\n",
            " |  >>> df.printSchema()\n",
            " |  root\n",
            " |   |-- name: string (nullable = true)\n",
            " |   |-- languagesSkills: array (nullable = true)\n",
            " |   |    |-- element: string (containsNull = true)\n",
            " |  >>> df.show()\n",
            " |  +-----+---------------+\n",
            " |  | name|languagesSkills|\n",
            " |  +-----+---------------+\n",
            " |  |Alice|  [Java, Scala]|\n",
            " |  |  Bob|[Python, Scala]|\n",
            " |  +-----+---------------+\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      StructType\n",
            " |      DataType\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __getitem__(self, key: Union[str, int]) -> pyspark.sql.types.StructField\n",
            " |      Access fields by name or slice.\n",
            " |  \n",
            " |  __init__(self, fields: Optional[List[pyspark.sql.types.StructField]] = None)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  __iter__(self) -> Iterator[pyspark.sql.types.StructField]\n",
            " |      Iterate the fields\n",
            " |  \n",
            " |  __len__(self) -> int\n",
            " |      Return the number of fields.\n",
            " |  \n",
            " |  __repr__(self) -> str\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  add(self, field: Union[str, pyspark.sql.types.StructField], data_type: Union[str, pyspark.sql.types.DataType, NoneType] = None, nullable: bool = True, metadata: Optional[Dict[str, Any]] = None) -> 'StructType'\n",
            " |      Construct a :class:`StructType` by adding new elements to it, to define the schema.\n",
            " |      The method accepts either:\n",
            " |      \n",
            " |          a) A single parameter which is a :class:`StructField` object.\n",
            " |          b) Between 2 and 4 parameters as (name, data_type, nullable (optional),\n",
            " |             metadata(optional). The data_type parameter may be either a String or a\n",
            " |             :class:`DataType` object.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      field : str or :class:`StructField`\n",
            " |          Either the name of the field or a :class:`StructField` object\n",
            " |      data_type : :class:`DataType`, optional\n",
            " |          If present, the DataType of the :class:`StructField` to create\n",
            " |      nullable : bool, optional\n",
            " |          Whether the field to add should be nullable (default True)\n",
            " |      metadata : dict, optional\n",
            " |          Any additional metadata (default None)\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`StructType`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from pyspark.sql.types import IntegerType, StringType, StructField, StructType\n",
            " |      >>> struct1 = StructType().add(\"f1\", StringType(), True).add(\"f2\", StringType(), True, None)\n",
            " |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True),\n",
            " |      ...     StructField(\"f2\", StringType(), True, None)])\n",
            " |      >>> struct1 == struct2\n",
            " |      True\n",
            " |      >>> struct1 = StructType().add(StructField(\"f1\", StringType(), True))\n",
            " |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n",
            " |      >>> struct1 == struct2\n",
            " |      True\n",
            " |      >>> struct1 = StructType().add(\"f1\", \"string\", True)\n",
            " |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n",
            " |      >>> struct1 == struct2\n",
            " |      True\n",
            " |  \n",
            " |  fieldNames(self) -> List[str]\n",
            " |      Returns all field names in a list.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from pyspark.sql.types import StringType, StructField, StructType\n",
            " |      >>> struct = StructType([StructField(\"f1\", StringType(), True)])\n",
            " |      >>> struct.fieldNames()\n",
            " |      ['f1']\n",
            " |  \n",
            " |  fromInternal(self, obj: Tuple) -> 'Row'\n",
            " |      Converts an internal SQL object into a native Python object.\n",
            " |  \n",
            " |  jsonValue(self) -> Dict[str, Any]\n",
            " |  \n",
            " |  needConversion(self) -> bool\n",
            " |      Does this type needs conversion between Python object and internal SQL object.\n",
            " |      \n",
            " |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
            " |  \n",
            " |  simpleString(self) -> str\n",
            " |  \n",
            " |  toInternal(self, obj: Tuple) -> Tuple\n",
            " |      Converts a Python object into an internal SQL object.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods defined here:\n",
            " |  \n",
            " |  fromJson(json: Dict[str, Any]) -> 'StructType' from builtins.type\n",
            " |      Constructs :class:`StructType` from a schema defined in JSON format.\n",
            " |      \n",
            " |      Below is a JSON schema it must adhere to::\n",
            " |      \n",
            " |          {\n",
            " |            \"title\":\"StructType\",\n",
            " |            \"description\":\"Schema of StructType in json format\",\n",
            " |            \"type\":\"object\",\n",
            " |            \"properties\":{\n",
            " |               \"fields\":{\n",
            " |                  \"description\":\"Array of struct fields\",\n",
            " |                  \"type\":\"array\",\n",
            " |                  \"items\":{\n",
            " |                      \"type\":\"object\",\n",
            " |                      \"properties\":{\n",
            " |                         \"name\":{\n",
            " |                            \"description\":\"Name of the field\",\n",
            " |                            \"type\":\"string\"\n",
            " |                         },\n",
            " |                         \"type\":{\n",
            " |                            \"description\": \"Type of the field. Can either be\n",
            " |                                            another nested StructType or primitive type\",\n",
            " |                            \"type\":\"object/string\"\n",
            " |                         },\n",
            " |                         \"nullable\":{\n",
            " |                            \"description\":\"If nulls are allowed\",\n",
            " |                            \"type\":\"boolean\"\n",
            " |                         },\n",
            " |                         \"metadata\":{\n",
            " |                            \"description\":\"Additional metadata to supply\",\n",
            " |                            \"type\":\"object\"\n",
            " |                         },\n",
            " |                         \"required\":[\n",
            " |                            \"name\",\n",
            " |                            \"type\",\n",
            " |                            \"nullable\",\n",
            " |                            \"metadata\"\n",
            " |                         ]\n",
            " |                      }\n",
            " |                 }\n",
            " |              }\n",
            " |           }\n",
            " |         }\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      json : dict or a dict-like object e.g. JSON object\n",
            " |          This \"dict\" must have \"fields\" key that returns an array of fields\n",
            " |          each of which must have specific keys (name, type, nullable, metadata).\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`StructType`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> json_str = '''\n",
            " |      ...  {\n",
            " |      ...      \"fields\": [\n",
            " |      ...          {\n",
            " |      ...              \"metadata\": {},\n",
            " |      ...              \"name\": \"Person\",\n",
            " |      ...              \"nullable\": true,\n",
            " |      ...              \"type\": {\n",
            " |      ...                  \"fields\": [\n",
            " |      ...                      {\n",
            " |      ...                          \"metadata\": {},\n",
            " |      ...                          \"name\": \"name\",\n",
            " |      ...                          \"nullable\": false,\n",
            " |      ...                          \"type\": \"string\"\n",
            " |      ...                      },\n",
            " |      ...                      {\n",
            " |      ...                          \"metadata\": {},\n",
            " |      ...                          \"name\": \"surname\",\n",
            " |      ...                          \"nullable\": false,\n",
            " |      ...                          \"type\": \"string\"\n",
            " |      ...                      }\n",
            " |      ...                  ],\n",
            " |      ...                  \"type\": \"struct\"\n",
            " |      ...              }\n",
            " |      ...          }\n",
            " |      ...      ],\n",
            " |      ...      \"type\": \"struct\"\n",
            " |      ...  }\n",
            " |      ...  '''\n",
            " |      >>> import json\n",
            " |      >>> scheme = StructType.fromJson(json.loads(json_str))\n",
            " |      >>> scheme.simpleString()\n",
            " |      'struct<Person:struct<name:string,surname:string>>'\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from DataType:\n",
            " |  \n",
            " |  __eq__(self, other: Any) -> bool\n",
            " |      Return self==value.\n",
            " |  \n",
            " |  __hash__(self) -> int\n",
            " |      Return hash(self).\n",
            " |  \n",
            " |  __ne__(self, other: Any) -> bool\n",
            " |      Return self!=value.\n",
            " |  \n",
            " |  json(self) -> str\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from DataType:\n",
            " |  \n",
            " |  typeName() -> str from builtins.type\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from DataType:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n",
            "Help on class StructField in module pyspark.sql.types:\n",
            "\n",
            "class StructField(DataType)\n",
            " |  StructField(name: str, dataType: pyspark.sql.types.DataType, nullable: bool = True, metadata: Optional[Dict[str, Any]] = None)\n",
            " |  \n",
            " |  A field in :class:`StructType`.\n",
            " |  \n",
            " |  Parameters\n",
            " |  ----------\n",
            " |  name : str\n",
            " |      name of the field.\n",
            " |  dataType : :class:`DataType`\n",
            " |      :class:`DataType` of the field.\n",
            " |  nullable : bool, optional\n",
            " |      whether the field can be null (None) or not.\n",
            " |  metadata : dict, optional\n",
            " |      a dict from string to simple type that can be toInternald to JSON automatically\n",
            " |  \n",
            " |  Examples\n",
            " |  --------\n",
            " |  >>> from pyspark.sql.types import StringType, StructField\n",
            " |  >>> (StructField(\"f1\", StringType(), True)\n",
            " |  ...      == StructField(\"f1\", StringType(), True))\n",
            " |  True\n",
            " |  >>> (StructField(\"f1\", StringType(), True)\n",
            " |  ...      == StructField(\"f2\", StringType(), True))\n",
            " |  False\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      StructField\n",
            " |      DataType\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, name: str, dataType: pyspark.sql.types.DataType, nullable: bool = True, metadata: Optional[Dict[str, Any]] = None)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  __repr__(self) -> str\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  fromInternal(self, obj: ~T) -> ~T\n",
            " |      Converts an internal SQL object into a native Python object.\n",
            " |  \n",
            " |  jsonValue(self) -> Dict[str, Any]\n",
            " |  \n",
            " |  needConversion(self) -> bool\n",
            " |      Does this type needs conversion between Python object and internal SQL object.\n",
            " |      \n",
            " |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
            " |  \n",
            " |  simpleString(self) -> str\n",
            " |  \n",
            " |  toInternal(self, obj: ~T) -> ~T\n",
            " |      Converts a Python object into an internal SQL object.\n",
            " |  \n",
            " |  typeName(self) -> str\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods defined here:\n",
            " |  \n",
            " |  fromJson(json: Dict[str, Any]) -> 'StructField' from builtins.type\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from DataType:\n",
            " |  \n",
            " |  __eq__(self, other: Any) -> bool\n",
            " |      Return self==value.\n",
            " |  \n",
            " |  __hash__(self) -> int\n",
            " |      Return hash(self).\n",
            " |  \n",
            " |  __ne__(self, other: Any) -> bool\n",
            " |      Return self!=value.\n",
            " |  \n",
            " |  json(self) -> str\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from DataType:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n",
            "+---+-------+\n",
            "| id|   name|\n",
            "+---+-------+\n",
            "|  1|Stephen|\n",
            "|  2|  Ayden|\n",
            "+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "data = [(1, 'Stephen'), (2, 'Ayden')]\n",
        "schema = StructType([StructField(name=\"id\", dataType=IntegerType(), nullable=True),\n",
        "    StructField(name=\"name\", dataType=StringType(), nullable=True)\n",
        "])\n",
        "df = spark.createDataFrame(data, schema)\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1z72t5Rf0Ot",
        "outputId": "8492a6e1-a789-46ac-9410-86d8151636d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "| id|   name|\n",
            "+---+-------+\n",
            "|  1|Stephen|\n",
            "|  2|  Ayden|\n",
            "+---+-------+\n",
            "\n",
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [{'id': 1, \"name\": 'Stephen'},\n",
        "        {'id': 2, \"name\": 'Ayden'}]\n",
        "\n",
        "df = spark.createDataFrame(data)\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRYv0lDchSg6",
        "outputId": "9f7a8947-e6cc-4598-aabb-e8fef6289e0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "| id|   name|\n",
            "+---+-------+\n",
            "|  1|Stephen|\n",
            "|  2|  Ayden|\n",
            "+---+-------+\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(spark.read)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYLwD_9wkH1s",
        "outputId": "5339ef0c-a8a3-4c3a-bafe-acb91c402d34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on DataFrameReader in module pyspark.sql.readwriter object:\n",
            "\n",
            "class DataFrameReader(OptionUtils)\n",
            " |  DataFrameReader(spark: 'SparkSession')\n",
            " |  \n",
            " |  Interface used to load a :class:`DataFrame` from external storage systems\n",
            " |  (e.g. file systems, key-value stores, etc). Use :attr:`SparkSession.read`\n",
            " |  to access this.\n",
            " |  \n",
            " |  .. versionadded:: 1.4.0\n",
            " |  \n",
            " |  .. versionchanged:: 3.4.0\n",
            " |      Supports Spark Connect.\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      DataFrameReader\n",
            " |      OptionUtils\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, spark: 'SparkSession')\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  csv(self, path: Union[str, List[str]], schema: Union[pyspark.sql.types.StructType, str, NoneType] = None, sep: Optional[str] = None, encoding: Optional[str] = None, quote: Optional[str] = None, escape: Optional[str] = None, comment: Optional[str] = None, header: Union[bool, str, NoneType] = None, inferSchema: Union[bool, str, NoneType] = None, ignoreLeadingWhiteSpace: Union[bool, str, NoneType] = None, ignoreTrailingWhiteSpace: Union[bool, str, NoneType] = None, nullValue: Optional[str] = None, nanValue: Optional[str] = None, positiveInf: Optional[str] = None, negativeInf: Optional[str] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, maxColumns: Union[str, int, NoneType] = None, maxCharsPerColumn: Union[str, int, NoneType] = None, maxMalformedLogPerPartition: Union[str, int, NoneType] = None, mode: Optional[str] = None, columnNameOfCorruptRecord: Optional[str] = None, multiLine: Union[bool, str, NoneType] = None, charToEscapeQuoteEscaping: Optional[str] = None, samplingRatio: Union[str, float, NoneType] = None, enforceSchema: Union[bool, str, NoneType] = None, emptyValue: Optional[str] = None, locale: Optional[str] = None, lineSep: Optional[str] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None, unescapedQuoteHandling: Optional[str] = None) -> 'DataFrame'\n",
            " |      Loads a CSV file and returns the result as a  :class:`DataFrame`.\n",
            " |      \n",
            " |      This function will go through the input once to determine the input schema if\n",
            " |      ``inferSchema`` is enabled. To avoid going through the entire data once, disable\n",
            " |      ``inferSchema`` option or specify the schema explicitly using ``schema``.\n",
            " |      \n",
            " |      .. versionadded:: 2.0.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      path : str or list\n",
            " |          string, or list of strings, for input path(s),\n",
            " |          or RDD of Strings storing CSV rows.\n",
            " |      schema : :class:`pyspark.sql.types.StructType` or str, optional\n",
            " |          an optional :class:`pyspark.sql.types.StructType` for the input schema\n",
            " |          or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
            " |      \n",
            " |      Other Parameters\n",
            " |      ----------------\n",
            " |      Extra options\n",
            " |          For the extra options, refer to\n",
            " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_\n",
            " |          for the version you use.\n",
            " |      \n",
            " |          .. # noqa\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      Write a DataFrame into a CSV file and read it back.\n",
            " |      \n",
            " |      >>> import tempfile\n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     # Write a DataFrame into a CSV file\n",
            " |      ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n",
            " |      ...     df.write.mode(\"overwrite\").format(\"csv\").save(d)\n",
            " |      ...\n",
            " |      ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.\n",
            " |      ...     spark.read.csv(d, schema=df.schema, nullValue=\"Hyukjin Kwon\").show()\n",
            " |      +---+----+\n",
            " |      |age|name|\n",
            " |      +---+----+\n",
            " |      |100|NULL|\n",
            " |      +---+----+\n",
            " |  \n",
            " |  format(self, source: str) -> 'DataFrameReader'\n",
            " |      Specifies the input data source format.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      source : str\n",
            " |          string, name of the data source, e.g. 'json', 'parquet'.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> spark.read.format('json')\n",
            " |      <...readwriter.DataFrameReader object ...>\n",
            " |      \n",
            " |      Write a DataFrame into a JSON file and read it back.\n",
            " |      \n",
            " |      >>> import tempfile\n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     # Write a DataFrame into a JSON file\n",
            " |      ...     spark.createDataFrame(\n",
            " |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
            " |      ...     ).write.mode(\"overwrite\").format(\"json\").save(d)\n",
            " |      ...\n",
            " |      ...     # Read the JSON file as a DataFrame.\n",
            " |      ...     spark.read.format('json').load(d).show()\n",
            " |      +---+------------+\n",
            " |      |age|        name|\n",
            " |      +---+------------+\n",
            " |      |100|Hyukjin Kwon|\n",
            " |      +---+------------+\n",
            " |  \n",
            " |  jdbc(self, url: str, table: str, column: Optional[str] = None, lowerBound: Union[str, int, NoneType] = None, upperBound: Union[str, int, NoneType] = None, numPartitions: Optional[int] = None, predicates: Optional[List[str]] = None, properties: Optional[Dict[str, str]] = None) -> 'DataFrame'\n",
            " |      Construct a :class:`DataFrame` representing the database table named ``table``\n",
            " |      accessible via JDBC URL ``url`` and connection ``properties``.\n",
            " |      \n",
            " |      Partitions of the table will be retrieved in parallel if either ``column`` or\n",
            " |      ``predicates`` is specified. ``lowerBound``, ``upperBound`` and ``numPartitions``\n",
            " |      is needed when ``column`` is specified.\n",
            " |      \n",
            " |      If both ``column`` and ``predicates`` are specified, ``column`` will be used.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      table : str\n",
            " |          the name of the table\n",
            " |      column : str, optional\n",
            " |          alias of ``partitionColumn`` option. Refer to ``partitionColumn`` in\n",
            " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#data-source-option>`_\n",
            " |          for the version you use.\n",
            " |      predicates : list, optional\n",
            " |          a list of expressions suitable for inclusion in WHERE clauses;\n",
            " |          each one defines one partition of the :class:`DataFrame`\n",
            " |      properties : dict, optional\n",
            " |          a dictionary of JDBC database connection arguments. Normally at\n",
            " |          least properties \"user\" and \"password\" with their corresponding values.\n",
            " |          For example { 'user' : 'SYSTEM', 'password' : 'mypassword' }\n",
            " |      \n",
            " |      Other Parameters\n",
            " |      ----------------\n",
            " |      Extra options\n",
            " |          For the extra options, refer to\n",
            " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#data-source-option>`_\n",
            " |          for the version you use.\n",
            " |      \n",
            " |          .. # noqa\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      Don't create too many partitions in parallel on a large cluster;\n",
            " |      otherwise Spark might crash your external database systems.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |  \n",
            " |  json(self, path: Union[str, List[str], pyspark.rdd.RDD[str]], schema: Union[pyspark.sql.types.StructType, str, NoneType] = None, primitivesAsString: Union[bool, str, NoneType] = None, prefersDecimal: Union[bool, str, NoneType] = None, allowComments: Union[bool, str, NoneType] = None, allowUnquotedFieldNames: Union[bool, str, NoneType] = None, allowSingleQuotes: Union[bool, str, NoneType] = None, allowNumericLeadingZero: Union[bool, str, NoneType] = None, allowBackslashEscapingAnyCharacter: Union[bool, str, NoneType] = None, mode: Optional[str] = None, columnNameOfCorruptRecord: Optional[str] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, multiLine: Union[bool, str, NoneType] = None, allowUnquotedControlChars: Union[bool, str, NoneType] = None, lineSep: Optional[str] = None, samplingRatio: Union[str, float, NoneType] = None, dropFieldIfAllNull: Union[bool, str, NoneType] = None, encoding: Optional[str] = None, locale: Optional[str] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None, allowNonNumericNumbers: Union[bool, str, NoneType] = None) -> 'DataFrame'\n",
            " |      Loads JSON files and returns the results as a :class:`DataFrame`.\n",
            " |      \n",
            " |      `JSON Lines <http://jsonlines.org/>`_ (newline-delimited JSON) is supported by default.\n",
            " |      For JSON (one record per file), set the ``multiLine`` parameter to ``true``.\n",
            " |      \n",
            " |      If the ``schema`` parameter is not specified, this function goes\n",
            " |      through the input once to determine the input schema.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      path : str, list or :class:`RDD`\n",
            " |          string represents path to the JSON dataset, or a list of paths,\n",
            " |          or RDD of Strings storing JSON objects.\n",
            " |      schema : :class:`pyspark.sql.types.StructType` or str, optional\n",
            " |          an optional :class:`pyspark.sql.types.StructType` for the input schema or\n",
            " |          a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
            " |      \n",
            " |      Other Parameters\n",
            " |      ----------------\n",
            " |      Extra options\n",
            " |          For the extra options, refer to\n",
            " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option>`_\n",
            " |          for the version you use.\n",
            " |      \n",
            " |          .. # noqa\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      Write a DataFrame into a JSON file and read it back.\n",
            " |      \n",
            " |      >>> import tempfile\n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     # Write a DataFrame into a JSON file\n",
            " |      ...     spark.createDataFrame(\n",
            " |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
            " |      ...     ).write.mode(\"overwrite\").format(\"json\").save(d)\n",
            " |      ...\n",
            " |      ...     # Read the JSON file as a DataFrame.\n",
            " |      ...     spark.read.json(d).show()\n",
            " |      +---+------------+\n",
            " |      |age|        name|\n",
            " |      +---+------------+\n",
            " |      |100|Hyukjin Kwon|\n",
            " |      +---+------------+\n",
            " |  \n",
            " |  load(self, path: Union[str, List[str], NoneType] = None, format: Optional[str] = None, schema: Union[pyspark.sql.types.StructType, str, NoneType] = None, **options: 'OptionalPrimitiveType') -> 'DataFrame'\n",
            " |      Loads data from a data source and returns it as a :class:`DataFrame`.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      path : str or list, optional\n",
            " |          optional string or a list of string for file-system backed data sources.\n",
            " |      format : str, optional\n",
            " |          optional string for format of the data source. Default to 'parquet'.\n",
            " |      schema : :class:`pyspark.sql.types.StructType` or str, optional\n",
            " |          optional :class:`pyspark.sql.types.StructType` for the input schema\n",
            " |          or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
            " |      **options : dict\n",
            " |          all other string options\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      Load a CSV file with format, schema and options specified.\n",
            " |      \n",
            " |      >>> import tempfile\n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     # Write a DataFrame into a CSV file with a header\n",
            " |      ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n",
            " |      ...     df.write.option(\"header\", True).mode(\"overwrite\").format(\"csv\").save(d)\n",
            " |      ...\n",
            " |      ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon',\n",
            " |      ...     # and 'header' option set to `True`.\n",
            " |      ...     df = spark.read.load(\n",
            " |      ...         d, schema=df.schema, format=\"csv\", nullValue=\"Hyukjin Kwon\", header=True)\n",
            " |      ...     df.printSchema()\n",
            " |      ...     df.show()\n",
            " |      root\n",
            " |       |-- age: long (nullable = true)\n",
            " |       |-- name: string (nullable = true)\n",
            " |      +---+----+\n",
            " |      |age|name|\n",
            " |      +---+----+\n",
            " |      |100|NULL|\n",
            " |      +---+----+\n",
            " |  \n",
            " |  option(self, key: str, value: 'OptionalPrimitiveType') -> 'DataFrameReader'\n",
            " |      Adds an input option for the underlying data source.\n",
            " |      \n",
            " |      .. versionadded:: 1.5.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      key : str\n",
            " |          The key for the option to set.\n",
            " |      value\n",
            " |          The value for the option to set.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> spark.read.option(\"key\", \"value\")\n",
            " |      <...readwriter.DataFrameReader object ...>\n",
            " |      \n",
            " |      Specify the option 'nullValue' with reading a CSV file.\n",
            " |      \n",
            " |      >>> import tempfile\n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     # Write a DataFrame into a CSV file\n",
            " |      ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n",
            " |      ...     df.write.mode(\"overwrite\").format(\"csv\").save(d)\n",
            " |      ...\n",
            " |      ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.\n",
            " |      ...     spark.read.schema(df.schema).option(\n",
            " |      ...         \"nullValue\", \"Hyukjin Kwon\").format('csv').load(d).show()\n",
            " |      +---+----+\n",
            " |      |age|name|\n",
            " |      +---+----+\n",
            " |      |100|NULL|\n",
            " |      +---+----+\n",
            " |  \n",
            " |  options(self, **options: 'OptionalPrimitiveType') -> 'DataFrameReader'\n",
            " |      Adds input options for the underlying data source.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      **options : dict\n",
            " |          The dictionary of string keys and prmitive-type values.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> spark.read.option(\"key\", \"value\")\n",
            " |      <...readwriter.DataFrameReader object ...>\n",
            " |      \n",
            " |      Specify the option 'nullValue' and 'header' with reading a CSV file.\n",
            " |      \n",
            " |      >>> import tempfile\n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     # Write a DataFrame into a CSV file with a header.\n",
            " |      ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n",
            " |      ...     df.write.option(\"header\", True).mode(\"overwrite\").format(\"csv\").save(d)\n",
            " |      ...\n",
            " |      ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon',\n",
            " |      ...     # and 'header' option set to `True`.\n",
            " |      ...     spark.read.options(\n",
            " |      ...         nullValue=\"Hyukjin Kwon\",\n",
            " |      ...         header=True\n",
            " |      ...     ).format('csv').load(d).show()\n",
            " |      +---+----+\n",
            " |      |age|name|\n",
            " |      +---+----+\n",
            " |      |100|NULL|\n",
            " |      +---+----+\n",
            " |  \n",
            " |  orc(self, path: Union[str, List[str]], mergeSchema: Optional[bool] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None) -> 'DataFrame'\n",
            " |      Loads ORC files, returning the result as a :class:`DataFrame`.\n",
            " |      \n",
            " |      .. versionadded:: 1.5.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      path : str or list\n",
            " |      \n",
            " |      Other Parameters\n",
            " |      ----------------\n",
            " |      Extra options\n",
            " |          For the extra options, refer to\n",
            " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-orc.html#data-source-option>`_\n",
            " |          for the version you use.\n",
            " |      \n",
            " |          .. # noqa\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      Write a DataFrame into a ORC file and read it back.\n",
            " |      \n",
            " |      >>> import tempfile\n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     # Write a DataFrame into a ORC file\n",
            " |      ...     spark.createDataFrame(\n",
            " |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
            " |      ...     ).write.mode(\"overwrite\").format(\"orc\").save(d)\n",
            " |      ...\n",
            " |      ...     # Read the Parquet file as a DataFrame.\n",
            " |      ...     spark.read.orc(d).show()\n",
            " |      +---+------------+\n",
            " |      |age|        name|\n",
            " |      +---+------------+\n",
            " |      |100|Hyukjin Kwon|\n",
            " |      +---+------------+\n",
            " |  \n",
            " |  parquet(self, *paths: str, **options: 'OptionalPrimitiveType') -> 'DataFrame'\n",
            " |      Loads Parquet files, returning the result as a :class:`DataFrame`.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      paths : str\n",
            " |      \n",
            " |      Other Parameters\n",
            " |      ----------------\n",
            " |      **options\n",
            " |          For the extra options, refer to\n",
            " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#data-source-option>`_\n",
            " |          for the version you use.\n",
            " |      \n",
            " |          .. # noqa\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      Write a DataFrame into a Parquet file and read it back.\n",
            " |      \n",
            " |      >>> import tempfile\n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     # Write a DataFrame into a Parquet file\n",
            " |      ...     spark.createDataFrame(\n",
            " |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
            " |      ...     ).write.mode(\"overwrite\").format(\"parquet\").save(d)\n",
            " |      ...\n",
            " |      ...     # Read the Parquet file as a DataFrame.\n",
            " |      ...     spark.read.parquet(d).show()\n",
            " |      +---+------------+\n",
            " |      |age|        name|\n",
            " |      +---+------------+\n",
            " |      |100|Hyukjin Kwon|\n",
            " |      +---+------------+\n",
            " |  \n",
            " |  schema(self, schema: Union[pyspark.sql.types.StructType, str]) -> 'DataFrameReader'\n",
            " |      Specifies the input schema.\n",
            " |      \n",
            " |      Some data sources (e.g. JSON) can infer the input schema automatically from data.\n",
            " |      By specifying the schema here, the underlying data source can skip the schema\n",
            " |      inference step, and thus speed up data loading.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      schema : :class:`pyspark.sql.types.StructType` or str\n",
            " |          a :class:`pyspark.sql.types.StructType` object or a DDL-formatted string\n",
            " |          (For example ``col0 INT, col1 DOUBLE``).\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> spark.read.schema(\"col0 INT, col1 DOUBLE\")\n",
            " |      <...readwriter.DataFrameReader object ...>\n",
            " |      \n",
            " |      Specify the schema with reading a CSV file.\n",
            " |      \n",
            " |      >>> import tempfile\n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     spark.read.schema(\"col0 INT, col1 DOUBLE\").format(\"csv\").load(d).printSchema()\n",
            " |      root\n",
            " |       |-- col0: integer (nullable = true)\n",
            " |       |-- col1: double (nullable = true)\n",
            " |  \n",
            " |  table(self, tableName: str) -> 'DataFrame'\n",
            " |      Returns the specified table as a :class:`DataFrame`.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      tableName : str\n",
            " |          string, name of the table.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.range(10)\n",
            " |      >>> df.createOrReplaceTempView('tblA')\n",
            " |      >>> spark.read.table('tblA').show()\n",
            " |      +---+\n",
            " |      | id|\n",
            " |      +---+\n",
            " |      |  0|\n",
            " |      |  1|\n",
            " |      |  2|\n",
            " |      |  3|\n",
            " |      |  4|\n",
            " |      |  5|\n",
            " |      |  6|\n",
            " |      |  7|\n",
            " |      |  8|\n",
            " |      |  9|\n",
            " |      +---+\n",
            " |      >>> _ = spark.sql(\"DROP TABLE tblA\")\n",
            " |  \n",
            " |  text(self, paths: Union[str, List[str]], wholetext: bool = False, lineSep: Optional[str] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None) -> 'DataFrame'\n",
            " |      Loads text files and returns a :class:`DataFrame` whose schema starts with a\n",
            " |      string column named \"value\", and followed by partitioned columns if there\n",
            " |      are any.\n",
            " |      The text files must be encoded as UTF-8.\n",
            " |      \n",
            " |      By default, each line in the text file is a new row in the resulting DataFrame.\n",
            " |      \n",
            " |      .. versionadded:: 1.6.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      paths : str or list\n",
            " |          string, or list of strings, for input path(s).\n",
            " |      \n",
            " |      Other Parameters\n",
            " |      ----------------\n",
            " |      Extra options\n",
            " |          For the extra options, refer to\n",
            " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-text.html#data-source-option>`_\n",
            " |          for the version you use.\n",
            " |      \n",
            " |          .. # noqa\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      Write a DataFrame into a text file and read it back.\n",
            " |      \n",
            " |      >>> import tempfile\n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     # Write a DataFrame into a text file\n",
            " |      ...     df = spark.createDataFrame([(\"a\",), (\"b\",), (\"c\",)], schema=[\"alphabets\"])\n",
            " |      ...     df.write.mode(\"overwrite\").format(\"text\").save(d)\n",
            " |      ...\n",
            " |      ...     # Read the text file as a DataFrame.\n",
            " |      ...     spark.read.schema(df.schema).text(d).sort(\"alphabets\").show()\n",
            " |      +---------+\n",
            " |      |alphabets|\n",
            " |      +---------+\n",
            " |      |        a|\n",
            " |      |        b|\n",
            " |      |        c|\n",
            " |      +---------+\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from OptionUtils:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "source": [
        "df = pd.read_csv('/content/results of dk 777 on bundle inputs  1.csv')"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "7K5adsCHotL1",
        "outputId": "555a298a-2498-49d8-af09-77c76b9d67f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-11f944ec08f5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/results of dk 777 on bundle inputs  1.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install pandas==1.5.3\n",
        "import pandas as pd"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "Ych6qvVmo0bP",
        "outputId": "aae70415-6ef8-4a67-c2ef-6f534e56e1eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas==1.5.3\n",
            "  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3) (2023.4)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas==1.5.3) (1.16.0)\n",
            "Installing collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.0.3\n",
            "    Uninstalling pandas-2.0.3:\n",
            "      Successfully uninstalled pandas-2.0.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pandas<2.2.2dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.0.3, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-1.5.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              },
              "id": "d6bbcb3710e5457e867e48f98644e0d7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"BostonHousePrices\").getOrCreate()\n",
        "\n",
        "# Read the CSV file using the correct path for Colab\n",
        "file_path = \"/content/results of dk 777 on bundle inputs  1.csv\"\n",
        "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5sCCydjtiT6",
        "outputId": "6cdde1a2-42d9-4271-cc76-0e23dec22bb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+-----+-----+------+----+------+---------+-----+------+-------+------+------+------+------+\n",
            "| 1834|-21680|-7866|40431|DK 777|  kg|0.0000|7085.0000|0.000|1.0009|1.00010|NULL11|NULL12|NULL13|NULL14|\n",
            "+-----+------+-----+-----+------+----+------+---------+-----+------+-------+------+------+------+------+\n",
            "| 1834|-17092|-5863|40431|DK 777|  kg|   0.0|  13200.0|  0.0|   2.0|    1.0|  NULL|  NULL|  NULL|  NULL|\n",
            "| 2404|-11326|-2675|40431|DK 777|unit|   0.0|    510.0|  0.0|   2.0|    1.0|  NULL|  NULL|  NULL|  NULL|\n",
            "| 2834|-21680|-7866|40431|DK 777|  kg|   0.0|   7085.0|  0.0|   1.0|    1.0|  NULL|  NULL|  NULL|  NULL|\n",
            "| 2834|-17092|-5863|40431|DK 777|  kg|   0.0|  13200.0|  0.0|   2.0|    1.0|  NULL|  NULL|  NULL|  NULL|\n",
            "| 3834|-21680|-7866|40431|DK 777|  kg|   0.0|   7085.0|  0.0|   1.0|    1.0|  NULL|  NULL|  NULL|  NULL|\n",
            "| 3834|-17092|-5863|40431|DK 777|  kg|   0.0|  13200.0|  0.0|   2.0|    1.0|  NULL|  NULL|  NULL|  NULL|\n",
            "| 4834|-21680|-7866|40431|DK 777|  kg|   0.0|   7085.0|  0.0|   1.0|    1.0|  NULL|  NULL|  NULL|  NULL|\n",
            "| 4834|-17092|-5863|40431|DK 777|  kg|   0.0|  13200.0|  0.0|   2.0|    1.0|  NULL|  NULL|  NULL|  NULL|\n",
            "| 5834|-21680|-7866|40431|DK 777|  kg|   0.0|   7085.0|  0.0|   1.0|    1.0|  NULL|  NULL|  NULL|  NULL|\n",
            "| 5834|-17092|-5863|40431|DK 777|  kg|   0.0|  13200.0|  0.0|   2.0|    1.0|  NULL|  NULL|  NULL|  NULL|\n",
            "| 6834|-21680|-7866|40431|DK 777|  kg|   0.0|   7085.0|  0.0|   1.0|    1.0|  NULL|  NULL|  NULL|  NULL|\n",
            "| 6834|-17092|-5863|40431|DK 777|  kg|   0.0|  13200.0|  0.0|   2.0|    1.0|  NULL|  NULL|  NULL|  NULL|\n",
            "| 7834|-21680|-7866|40431|DK 777|  kg|   0.0|   7085.0|  0.0|   1.0|    1.0|  NULL|  NULL|  NULL|  NULL|\n",
            "| 7834|-17092|-5863|40431|DK 777|  kg|   0.0|  13200.0|  0.0|   2.0|    1.0|  NULL|  NULL|  NULL|  NULL|\n",
            "| 8834|-21680|-7866|40431|DK 777|  kg|   0.0|   7085.0|  0.0|   1.0|    1.0|  NULL|  NULL|  NULL|  NULL|\n",
            "| 8834|-17092|-5863|40431|DK 777|  kg|   0.0|  13200.0|  0.0|   2.0|    1.0|  NULL|  NULL|  NULL|  NULL|\n",
            "| 9834|-21680|-7866|40431|DK 777|  kg|   0.0|   6540.0|  0.0|   1.0|    1.0|  NULL|  NULL|  NULL|  NULL|\n",
            "| 9834|-17092|-5863|40431|DK 777|  kg|   0.0|  13200.0|  0.0|   2.0|    1.0|  NULL|  NULL|  NULL|  NULL|\n",
            "|10834|-21680|-7866|40431|DK 777|  kg|   0.0|   6540.0|  0.0|   1.0|    1.0|  NULL|  NULL|  NULL|  NULL|\n",
            "|10834|-17092|-5863|40431|DK 777|  kg|   0.0|  13200.0|  0.0|   2.0|    1.0|  NULL|  NULL|  NULL|  NULL|\n",
            "+-----+------+-----+-----+------+----+------+---------+-----+------+-------+------+------+------+------+\n",
            "only showing top 20 rows\n",
            "\n",
            "root\n",
            " |-- 1834: integer (nullable = true)\n",
            " |-- -21680: integer (nullable = true)\n",
            " |-- -7866: integer (nullable = true)\n",
            " |-- 40431: integer (nullable = true)\n",
            " |-- DK 777: string (nullable = true)\n",
            " |-- kg: string (nullable = true)\n",
            " |-- 0.0000: double (nullable = true)\n",
            " |-- 7085.0000: double (nullable = true)\n",
            " |-- 0.000: double (nullable = true)\n",
            " |-- 1.0009: double (nullable = true)\n",
            " |-- 1.00010: double (nullable = true)\n",
            " |-- NULL11: string (nullable = true)\n",
            " |-- NULL12: string (nullable = true)\n",
            " |-- NULL13: string (nullable = true)\n",
            " |-- NULL14: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "source": [
        "df = spark.read.csv(path='/content/population_total.csv', header=True, inferSchema=True)\n",
        "display(df)\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "CVxkh0RSpJQ1",
        "outputId": "8af6016b-804e-49be-c3a8-cef88e52616c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[country: string, year: int, population: int]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+----------+\n",
            "|country|year|population|\n",
            "+-------+----+----------+\n",
            "|  China|2020|1439323776|\n",
            "|  China|2019|1433783686|\n",
            "|  China|2018|1427647786|\n",
            "|  China|2017|1421021791|\n",
            "|  China|2016|1414049351|\n",
            "|  China|2015|1406847870|\n",
            "|  China|2010|1368810615|\n",
            "|  China|2005|1330776380|\n",
            "|  China|2000|1290550765|\n",
            "|  China|1995|1240920535|\n",
            "|  China|1990|1176883674|\n",
            "|  China|1985|1075589361|\n",
            "|  China|1980|1000089235|\n",
            "|  China|1975| 926240885|\n",
            "|  China|1970| 827601394|\n",
            "|  China|1965| 724218968|\n",
            "|  China|1960| 660408056|\n",
            "|  China|1955| 612241554|\n",
            "| France|2020|  65273511|\n",
            "| France|2019|  65129728|\n",
            "+-------+----+----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "root\n",
            " |-- country: string (nullable = true)\n",
            " |-- year: integer (nullable = true)\n",
            " |-- population: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"/gdp.csv\").getOrCreate()\n",
        "\n",
        "# Read the CSV file using the correct path for Colab\n",
        "file_path = \"/gdp.csv\"\n",
        "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "HVnAzaxZxg1l",
        "outputId": "01e1c05d-8a8f-4d9a-abb7-1dcaf3d2786f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pyspark'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-225e3fe9a528>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Initialize SparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/gdp.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "source": [
        "!pip install pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"/gdp.csv\").getOrCreate()\n",
        "\n",
        "# Read the CSV file using the correct path for Colab\n",
        "file_path = \"/gdp.csv\"\n",
        "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9pcj28Ax-YY",
        "outputId": "b53b305f-00bc-4bef-d477-e95d017c6863"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=5230cc555a81c6d0eeff0f8ecf7e4c4066dc1c987a16471d7d51fa24222d1215\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n",
            "+----+----+-----------+----+---------+---------+-----------+------------+\n",
            "|unid|wbid|    country|year|      SES|    gdppc|    yrseduc|    popshare|\n",
            "+----+----+-----------+----+---------+---------+-----------+------------+\n",
            "|   4| AFG|Afghanistan|1970|3.4742124|    709.0|       NULL|0.0030974303|\n",
            "|   4| AFG|Afghanistan|1920|26.968016|731.75677|       NULL|0.0032446014|\n",
            "|   4| AFG|Afghanistan|1990|1.2695296|    604.0|       NULL| 0.002346751|\n",
            "|   4| AFG|Afghanistan|1960|15.763076|    739.0|       NULL| 0.003039381|\n",
            "|   4| AFG|Afghanistan|2000|2.0611143|    565.0|       NULL|0.0033088236|\n",
            "|   4| AFG|Afghanistan|2010|5.6763997|1662.8035|       NULL|0.0041496116|\n",
            "|   4| AFG|Afghanistan|1880|37.957447|585.46509|       NULL|0.0032711735|\n",
            "|   4| AFG|Afghanistan|1940|14.320977|673.91895|       NULL|0.0032264683|\n",
            "|   4| AFG|Afghanistan|1890|29.591391|635.93024|       NULL|0.0032554974|\n",
            "|   4| AFG|Afghanistan|1980|3.4650476|    690.0|       NULL|0.0030573066|\n",
            "|   4| AFG|Afghanistan|1930|15.306766|702.83783|       NULL|0.0032587289|\n",
            "|   4| AFG|Afghanistan|1950|23.424145|    645.0|       NULL|0.0033020985|\n",
            "|   4| AFG|Afghanistan|1900|28.104797|686.39532|       NULL|0.0032445015|\n",
            "|   4| AFG|Afghanistan|1910|  26.9876|736.86047|       NULL|0.0031784344|\n",
            "|  24| AGO|     Angola|2010|21.247763|6492.1768|       2.79|0.0031490561|\n",
            "|  24| AGO|     Angola|1930|24.175644|747.81482|0.079999998| 0.001587122|\n",
            "|  24| AGO|     Angola|1920|24.223469|682.62964|0.079999998|0.0015237019|\n",
            "|  24| AGO|     Angola|1970|30.052431|   1768.0| 0.28999999|0.0017549359|\n",
            "|  24| AGO|     Angola|1960|27.918711|   1253.0|       0.11|0.0017810419|\n",
            "|  24| AGO|     Angola|1950| 28.05554|   1052.0|0.090000004| 0.001668241|\n",
            "+----+----+-----------+----+---------+---------+-----------+------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "root\n",
            " |-- unid: integer (nullable = true)\n",
            " |-- wbid: string (nullable = true)\n",
            " |-- country: string (nullable = true)\n",
            " |-- year: integer (nullable = true)\n",
            " |-- SES: double (nullable = true)\n",
            " |-- gdppc: double (nullable = true)\n",
            " |-- yrseduc: double (nullable = true)\n",
            " |-- popshare: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import dataframe\n",
        "help(dataframe.DataFrame.write)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oHqxgRs1V6k",
        "outputId": "728a4d48-f5c8-4f1c-d2ff-feca6a0baa3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on module pyspark.sql.dataframe in pyspark.sql:\n",
            "\n",
            "NAME\n",
            "    pyspark.sql.dataframe\n",
            "\n",
            "DESCRIPTION\n",
            "    # Licensed to the Apache Software Foundation (ASF) under one or more\n",
            "    # contributor license agreements.  See the NOTICE file distributed with\n",
            "    # this work for additional information regarding copyright ownership.\n",
            "    # The ASF licenses this file to You under the Apache License, Version 2.0\n",
            "    # (the \"License\"); you may not use this file except in compliance with\n",
            "    # the License.  You may obtain a copy of the License at\n",
            "    #\n",
            "    #    http://www.apache.org/licenses/LICENSE-2.0\n",
            "    #\n",
            "    # Unless required by applicable law or agreed to in writing, software\n",
            "    # distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "    # See the License for the specific language governing permissions and\n",
            "    # limitations under the License.\n",
            "    #\n",
            "\n",
            "CLASSES\n",
            "    builtins.object\n",
            "        DataFrameNaFunctions\n",
            "        DataFrameStatFunctions\n",
            "    pyspark.sql.pandas.conversion.PandasConversionMixin(builtins.object)\n",
            "        DataFrame(pyspark.sql.pandas.map_ops.PandasMapOpsMixin, pyspark.sql.pandas.conversion.PandasConversionMixin)\n",
            "    pyspark.sql.pandas.map_ops.PandasMapOpsMixin(builtins.object)\n",
            "        DataFrame(pyspark.sql.pandas.map_ops.PandasMapOpsMixin, pyspark.sql.pandas.conversion.PandasConversionMixin)\n",
            "    \n",
            "    class DataFrame(pyspark.sql.pandas.map_ops.PandasMapOpsMixin, pyspark.sql.pandas.conversion.PandasConversionMixin)\n",
            "     |  DataFrame(jdf: py4j.java_gateway.JavaObject, sql_ctx: Union[ForwardRef('SQLContext'), ForwardRef('SparkSession')])\n",
            "     |  \n",
            "     |  A distributed collection of data grouped into named columns.\n",
            "     |  \n",
            "     |  .. versionadded:: 1.3.0\n",
            "     |  \n",
            "     |  .. versionchanged:: 3.4.0\n",
            "     |      Supports Spark Connect.\n",
            "     |  \n",
            "     |  Examples\n",
            "     |  --------\n",
            "     |  A :class:`DataFrame` is equivalent to a relational table in Spark SQL,\n",
            "     |  and can be created using various functions in :class:`SparkSession`:\n",
            "     |  \n",
            "     |  >>> people = spark.createDataFrame([\n",
            "     |  ...     {\"deptId\": 1, \"age\": 40, \"name\": \"Hyukjin Kwon\", \"gender\": \"M\", \"salary\": 50},\n",
            "     |  ...     {\"deptId\": 1, \"age\": 50, \"name\": \"Takuya Ueshin\", \"gender\": \"M\", \"salary\": 100},\n",
            "     |  ...     {\"deptId\": 2, \"age\": 60, \"name\": \"Xinrong Meng\", \"gender\": \"F\", \"salary\": 150},\n",
            "     |  ...     {\"deptId\": 3, \"age\": 20, \"name\": \"Haejoon Lee\", \"gender\": \"M\", \"salary\": 200}\n",
            "     |  ... ])\n",
            "     |  \n",
            "     |  Once created, it can be manipulated using the various domain-specific-language\n",
            "     |  (DSL) functions defined in: :class:`DataFrame`, :class:`Column`.\n",
            "     |  \n",
            "     |  To select a column from the :class:`DataFrame`, use the apply method:\n",
            "     |  \n",
            "     |  >>> age_col = people.age\n",
            "     |  \n",
            "     |  A more concrete example:\n",
            "     |  \n",
            "     |  >>> # To create DataFrame using SparkSession\n",
            "     |  ... department = spark.createDataFrame([\n",
            "     |  ...     {\"id\": 1, \"name\": \"PySpark\"},\n",
            "     |  ...     {\"id\": 2, \"name\": \"ML\"},\n",
            "     |  ...     {\"id\": 3, \"name\": \"Spark SQL\"}\n",
            "     |  ... ])\n",
            "     |  \n",
            "     |  >>> people.filter(people.age > 30).join(\n",
            "     |  ...     department, people.deptId == department.id).groupBy(\n",
            "     |  ...     department.name, \"gender\").agg({\"salary\": \"avg\", \"age\": \"max\"}).show()\n",
            "     |  +-------+------+-----------+--------+\n",
            "     |  |   name|gender|avg(salary)|max(age)|\n",
            "     |  +-------+------+-----------+--------+\n",
            "     |  |     ML|     F|      150.0|      60|\n",
            "     |  |PySpark|     M|       75.0|      50|\n",
            "     |  +-------+------+-----------+--------+\n",
            "     |  \n",
            "     |  Notes\n",
            "     |  -----\n",
            "     |  A DataFrame should only be created as described above. It should not be directly\n",
            "     |  created via using the constructor.\n",
            "     |  \n",
            "     |  Method resolution order:\n",
            "     |      DataFrame\n",
            "     |      pyspark.sql.pandas.map_ops.PandasMapOpsMixin\n",
            "     |      pyspark.sql.pandas.conversion.PandasConversionMixin\n",
            "     |      builtins.object\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __dir__(self) -> List[str]\n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> from pyspark.sql.functions import lit\n",
            "     |      \n",
            "     |      Create a dataframe with a column named 'id'.\n",
            "     |      \n",
            "     |      >>> df = spark.range(3)\n",
            "     |      >>> [attr for attr in dir(df) if attr[0] == 'i'][:7] # Includes column id\n",
            "     |      ['id', 'inputFiles', 'intersect', 'intersectAll', 'isEmpty', 'isLocal', 'isStreaming']\n",
            "     |      \n",
            "     |      Add a column named 'i_like_pancakes'.\n",
            "     |      \n",
            "     |      >>> df = df.withColumn('i_like_pancakes', lit(1))\n",
            "     |      >>> [attr for attr in dir(df) if attr[0] == 'i'][:7] # Includes columns i_like_pancakes, id\n",
            "     |      ['i_like_pancakes', 'id', 'inputFiles', 'intersect', 'intersectAll', 'isEmpty', 'isLocal']\n",
            "     |      \n",
            "     |      Try to add an existed column 'inputFiles'.\n",
            "     |      \n",
            "     |      >>> df = df.withColumn('inputFiles', lit(2))\n",
            "     |      >>> [attr for attr in dir(df) if attr[0] == 'i'][:7] # Doesn't duplicate inputFiles\n",
            "     |      ['i_like_pancakes', 'id', 'inputFiles', 'intersect', 'intersectAll', 'isEmpty', 'isLocal']\n",
            "     |      \n",
            "     |      Try to add a column named 'id2'.\n",
            "     |      \n",
            "     |      >>> df = df.withColumn('id2', lit(3))\n",
            "     |      >>> [attr for attr in dir(df) if attr[0] == 'i'][:7] # result includes id2 and sorted\n",
            "     |      ['i_like_pancakes', 'id', 'id2', 'inputFiles', 'intersect', 'intersectAll', 'isEmpty']\n",
            "     |      \n",
            "     |      Don't include columns that are not valid python identifiers.\n",
            "     |      \n",
            "     |      >>> df = df.withColumn('1', lit(4))\n",
            "     |      >>> df = df.withColumn('name 1', lit(5))\n",
            "     |      >>> [attr for attr in dir(df) if attr[0] == 'i'][:7] # Doesn't include 1 or name 1\n",
            "     |      ['i_like_pancakes', 'id', 'id2', 'inputFiles', 'intersect', 'intersectAll', 'isEmpty']\n",
            "     |  \n",
            "     |  __getattr__(self, name: str) -> pyspark.sql.column.Column\n",
            "     |      Returns the :class:`Column` denoted by ``name``.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.3.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      name : str\n",
            "     |          Column name to return as :class:`Column`.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`Column`\n",
            "     |          Requested column.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame([\n",
            "     |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            "     |      \n",
            "     |      Retrieve a column instance.\n",
            "     |      \n",
            "     |      >>> df.select(df.age).show()\n",
            "     |      +---+\n",
            "     |      |age|\n",
            "     |      +---+\n",
            "     |      |  2|\n",
            "     |      |  5|\n",
            "     |      +---+\n",
            "     |  \n",
            "     |  __getitem__(self, item: Union[int, str, pyspark.sql.column.Column, List, Tuple]) -> Union[pyspark.sql.column.Column, ForwardRef('DataFrame')]\n",
            "     |      Returns the column as a :class:`Column`.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.3.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      item : int, str, :class:`Column`, list or tuple\n",
            "     |          column index, column name, column, or a list or tuple of columns\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`Column` or :class:`DataFrame`\n",
            "     |          a specified column, or a filtered or projected dataframe.\n",
            "     |      \n",
            "     |          * If the input `item` is an int or str, the output is a :class:`Column`.\n",
            "     |      \n",
            "     |          * If the input `item` is a :class:`Column`, the output is a :class:`DataFrame`\n",
            "     |              filtered by this given :class:`Column`.\n",
            "     |      \n",
            "     |          * If the input `item` is a list or tuple, the output is a :class:`DataFrame`\n",
            "     |              projected by this given list or tuple.\n",
            "     |      \n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame([\n",
            "     |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            "     |      \n",
            "     |      Retrieve a column instance.\n",
            "     |      \n",
            "     |      >>> df.select(df['age']).show()\n",
            "     |      +---+\n",
            "     |      |age|\n",
            "     |      +---+\n",
            "     |      |  2|\n",
            "     |      |  5|\n",
            "     |      +---+\n",
            "     |      \n",
            "     |      >>> df.select(df[1]).show()\n",
            "     |      +-----+\n",
            "     |      | name|\n",
            "     |      +-----+\n",
            "     |      |Alice|\n",
            "     |      |  Bob|\n",
            "     |      +-----+\n",
            "     |      \n",
            "     |      Select multiple string columns as index.\n",
            "     |      \n",
            "     |      >>> df[[\"name\", \"age\"]].show()\n",
            "     |      +-----+---+\n",
            "     |      | name|age|\n",
            "     |      +-----+---+\n",
            "     |      |Alice|  2|\n",
            "     |      |  Bob|  5|\n",
            "     |      +-----+---+\n",
            "     |      >>> df[df.age > 3].show()\n",
            "     |      +---+----+\n",
            "     |      |age|name|\n",
            "     |      +---+----+\n",
            "     |      |  5| Bob|\n",
            "     |      +---+----+\n",
            "     |      >>> df[df[0] > 3].show()\n",
            "     |      +---+----+\n",
            "     |      |age|name|\n",
            "     |      +---+----+\n",
            "     |      |  5| Bob|\n",
            "     |      +---+----+\n",
            "     |  \n",
            "     |  __init__(self, jdf: py4j.java_gateway.JavaObject, sql_ctx: Union[ForwardRef('SQLContext'), ForwardRef('SparkSession')])\n",
            "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
            "     |  \n",
            "     |  __repr__(self) -> str\n",
            "     |      Return repr(self).\n",
            "     |  \n",
            "     |  agg(self, *exprs: Union[pyspark.sql.column.Column, Dict[str, str]]) -> 'DataFrame'\n",
            "     |      Aggregate on the entire :class:`DataFrame` without groups\n",
            "     |      (shorthand for ``df.groupBy().agg()``).\n",
            "     |      \n",
            "     |      .. versionadded:: 1.3.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      exprs : :class:`Column` or dict of key and value strings\n",
            "     |          Columns or expressions to aggregate DataFrame by.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          Aggregated DataFrame.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> from pyspark.sql import functions as sf\n",
            "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            "     |      >>> df.agg({\"age\": \"max\"}).show()\n",
            "     |      +--------+\n",
            "     |      |max(age)|\n",
            "     |      +--------+\n",
            "     |      |       5|\n",
            "     |      +--------+\n",
            "     |      >>> df.agg(sf.min(df.age)).show()\n",
            "     |      +--------+\n",
            "     |      |min(age)|\n",
            "     |      +--------+\n",
            "     |      |       2|\n",
            "     |      +--------+\n",
            "     |  \n",
            "     |  alias(self, alias: str) -> 'DataFrame'\n",
            "     |      Returns a new :class:`DataFrame` with an alias set.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.3.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      alias : str\n",
            "     |          an alias name to be set for the :class:`DataFrame`.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          Aliased DataFrame.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> from pyspark.sql.functions import col, desc\n",
            "     |      >>> df = spark.createDataFrame(\n",
            "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            "     |      >>> df_as1 = df.alias(\"df_as1\")\n",
            "     |      >>> df_as2 = df.alias(\"df_as2\")\n",
            "     |      >>> joined_df = df_as1.join(df_as2, col(\"df_as1.name\") == col(\"df_as2.name\"), 'inner')\n",
            "     |      >>> joined_df.select(\n",
            "     |      ...     \"df_as1.name\", \"df_as2.name\", \"df_as2.age\").sort(desc(\"df_as1.name\")).show()\n",
            "     |      +-----+-----+---+\n",
            "     |      | name| name|age|\n",
            "     |      +-----+-----+---+\n",
            "     |      |  Tom|  Tom| 14|\n",
            "     |      |  Bob|  Bob| 16|\n",
            "     |      |Alice|Alice| 23|\n",
            "     |      +-----+-----+---+\n",
            "     |  \n",
            "     |  approxQuantile(self, col: Union[str, List[str], Tuple[str]], probabilities: Union[List[float], Tuple[float]], relativeError: float) -> Union[List[float], List[List[float]]]\n",
            "     |      Calculates the approximate quantiles of numerical columns of a\n",
            "     |      :class:`DataFrame`.\n",
            "     |      \n",
            "     |      The result of this algorithm has the following deterministic bound:\n",
            "     |      If the :class:`DataFrame` has N elements and if we request the quantile at\n",
            "     |      probability `p` up to error `err`, then the algorithm will return\n",
            "     |      a sample `x` from the :class:`DataFrame` so that the *exact* rank of `x` is\n",
            "     |      close to (p * N). More precisely,\n",
            "     |      \n",
            "     |        floor((p - err) * N) <= rank(x) <= ceil((p + err) * N).\n",
            "     |      \n",
            "     |      This method implements a variation of the Greenwald-Khanna\n",
            "     |      algorithm (with some speed optimizations). The algorithm was first\n",
            "     |      present in [[https://doi.org/10.1145/375663.375670\n",
            "     |      Space-efficient Online Computation of Quantile Summaries]]\n",
            "     |      by Greenwald and Khanna.\n",
            "     |      \n",
            "     |      .. versionadded:: 2.0.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      col: str, tuple or list\n",
            "     |          Can be a single column name, or a list of names for multiple columns.\n",
            "     |      \n",
            "     |          .. versionchanged:: 2.2.0\n",
            "     |             Added support for multiple columns.\n",
            "     |      probabilities : list or tuple\n",
            "     |          a list of quantile probabilities\n",
            "     |          Each number must belong to [0, 1].\n",
            "     |          For example 0 is the minimum, 0.5 is the median, 1 is the maximum.\n",
            "     |      relativeError : float\n",
            "     |          The relative target precision to achieve\n",
            "     |          (>= 0). If set to zero, the exact quantiles are computed, which\n",
            "     |          could be very expensive. Note that values greater than 1 are\n",
            "     |          accepted but gives the same result as 1.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      list\n",
            "     |          the approximate quantiles at the given probabilities.\n",
            "     |      \n",
            "     |          * If the input `col` is a string, the output is a list of floats.\n",
            "     |      \n",
            "     |          * If the input `col` is a list or tuple of strings, the output is also a\n",
            "     |              list, but each element in it is a list of floats, i.e., the output\n",
            "     |              is a list of list of floats.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      Null values will be ignored in numerical columns before calculation.\n",
            "     |      For columns only containing null values, an empty list is returned.\n",
            "     |  \n",
            "     |  cache(self) -> 'DataFrame'\n",
            "     |      Persists the :class:`DataFrame` with the default storage level (`MEMORY_AND_DISK_DESER`).\n",
            "     |      \n",
            "     |      .. versionadded:: 1.3.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      The default storage level has changed to `MEMORY_AND_DISK_DESER` to match Scala in 3.0.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          Cached DataFrame.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.range(1)\n",
            "     |      >>> df.cache()\n",
            "     |      DataFrame[id: bigint]\n",
            "     |      \n",
            "     |      >>> df.explain()\n",
            "     |      == Physical Plan ==\n",
            "     |      AdaptiveSparkPlan isFinalPlan=false\n",
            "     |      +- InMemoryTableScan ...\n",
            "     |  \n",
            "     |  checkpoint(self, eager: bool = True) -> 'DataFrame'\n",
            "     |      Returns a checkpointed version of this :class:`DataFrame`. Checkpointing can be used to\n",
            "     |      truncate the logical plan of this :class:`DataFrame`, which is especially useful in\n",
            "     |      iterative algorithms where the plan may grow exponentially. It will be saved to files\n",
            "     |      inside the checkpoint directory set with :meth:`SparkContext.setCheckpointDir`.\n",
            "     |      \n",
            "     |      .. versionadded:: 2.1.0\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      eager : bool, optional, default True\n",
            "     |          Whether to checkpoint this :class:`DataFrame` immediately.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          Checkpointed DataFrame.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      This API is experimental.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> import tempfile\n",
            "     |      >>> df = spark.createDataFrame([\n",
            "     |      ...     (14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
            "     |      ...     spark.sparkContext.setCheckpointDir(\"/tmp/bb\")\n",
            "     |      ...     df.checkpoint(False)\n",
            "     |      DataFrame[age: bigint, name: string]\n",
            "     |  \n",
            "     |  coalesce(self, numPartitions: int) -> 'DataFrame'\n",
            "     |      Returns a new :class:`DataFrame` that has exactly `numPartitions` partitions.\n",
            "     |      \n",
            "     |      Similar to coalesce defined on an :class:`RDD`, this operation results in a\n",
            "     |      narrow dependency, e.g. if you go from 1000 partitions to 100 partitions,\n",
            "     |      there will not be a shuffle, instead each of the 100 new partitions will\n",
            "     |      claim 10 of the current partitions. If a larger number of partitions is requested,\n",
            "     |      it will stay at the current number of partitions.\n",
            "     |      \n",
            "     |      However, if you're doing a drastic coalesce, e.g. to numPartitions = 1,\n",
            "     |      this may result in your computation taking place on fewer nodes than\n",
            "     |      you like (e.g. one node in the case of numPartitions = 1). To avoid this,\n",
            "     |      you can call repartition(). This will add a shuffle step, but means the\n",
            "     |      current upstream partitions will be executed in parallel (per whatever\n",
            "     |      the current partitioning is).\n",
            "     |      \n",
            "     |      .. versionadded:: 1.4.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      numPartitions : int\n",
            "     |          specify the target number of partitions\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.range(10)\n",
            "     |      >>> df.coalesce(1).rdd.getNumPartitions()\n",
            "     |      1\n",
            "     |  \n",
            "     |  colRegex(self, colName: str) -> pyspark.sql.column.Column\n",
            "     |      Selects column based on the column name specified as a regex and returns it\n",
            "     |      as :class:`Column`.\n",
            "     |      \n",
            "     |      .. versionadded:: 2.3.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      colName : str\n",
            "     |          string, column name specified as a regex.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`Column`\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame([(\"a\", 1), (\"b\", 2), (\"c\",  3)], [\"Col1\", \"Col2\"])\n",
            "     |      >>> df.select(df.colRegex(\"`(Col1)?+.+`\")).show()\n",
            "     |      +----+\n",
            "     |      |Col2|\n",
            "     |      +----+\n",
            "     |      |   1|\n",
            "     |      |   2|\n",
            "     |      |   3|\n",
            "     |      +----+\n",
            "     |  \n",
            "     |  collect(self) -> List[pyspark.sql.types.Row]\n",
            "     |      Returns all the records as a list of :class:`Row`.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.3.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      list\n",
            "     |          List of rows.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame(\n",
            "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            "     |      >>> df.collect()\n",
            "     |      [Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\n",
            "     |  \n",
            "     |  corr(self, col1: str, col2: str, method: Optional[str] = None) -> float\n",
            "     |      Calculates the correlation of two columns of a :class:`DataFrame` as a double value.\n",
            "     |      Currently only supports the Pearson Correlation Coefficient.\n",
            "     |      :func:`DataFrame.corr` and :func:`DataFrameStatFunctions.corr` are aliases of each other.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.4.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      col1 : str\n",
            "     |          The name of the first column\n",
            "     |      col2 : str\n",
            "     |          The name of the second column\n",
            "     |      method : str, optional\n",
            "     |          The correlation method. Currently only supports \"pearson\"\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      float\n",
            "     |          Pearson Correlation Coefficient of two columns.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame([(1, 12), (10, 1), (19, 8)], [\"c1\", \"c2\"])\n",
            "     |      >>> df.corr(\"c1\", \"c2\")\n",
            "     |      -0.3592106040535498\n",
            "     |      >>> df = spark.createDataFrame([(11, 12), (10, 11), (9, 10)], [\"small\", \"bigger\"])\n",
            "     |      >>> df.corr(\"small\", \"bigger\")\n",
            "     |      1.0\n",
            "     |  \n",
            "     |  count(self) -> int\n",
            "     |      Returns the number of rows in this :class:`DataFrame`.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.3.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      int\n",
            "     |          Number of rows.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame(\n",
            "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            "     |      \n",
            "     |      Return the number of rows in the :class:`DataFrame`.\n",
            "     |      \n",
            "     |      >>> df.count()\n",
            "     |      3\n",
            "     |  \n",
            "     |  cov(self, col1: str, col2: str) -> float\n",
            "     |      Calculate the sample covariance for the given columns, specified by their names, as a\n",
            "     |      double value. :func:`DataFrame.cov` and :func:`DataFrameStatFunctions.cov` are aliases.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.4.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      col1 : str\n",
            "     |          The name of the first column\n",
            "     |      col2 : str\n",
            "     |          The name of the second column\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      float\n",
            "     |          Covariance of two columns.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame([(1, 12), (10, 1), (19, 8)], [\"c1\", \"c2\"])\n",
            "     |      >>> df.cov(\"c1\", \"c2\")\n",
            "     |      -18.0\n",
            "     |      >>> df = spark.createDataFrame([(11, 12), (10, 11), (9, 10)], [\"small\", \"bigger\"])\n",
            "     |      >>> df.cov(\"small\", \"bigger\")\n",
            "     |      1.0\n",
            "     |  \n",
            "     |  createGlobalTempView(self, name: str) -> None\n",
            "     |      Creates a global temporary view with this :class:`DataFrame`.\n",
            "     |      \n",
            "     |      The lifetime of this temporary view is tied to this Spark application.\n",
            "     |      throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the\n",
            "     |      catalog.\n",
            "     |      \n",
            "     |      .. versionadded:: 2.1.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      name : str\n",
            "     |          Name of the view.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      Create a global temporary view.\n",
            "     |      \n",
            "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            "     |      >>> df.createGlobalTempView(\"people\")\n",
            "     |      >>> df2 = spark.sql(\"SELECT * FROM global_temp.people\")\n",
            "     |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
            "     |      True\n",
            "     |      \n",
            "     |      Throws an exception if the global temporary view already exists.\n",
            "     |      \n",
            "     |      >>> df.createGlobalTempView(\"people\")  # doctest: +IGNORE_EXCEPTION_DETAIL\n",
            "     |      Traceback (most recent call last):\n",
            "     |      ...\n",
            "     |      AnalysisException: \"Temporary table 'people' already exists;\"\n",
            "     |      >>> spark.catalog.dropGlobalTempView(\"people\")\n",
            "     |      True\n",
            "     |  \n",
            "     |  createOrReplaceGlobalTempView(self, name: str) -> None\n",
            "     |      Creates or replaces a global temporary view using the given name.\n",
            "     |      \n",
            "     |      The lifetime of this temporary view is tied to this Spark application.\n",
            "     |      \n",
            "     |      .. versionadded:: 2.2.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      name : str\n",
            "     |          Name of the view.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      Create a global temporary view.\n",
            "     |      \n",
            "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            "     |      >>> df.createOrReplaceGlobalTempView(\"people\")\n",
            "     |      \n",
            "     |      Replace the global temporary view.\n",
            "     |      \n",
            "     |      >>> df2 = df.filter(df.age > 3)\n",
            "     |      >>> df2.createOrReplaceGlobalTempView(\"people\")\n",
            "     |      >>> df3 = spark.sql(\"SELECT * FROM global_temp.people\")\n",
            "     |      >>> sorted(df3.collect()) == sorted(df2.collect())\n",
            "     |      True\n",
            "     |      >>> spark.catalog.dropGlobalTempView(\"people\")\n",
            "     |      True\n",
            "     |  \n",
            "     |  createOrReplaceTempView(self, name: str) -> None\n",
            "     |      Creates or replaces a local temporary view with this :class:`DataFrame`.\n",
            "     |      \n",
            "     |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
            "     |      that was used to create this :class:`DataFrame`.\n",
            "     |      \n",
            "     |      .. versionadded:: 2.0.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      name : str\n",
            "     |          Name of the view.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      Create a local temporary view named 'people'.\n",
            "     |      \n",
            "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            "     |      >>> df.createOrReplaceTempView(\"people\")\n",
            "     |      \n",
            "     |      Replace the local temporary view.\n",
            "     |      \n",
            "     |      >>> df2 = df.filter(df.age > 3)\n",
            "     |      >>> df2.createOrReplaceTempView(\"people\")\n",
            "     |      >>> df3 = spark.sql(\"SELECT * FROM people\")\n",
            "     |      >>> sorted(df3.collect()) == sorted(df2.collect())\n",
            "     |      True\n",
            "     |      >>> spark.catalog.dropTempView(\"people\")\n",
            "     |      True\n",
            "     |  \n",
            "     |  createTempView(self, name: str) -> None\n",
            "     |      Creates a local temporary view with this :class:`DataFrame`.\n",
            "     |      \n",
            "     |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
            "     |      that was used to create this :class:`DataFrame`.\n",
            "     |      throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the\n",
            "     |      catalog.\n",
            "     |      \n",
            "     |      .. versionadded:: 2.0.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      name : str\n",
            "     |          Name of the view.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      Create a local temporary view.\n",
            "     |      \n",
            "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            "     |      >>> df.createTempView(\"people\")\n",
            "     |      >>> df2 = spark.sql(\"SELECT * FROM people\")\n",
            "     |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
            "     |      True\n",
            "     |      \n",
            "     |      Throw an exception if the table already exists.\n",
            "     |      \n",
            "     |      >>> df.createTempView(\"people\")  # doctest: +IGNORE_EXCEPTION_DETAIL\n",
            "     |      Traceback (most recent call last):\n",
            "     |      ...\n",
            "     |      AnalysisException: \"Temporary table 'people' already exists;\"\n",
            "     |      >>> spark.catalog.dropTempView(\"people\")\n",
            "     |      True\n",
            "     |  \n",
            "     |  crossJoin(self, other: 'DataFrame') -> 'DataFrame'\n",
            "     |      Returns the cartesian product with another :class:`DataFrame`.\n",
            "     |      \n",
            "     |      .. versionadded:: 2.1.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      other : :class:`DataFrame`\n",
            "     |          Right side of the cartesian product.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          Joined DataFrame.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> from pyspark.sql import Row\n",
            "     |      >>> df = spark.createDataFrame(\n",
            "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            "     |      >>> df2 = spark.createDataFrame(\n",
            "     |      ...     [Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])\n",
            "     |      >>> df.crossJoin(df2.select(\"height\")).select(\"age\", \"name\", \"height\").show()\n",
            "     |      +---+-----+------+\n",
            "     |      |age| name|height|\n",
            "     |      +---+-----+------+\n",
            "     |      | 14|  Tom|    80|\n",
            "     |      | 14|  Tom|    85|\n",
            "     |      | 23|Alice|    80|\n",
            "     |      | 23|Alice|    85|\n",
            "     |      | 16|  Bob|    80|\n",
            "     |      | 16|  Bob|    85|\n",
            "     |      +---+-----+------+\n",
            "     |  \n",
            "     |  crosstab(self, col1: str, col2: str) -> 'DataFrame'\n",
            "     |      Computes a pair-wise frequency table of the given columns. Also known as a contingency\n",
            "     |      table.\n",
            "     |      The first column of each row will be the distinct values of `col1` and the column names\n",
            "     |      will be the distinct values of `col2`. The name of the first column will be `$col1_$col2`.\n",
            "     |      Pairs that have no occurrences will have zero as their counts.\n",
            "     |      :func:`DataFrame.crosstab` and :func:`DataFrameStatFunctions.crosstab` are aliases.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.4.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      col1 : str\n",
            "     |          The name of the first column. Distinct items will make the first item of\n",
            "     |          each row.\n",
            "     |      col2 : str\n",
            "     |          The name of the second column. Distinct items will make the column names\n",
            "     |          of the :class:`DataFrame`.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          Frequency matrix of two columns.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame([(1, 11), (1, 11), (3, 10), (4, 8), (4, 8)], [\"c1\", \"c2\"])\n",
            "     |      >>> df.crosstab(\"c1\", \"c2\").sort(\"c1_c2\").show()\n",
            "     |      +-----+---+---+---+\n",
            "     |      |c1_c2| 10| 11|  8|\n",
            "     |      +-----+---+---+---+\n",
            "     |      |    1|  0|  2|  0|\n",
            "     |      |    3|  1|  0|  0|\n",
            "     |      |    4|  0|  0|  2|\n",
            "     |      +-----+---+---+---+\n",
            "     |  \n",
            "     |  cube(self, *cols: 'ColumnOrName') -> 'GroupedData'\n",
            "     |      Create a multi-dimensional cube for the current :class:`DataFrame` using\n",
            "     |      the specified columns, so we can run aggregations on them.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.4.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      cols : list, str or :class:`Column`\n",
            "     |          columns to create cube by.\n",
            "     |          Each element should be a column name (string) or an expression (:class:`Column`)\n",
            "     |          or list of them.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`GroupedData`\n",
            "     |          Cube of the data by given columns.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            "     |      >>> df.cube(\"name\", df.age).count().orderBy(\"name\", \"age\").show()\n",
            "     |      +-----+----+-----+\n",
            "     |      | name| age|count|\n",
            "     |      +-----+----+-----+\n",
            "     |      | NULL|NULL|    2|\n",
            "     |      | NULL|   2|    1|\n",
            "     |      | NULL|   5|    1|\n",
            "     |      |Alice|NULL|    1|\n",
            "     |      |Alice|   2|    1|\n",
            "     |      |  Bob|NULL|    1|\n",
            "     |      |  Bob|   5|    1|\n",
            "     |      +-----+----+-----+\n",
            "     |  \n",
            "     |  describe(self, *cols: Union[str, List[str]]) -> 'DataFrame'\n",
            "     |      Computes basic statistics for numeric and string columns.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.3.1\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      This includes count, mean, stddev, min, and max. If no columns are\n",
            "     |      given, this function computes statistics for all numerical or string columns.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      This function is meant for exploratory data analysis, as we make no\n",
            "     |      guarantee about the backward compatibility of the schema of the resulting\n",
            "     |      :class:`DataFrame`.\n",
            "     |      \n",
            "     |      Use summary for expanded statistics and control over which statistics to compute.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      cols : str, list, optional\n",
            "     |           Column name or list of column names to describe by (default All columns).\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          A new DataFrame that describes (provides statistics) given DataFrame.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame(\n",
            "     |      ...     [(\"Bob\", 13, 40.3, 150.5), (\"Alice\", 12, 37.8, 142.3), (\"Tom\", 11, 44.1, 142.2)],\n",
            "     |      ...     [\"name\", \"age\", \"weight\", \"height\"],\n",
            "     |      ... )\n",
            "     |      >>> df.describe(['age']).show()\n",
            "     |      +-------+----+\n",
            "     |      |summary| age|\n",
            "     |      +-------+----+\n",
            "     |      |  count|   3|\n",
            "     |      |   mean|12.0|\n",
            "     |      | stddev| 1.0|\n",
            "     |      |    min|  11|\n",
            "     |      |    max|  13|\n",
            "     |      +-------+----+\n",
            "     |      \n",
            "     |      >>> df.describe(['age', 'weight', 'height']).show()\n",
            "     |      +-------+----+------------------+-----------------+\n",
            "     |      |summary| age|            weight|           height|\n",
            "     |      +-------+----+------------------+-----------------+\n",
            "     |      |  count|   3|                 3|                3|\n",
            "     |      |   mean|12.0| 40.73333333333333|            145.0|\n",
            "     |      | stddev| 1.0|3.1722757341273704|4.763402145525822|\n",
            "     |      |    min|  11|              37.8|            142.2|\n",
            "     |      |    max|  13|              44.1|            150.5|\n",
            "     |      +-------+----+------------------+-----------------+\n",
            "     |      \n",
            "     |      See Also\n",
            "     |      --------\n",
            "     |      DataFrame.summary\n",
            "     |  \n",
            "     |  distinct(self) -> 'DataFrame'\n",
            "     |      Returns a new :class:`DataFrame` containing the distinct rows in this :class:`DataFrame`.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.3.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          DataFrame with distinct records.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame(\n",
            "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (23, \"Alice\")], [\"age\", \"name\"])\n",
            "     |      \n",
            "     |      Return the number of distinct rows in the :class:`DataFrame`\n",
            "     |      \n",
            "     |      >>> df.distinct().count()\n",
            "     |      2\n",
            "     |  \n",
            "     |  drop(self, *cols: 'ColumnOrName') -> 'DataFrame'\n",
            "     |      Returns a new :class:`DataFrame` without specified columns.\n",
            "     |      This is a no-op if the schema doesn't contain the given column name(s).\n",
            "     |      \n",
            "     |      .. versionadded:: 1.4.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      cols: str or :class:`Column`\n",
            "     |          a name of the column, or the :class:`Column` to drop\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          DataFrame without given columns.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      When an input is a column name, it is treated literally without further interpretation.\n",
            "     |      Otherwise, will try to match the equivalent expression.\n",
            "     |      So that dropping column by its name `drop(colName)` has different semantic with directly\n",
            "     |      dropping the column `drop(col(colName))`.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> from pyspark.sql import Row\n",
            "     |      >>> from pyspark.sql.functions import col, lit\n",
            "     |      >>> df = spark.createDataFrame(\n",
            "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            "     |      >>> df2 = spark.createDataFrame([Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])\n",
            "     |      \n",
            "     |      >>> df.drop('age').show()\n",
            "     |      +-----+\n",
            "     |      | name|\n",
            "     |      +-----+\n",
            "     |      |  Tom|\n",
            "     |      |Alice|\n",
            "     |      |  Bob|\n",
            "     |      +-----+\n",
            "     |      >>> df.drop(df.age).show()\n",
            "     |      +-----+\n",
            "     |      | name|\n",
            "     |      +-----+\n",
            "     |      |  Tom|\n",
            "     |      |Alice|\n",
            "     |      |  Bob|\n",
            "     |      +-----+\n",
            "     |      \n",
            "     |      Drop the column that joined both DataFrames on.\n",
            "     |      \n",
            "     |      >>> df.join(df2, df.name == df2.name, 'inner').drop('name').sort('age').show()\n",
            "     |      +---+------+\n",
            "     |      |age|height|\n",
            "     |      +---+------+\n",
            "     |      | 14|    80|\n",
            "     |      | 16|    85|\n",
            "     |      +---+------+\n",
            "     |      \n",
            "     |      >>> df3 = df.join(df2)\n",
            "     |      >>> df3.show()\n",
            "     |      +---+-----+------+----+\n",
            "     |      |age| name|height|name|\n",
            "     |      +---+-----+------+----+\n",
            "     |      | 14|  Tom|    80| Tom|\n",
            "     |      | 14|  Tom|    85| Bob|\n",
            "     |      | 23|Alice|    80| Tom|\n",
            "     |      | 23|Alice|    85| Bob|\n",
            "     |      | 16|  Bob|    80| Tom|\n",
            "     |      | 16|  Bob|    85| Bob|\n",
            "     |      +---+-----+------+----+\n",
            "     |      \n",
            "     |      Drop two column by the same name.\n",
            "     |      \n",
            "     |      >>> df3.drop(\"name\").show()\n",
            "     |      +---+------+\n",
            "     |      |age|height|\n",
            "     |      +---+------+\n",
            "     |      | 14|    80|\n",
            "     |      | 14|    85|\n",
            "     |      | 23|    80|\n",
            "     |      | 23|    85|\n",
            "     |      | 16|    80|\n",
            "     |      | 16|    85|\n",
            "     |      +---+------+\n",
            "     |      \n",
            "     |      Can not drop col('name') due to ambiguous reference.\n",
            "     |      \n",
            "     |      >>> df3.drop(col(\"name\")).show()\n",
            "     |      Traceback (most recent call last):\n",
            "     |      ...\n",
            "     |      pyspark.errors.exceptions.captured.AnalysisException: [AMBIGUOUS_REFERENCE] Reference...\n",
            "     |      \n",
            "     |      >>> df4 = df.withColumn(\"a.b.c\", lit(1))\n",
            "     |      >>> df4.show()\n",
            "     |      +---+-----+-----+\n",
            "     |      |age| name|a.b.c|\n",
            "     |      +---+-----+-----+\n",
            "     |      | 14|  Tom|    1|\n",
            "     |      | 23|Alice|    1|\n",
            "     |      | 16|  Bob|    1|\n",
            "     |      +---+-----+-----+\n",
            "     |      \n",
            "     |      >>> df4.drop(\"a.b.c\").show()\n",
            "     |      +---+-----+\n",
            "     |      |age| name|\n",
            "     |      +---+-----+\n",
            "     |      | 14|  Tom|\n",
            "     |      | 23|Alice|\n",
            "     |      | 16|  Bob|\n",
            "     |      +---+-----+\n",
            "     |      \n",
            "     |      Can not find a column matching the expression \"a.b.c\".\n",
            "     |      \n",
            "     |      >>> df4.drop(col(\"a.b.c\")).show()\n",
            "     |      +---+-----+-----+\n",
            "     |      |age| name|a.b.c|\n",
            "     |      +---+-----+-----+\n",
            "     |      | 14|  Tom|    1|\n",
            "     |      | 23|Alice|    1|\n",
            "     |      | 16|  Bob|    1|\n",
            "     |      +---+-----+-----+\n",
            "     |  \n",
            "     |  dropDuplicates(self, subset: Optional[List[str]] = None) -> 'DataFrame'\n",
            "     |      Return a new :class:`DataFrame` with duplicate rows removed,\n",
            "     |      optionally only considering certain columns.\n",
            "     |      \n",
            "     |      For a static batch :class:`DataFrame`, it just drops duplicate rows. For a streaming\n",
            "     |      :class:`DataFrame`, it will keep all data across triggers as intermediate state to drop\n",
            "     |      duplicates rows. You can use :func:`withWatermark` to limit how late the duplicate data can\n",
            "     |      be and the system will accordingly limit the state. In addition, data older than\n",
            "     |      watermark will be dropped to avoid any possibility of duplicates.\n",
            "     |      \n",
            "     |      :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.4.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      subset : List of column names, optional\n",
            "     |          List of columns to use for duplicate comparison (default All columns).\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          DataFrame without duplicates.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> from pyspark.sql import Row\n",
            "     |      >>> df = spark.createDataFrame([\n",
            "     |      ...     Row(name='Alice', age=5, height=80),\n",
            "     |      ...     Row(name='Alice', age=5, height=80),\n",
            "     |      ...     Row(name='Alice', age=10, height=80)\n",
            "     |      ... ])\n",
            "     |      \n",
            "     |      Deduplicate the same rows.\n",
            "     |      \n",
            "     |      >>> df.dropDuplicates().show()\n",
            "     |      +-----+---+------+\n",
            "     |      | name|age|height|\n",
            "     |      +-----+---+------+\n",
            "     |      |Alice|  5|    80|\n",
            "     |      |Alice| 10|    80|\n",
            "     |      +-----+---+------+\n",
            "     |      \n",
            "     |      Deduplicate values on 'name' and 'height' columns.\n",
            "     |      \n",
            "     |      >>> df.dropDuplicates(['name', 'height']).show()\n",
            "     |      +-----+---+------+\n",
            "     |      | name|age|height|\n",
            "     |      +-----+---+------+\n",
            "     |      |Alice|  5|    80|\n",
            "     |      +-----+---+------+\n",
            "     |  \n",
            "     |  dropDuplicatesWithinWatermark(self, subset: Optional[List[str]] = None) -> 'DataFrame'\n",
            "     |      Return a new :class:`DataFrame` with duplicate rows removed,\n",
            "     |       optionally only considering certain columns, within watermark.\n",
            "     |      \n",
            "     |       This only works with streaming :class:`DataFrame`, and watermark for the input\n",
            "     |       :class:`DataFrame` must be set via :func:`withWatermark`.\n",
            "     |      \n",
            "     |      For a streaming :class:`DataFrame`, this will keep all data across triggers as intermediate\n",
            "     |      state to drop duplicated rows. The state will be kept to guarantee the semantic, \"Events\n",
            "     |      are deduplicated as long as the time distance of earliest and latest events are smaller\n",
            "     |      than the delay threshold of watermark.\" Users are encouraged to set the delay threshold of\n",
            "     |      watermark longer than max timestamp differences among duplicated events.\n",
            "     |      \n",
            "     |      Note: too late data older than watermark will be dropped.\n",
            "     |      \n",
            "     |       .. versionadded:: 3.5.0\n",
            "     |      \n",
            "     |       Parameters\n",
            "     |       ----------\n",
            "     |       subset : List of column names, optional\n",
            "     |           List of columns to use for duplicate comparison (default All columns).\n",
            "     |      \n",
            "     |       Returns\n",
            "     |       -------\n",
            "     |       :class:`DataFrame`\n",
            "     |           DataFrame without duplicates.\n",
            "     |      \n",
            "     |       Notes\n",
            "     |       -----\n",
            "     |       Supports Spark Connect.\n",
            "     |      \n",
            "     |       Examples\n",
            "     |       --------\n",
            "     |       >>> from pyspark.sql import Row\n",
            "     |       >>> from pyspark.sql.functions import timestamp_seconds\n",
            "     |       >>> df = spark.readStream.format(\"rate\").load().selectExpr(\n",
            "     |       ...     \"value % 5 AS value\", \"timestamp\")\n",
            "     |       >>> df.select(\"value\", df.timestamp.alias(\"time\")).withWatermark(\"time\", '10 minutes')\n",
            "     |       DataFrame[value: bigint, time: timestamp]\n",
            "     |      \n",
            "     |       Deduplicate the same rows.\n",
            "     |      \n",
            "     |       >>> df.dropDuplicatesWithinWatermark() # doctest: +SKIP\n",
            "     |      \n",
            "     |       Deduplicate values on 'value' columns.\n",
            "     |      \n",
            "     |       >>> df.dropDuplicatesWithinWatermark(['value'])  # doctest: +SKIP\n",
            "     |  \n",
            "     |  drop_duplicates = dropDuplicates(self, subset=None)\n",
            "     |      :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.4\n",
            "     |  \n",
            "     |  dropna(self, how: str = 'any', thresh: Optional[int] = None, subset: Union[str, Tuple[str, ...], List[str], NoneType] = None) -> 'DataFrame'\n",
            "     |      Returns a new :class:`DataFrame` omitting rows with null values.\n",
            "     |      :func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.3.1\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      how : str, optional\n",
            "     |          'any' or 'all'.\n",
            "     |          If 'any', drop a row if it contains any nulls.\n",
            "     |          If 'all', drop a row only if all its values are null.\n",
            "     |      thresh: int, optional\n",
            "     |          default None\n",
            "     |          If specified, drop rows that have less than `thresh` non-null values.\n",
            "     |          This overwrites the `how` parameter.\n",
            "     |      subset : str, tuple or list, optional\n",
            "     |          optional list of column names to consider.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          DataFrame with null only rows excluded.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> from pyspark.sql import Row\n",
            "     |      >>> df = spark.createDataFrame([\n",
            "     |      ...     Row(age=10, height=80, name=\"Alice\"),\n",
            "     |      ...     Row(age=5, height=None, name=\"Bob\"),\n",
            "     |      ...     Row(age=None, height=None, name=\"Tom\"),\n",
            "     |      ...     Row(age=None, height=None, name=None),\n",
            "     |      ... ])\n",
            "     |      >>> df.na.drop().show()\n",
            "     |      +---+------+-----+\n",
            "     |      |age|height| name|\n",
            "     |      +---+------+-----+\n",
            "     |      | 10|    80|Alice|\n",
            "     |      +---+------+-----+\n",
            "     |  \n",
            "     |  exceptAll(self, other: 'DataFrame') -> 'DataFrame'\n",
            "     |      Return a new :class:`DataFrame` containing rows in this :class:`DataFrame` but\n",
            "     |      not in another :class:`DataFrame` while preserving duplicates.\n",
            "     |      \n",
            "     |      This is equivalent to `EXCEPT ALL` in SQL.\n",
            "     |      As standard in SQL, this function resolves columns by position (not by name).\n",
            "     |      \n",
            "     |      .. versionadded:: 2.4.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      other : :class:`DataFrame`\n",
            "     |          The other :class:`DataFrame` to compare to.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df1 = spark.createDataFrame(\n",
            "     |      ...         [(\"a\", 1), (\"a\", 1), (\"a\", 1), (\"a\", 2), (\"b\",  3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
            "     |      >>> df2 = spark.createDataFrame([(\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
            "     |      >>> df1.exceptAll(df2).show()\n",
            "     |      +---+---+\n",
            "     |      | C1| C2|\n",
            "     |      +---+---+\n",
            "     |      |  a|  1|\n",
            "     |      |  a|  1|\n",
            "     |      |  a|  2|\n",
            "     |      |  c|  4|\n",
            "     |      +---+---+\n",
            "     |  \n",
            "     |  explain(self, extended: Union[bool, str, NoneType] = None, mode: Optional[str] = None) -> None\n",
            "     |      Prints the (logical and physical) plans to the console for debugging purposes.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.3.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      extended : bool, optional\n",
            "     |          default ``False``. If ``False``, prints only the physical plan.\n",
            "     |          When this is a string without specifying the ``mode``, it works as the mode is\n",
            "     |          specified.\n",
            "     |      mode : str, optional\n",
            "     |          specifies the expected output format of plans.\n",
            "     |      \n",
            "     |          * ``simple``: Print only a physical plan.\n",
            "     |          * ``extended``: Print both logical and physical plans.\n",
            "     |          * ``codegen``: Print a physical plan and generated codes if they are available.\n",
            "     |          * ``cost``: Print a logical plan and statistics if they are available.\n",
            "     |          * ``formatted``: Split explain output into two sections: a physical plan outline                 and node details.\n",
            "     |      \n",
            "     |          .. versionchanged:: 3.0.0\n",
            "     |             Added optional argument `mode` to specify the expected output format of plans.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame(\n",
            "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            "     |      \n",
            "     |      Print out the physical plan only (default).\n",
            "     |      \n",
            "     |      >>> df.explain()  # doctest: +SKIP\n",
            "     |      == Physical Plan ==\n",
            "     |      *(1) Scan ExistingRDD[age...,name...]\n",
            "     |      \n",
            "     |      Print out all of the parsed, analyzed, optimized and physical plans.\n",
            "     |      \n",
            "     |      >>> df.explain(True)\n",
            "     |      == Parsed Logical Plan ==\n",
            "     |      ...\n",
            "     |      == Analyzed Logical Plan ==\n",
            "     |      ...\n",
            "     |      == Optimized Logical Plan ==\n",
            "     |      ...\n",
            "     |      == Physical Plan ==\n",
            "     |      ...\n",
            "     |      \n",
            "     |      Print out the plans with two sections: a physical plan outline and node details\n",
            "     |      \n",
            "     |      >>> df.explain(mode=\"formatted\")  # doctest: +SKIP\n",
            "     |      == Physical Plan ==\n",
            "     |      * Scan ExistingRDD (...)\n",
            "     |      (1) Scan ExistingRDD [codegen id : ...]\n",
            "     |      Output [2]: [age..., name...]\n",
            "     |      ...\n",
            "     |      \n",
            "     |      Print a logical plan and statistics if they are available.\n",
            "     |      \n",
            "     |      >>> df.explain(\"cost\")\n",
            "     |      == Optimized Logical Plan ==\n",
            "     |      ...Statistics...\n",
            "     |      ...\n",
            "     |  \n",
            "     |  fillna(self, value: Union[ForwardRef('LiteralType'), Dict[str, ForwardRef('LiteralType')]], subset: Union[str, Tuple[str, ...], List[str], NoneType] = None) -> 'DataFrame'\n",
            "     |      Replace null values, alias for ``na.fill()``.\n",
            "     |      :func:`DataFrame.fillna` and :func:`DataFrameNaFunctions.fill` are aliases of each other.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.3.1\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      value : int, float, string, bool or dict\n",
            "     |          Value to replace null values with.\n",
            "     |          If the value is a dict, then `subset` is ignored and `value` must be a mapping\n",
            "     |          from column name (string) to replacement value. The replacement value must be\n",
            "     |          an int, float, boolean, or string.\n",
            "     |      subset : str, tuple or list, optional\n",
            "     |          optional list of column names to consider.\n",
            "     |          Columns specified in subset that do not have matching data types are ignored.\n",
            "     |          For example, if `value` is a string, and subset contains a non-string column,\n",
            "     |          then the non-string column is simply ignored.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          DataFrame with replaced null values.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame([\n",
            "     |      ...     (10, 80.5, \"Alice\", None),\n",
            "     |      ...     (5, None, \"Bob\", None),\n",
            "     |      ...     (None, None, \"Tom\", None),\n",
            "     |      ...     (None, None, None, True)],\n",
            "     |      ...     schema=[\"age\", \"height\", \"name\", \"bool\"])\n",
            "     |      \n",
            "     |      Fill all null values with 50 for numeric columns.\n",
            "     |      \n",
            "     |      >>> df.na.fill(50).show()\n",
            "     |      +---+------+-----+----+\n",
            "     |      |age|height| name|bool|\n",
            "     |      +---+------+-----+----+\n",
            "     |      | 10|  80.5|Alice|NULL|\n",
            "     |      |  5|  50.0|  Bob|NULL|\n",
            "     |      | 50|  50.0|  Tom|NULL|\n",
            "     |      | 50|  50.0| NULL|true|\n",
            "     |      +---+------+-----+----+\n",
            "     |      \n",
            "     |      Fill all null values with ``False`` for boolean columns.\n",
            "     |      \n",
            "     |      >>> df.na.fill(False).show()\n",
            "     |      +----+------+-----+-----+\n",
            "     |      | age|height| name| bool|\n",
            "     |      +----+------+-----+-----+\n",
            "     |      |  10|  80.5|Alice|false|\n",
            "     |      |   5|  NULL|  Bob|false|\n",
            "     |      |NULL|  NULL|  Tom|false|\n",
            "     |      |NULL|  NULL| NULL| true|\n",
            "     |      +----+------+-----+-----+\n",
            "     |      \n",
            "     |      Fill all null values with to 50 and \"unknown\" for 'age' and 'name' column respectively.\n",
            "     |      \n",
            "     |      >>> df.na.fill({'age': 50, 'name': 'unknown'}).show()\n",
            "     |      +---+------+-------+----+\n",
            "     |      |age|height|   name|bool|\n",
            "     |      +---+------+-------+----+\n",
            "     |      | 10|  80.5|  Alice|NULL|\n",
            "     |      |  5|  NULL|    Bob|NULL|\n",
            "     |      | 50|  NULL|    Tom|NULL|\n",
            "     |      | 50|  NULL|unknown|true|\n",
            "     |      +---+------+-------+----+\n",
            "     |  \n",
            "     |  filter(self, condition: 'ColumnOrName') -> 'DataFrame'\n",
            "     |      Filters rows using the given condition.\n",
            "     |      \n",
            "     |      :func:`where` is an alias for :func:`filter`.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.3.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      condition : :class:`Column` or str\n",
            "     |          a :class:`Column` of :class:`types.BooleanType`\n",
            "     |          or a string of SQL expressions.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          Filtered DataFrame.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame([\n",
            "     |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            "     |      \n",
            "     |      Filter by :class:`Column` instances.\n",
            "     |      \n",
            "     |      >>> df.filter(df.age > 3).show()\n",
            "     |      +---+----+\n",
            "     |      |age|name|\n",
            "     |      +---+----+\n",
            "     |      |  5| Bob|\n",
            "     |      +---+----+\n",
            "     |      >>> df.where(df.age == 2).show()\n",
            "     |      +---+-----+\n",
            "     |      |age| name|\n",
            "     |      +---+-----+\n",
            "     |      |  2|Alice|\n",
            "     |      +---+-----+\n",
            "     |      \n",
            "     |      Filter by SQL expression in a string.\n",
            "     |      \n",
            "     |      >>> df.filter(\"age > 3\").show()\n",
            "     |      +---+----+\n",
            "     |      |age|name|\n",
            "     |      +---+----+\n",
            "     |      |  5| Bob|\n",
            "     |      +---+----+\n",
            "     |      >>> df.where(\"age = 2\").show()\n",
            "     |      +---+-----+\n",
            "     |      |age| name|\n",
            "     |      +---+-----+\n",
            "     |      |  2|Alice|\n",
            "     |      +---+-----+\n",
            "     |  \n",
            "     |  first(self) -> Optional[pyspark.sql.types.Row]\n",
            "     |      Returns the first row as a :class:`Row`.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.3.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`Row`\n",
            "     |          First row if :class:`DataFrame` is not empty, otherwise ``None``.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame([\n",
            "     |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            "     |      >>> df.first()\n",
            "     |      Row(age=2, name='Alice')\n",
            "     |  \n",
            "     |  foreach(self, f: Callable[[pyspark.sql.types.Row], NoneType]) -> None\n",
            "     |      Applies the ``f`` function to all :class:`Row` of this :class:`DataFrame`.\n",
            "     |      \n",
            "     |      This is a shorthand for ``df.rdd.foreach()``.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.3.0\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      f : function\n",
            "     |          A function that accepts one parameter which will\n",
            "     |          receive each row to process.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame(\n",
            "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            "     |      >>> def func(person):\n",
            "     |      ...     print(person.name)\n",
            "     |      ...\n",
            "     |      >>> df.foreach(func)\n",
            "     |  \n",
            "     |  foreachPartition(self, f: Callable[[Iterator[pyspark.sql.types.Row]], NoneType]) -> None\n",
            "     |      Applies the ``f`` function to each partition of this :class:`DataFrame`.\n",
            "     |      \n",
            "     |      This a shorthand for ``df.rdd.foreachPartition()``.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.3.0\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      f : function\n",
            "     |          A function that accepts one parameter which will receive\n",
            "     |          each partition to process.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame(\n",
            "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            "     |      >>> def func(itr):\n",
            "     |      ...     for person in itr:\n",
            "     |      ...         print(person.name)\n",
            "     |      ...\n",
            "     |      >>> df.foreachPartition(func)\n",
            "     |  \n",
            "     |  freqItems(self, cols: Union[List[str], Tuple[str]], support: Optional[float] = None) -> 'DataFrame'\n",
            "     |      Finding frequent items for columns, possibly with false positives. Using the\n",
            "     |      frequent element count algorithm described in\n",
            "     |      \"https://doi.org/10.1145/762471.762473, proposed by Karp, Schenker, and Papadimitriou\".\n",
            "     |      :func:`DataFrame.freqItems` and :func:`DataFrameStatFunctions.freqItems` are aliases.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.4.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      cols : list or tuple\n",
            "     |          Names of the columns to calculate frequent items for as a list or tuple of\n",
            "     |          strings.\n",
            "     |      support : float, optional\n",
            "     |          The frequency with which to consider an item 'frequent'. Default is 1%.\n",
            "     |          The support must be greater than 1e-4.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          DataFrame with frequent items.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      This function is meant for exploratory data analysis, as we make no\n",
            "     |      guarantee about the backward compatibility of the schema of the resulting\n",
            "     |      :class:`DataFrame`.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame([(1, 11), (1, 11), (3, 10), (4, 8), (4, 8)], [\"c1\", \"c2\"])\n",
            "     |      >>> df.freqItems([\"c1\", \"c2\"]).show()  # doctest: +SKIP\n",
            "     |      +------------+------------+\n",
            "     |      |c1_freqItems|c2_freqItems|\n",
            "     |      +------------+------------+\n",
            "     |      |   [4, 1, 3]| [8, 11, 10]|\n",
            "     |      +------------+------------+\n",
            "     |  \n",
            "     |  groupBy(self, *cols: 'ColumnOrName') -> 'GroupedData'\n",
            "     |      Groups the :class:`DataFrame` using the specified columns,\n",
            "     |      so we can run aggregation on them. See :class:`GroupedData`\n",
            "     |      for all the available aggregate functions.\n",
            "     |      \n",
            "     |      :func:`groupby` is an alias for :func:`groupBy`.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.3.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      cols : list, str or :class:`Column`\n",
            "     |          columns to group by.\n",
            "     |          Each element should be a column name (string) or an expression (:class:`Column`)\n",
            "     |          or list of them.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`GroupedData`\n",
            "     |          Grouped data by given columns.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame([\n",
            "     |      ...     (2, \"Alice\"), (2, \"Bob\"), (2, \"Bob\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            "     |      \n",
            "     |      Empty grouping columns triggers a global aggregation.\n",
            "     |      \n",
            "     |      >>> df.groupBy().avg().show()\n",
            "     |      +--------+\n",
            "     |      |avg(age)|\n",
            "     |      +--------+\n",
            "     |      |    2.75|\n",
            "     |      +--------+\n",
            "     |      \n",
            "     |      Group-by 'name', and specify a dictionary to calculate the summation of 'age'.\n",
            "     |      \n",
            "     |      >>> df.groupBy(\"name\").agg({\"age\": \"sum\"}).sort(\"name\").show()\n",
            "     |      +-----+--------+\n",
            "     |      | name|sum(age)|\n",
            "     |      +-----+--------+\n",
            "     |      |Alice|       2|\n",
            "     |      |  Bob|       9|\n",
            "     |      +-----+--------+\n",
            "     |      \n",
            "     |      Group-by 'name', and calculate maximum values.\n",
            "     |      \n",
            "     |      >>> df.groupBy(df.name).max().sort(\"name\").show()\n",
            "     |      +-----+--------+\n",
            "     |      | name|max(age)|\n",
            "     |      +-----+--------+\n",
            "     |      |Alice|       2|\n",
            "     |      |  Bob|       5|\n",
            "     |      +-----+--------+\n",
            "     |      \n",
            "     |      Group-by 'name' and 'age', and calculate the number of rows in each group.\n",
            "     |      \n",
            "     |      >>> df.groupBy([\"name\", df.age]).count().sort(\"name\", \"age\").show()\n",
            "     |      +-----+---+-----+\n",
            "     |      | name|age|count|\n",
            "     |      +-----+---+-----+\n",
            "     |      |Alice|  2|    1|\n",
            "     |      |  Bob|  2|    2|\n",
            "     |      |  Bob|  5|    1|\n",
            "     |      +-----+---+-----+\n",
            "     |  \n",
            "     |  groupby = groupBy(self, *cols)\n",
            "     |      :func:`groupby` is an alias for :func:`groupBy`.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.4\n",
            "     |  \n",
            "     |  head(self, n: Optional[int] = None) -> Union[pyspark.sql.types.Row, NoneType, List[pyspark.sql.types.Row]]\n",
            "     |      Returns the first ``n`` rows.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.3.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      This method should only be used if the resulting array is expected\n",
            "     |      to be small, as all the data is loaded into the driver's memory.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      n : int, optional\n",
            "     |          default 1. Number of rows to return.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      If n is greater than 1, return a list of :class:`Row`.\n",
            "     |      If n is 1, return a single Row.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame([\n",
            "     |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            "     |      >>> df.head()\n",
            "     |      Row(age=2, name='Alice')\n",
            "     |      >>> df.head(1)\n",
            "     |      [Row(age=2, name='Alice')]\n",
            "     |  \n",
            "     |  hint(self, name: str, *parameters: Union[ForwardRef('PrimitiveType'), List[ForwardRef('PrimitiveType')]]) -> 'DataFrame'\n",
            "     |      Specifies some hint on the current :class:`DataFrame`.\n",
            "     |      \n",
            "     |      .. versionadded:: 2.2.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      name : str\n",
            "     |          A name of the hint.\n",
            "     |      parameters : str, list, float or int\n",
            "     |          Optional parameters.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          Hinted DataFrame\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            "     |      >>> df2 = spark.createDataFrame([Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])\n",
            "     |      >>> df.join(df2, \"name\").explain()  # doctest: +SKIP\n",
            "     |      == Physical Plan ==\n",
            "     |      ...\n",
            "     |      ... +- SortMergeJoin ...\n",
            "     |      ...\n",
            "     |      \n",
            "     |      Explicitly trigger the broadcast hashjoin by providing the hint in ``df2``.\n",
            "     |      \n",
            "     |      >>> df.join(df2.hint(\"broadcast\"), \"name\").explain()\n",
            "     |      == Physical Plan ==\n",
            "     |      ...\n",
            "     |      ... +- BroadcastHashJoin ...\n",
            "     |      ...\n",
            "     |  \n",
            "     |  inputFiles(self) -> List[str]\n",
            "     |      Returns a best-effort snapshot of the files that compose this :class:`DataFrame`.\n",
            "     |      This method simply asks each constituent BaseRelation for its respective files and\n",
            "     |      takes the union of all results. Depending on the source relations, this may not find\n",
            "     |      all input files. Duplicates are removed.\n",
            "     |      \n",
            "     |      .. versionadded:: 3.1.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      list\n",
            "     |          List of file paths.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> import tempfile\n",
            "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
            "     |      ...     # Write a single-row DataFrame into a JSON file\n",
            "     |      ...     spark.createDataFrame(\n",
            "     |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
            "     |      ...     ).repartition(1).write.json(d, mode=\"overwrite\")\n",
            "     |      ...\n",
            "     |      ...     # Read the JSON file as a DataFrame.\n",
            "     |      ...     df = spark.read.format(\"json\").load(d)\n",
            "     |      ...\n",
            "     |      ...     # Returns the number of input files.\n",
            "     |      ...     len(df.inputFiles())\n",
            "     |      1\n",
            "     |  \n",
            "     |  intersect(self, other: 'DataFrame') -> 'DataFrame'\n",
            "     |      Return a new :class:`DataFrame` containing rows only in\n",
            "     |      both this :class:`DataFrame` and another :class:`DataFrame`.\n",
            "     |      Note that any duplicates are removed. To preserve duplicates\n",
            "     |      use :func:`intersectAll`.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.3.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      other : :class:`DataFrame`\n",
            "     |          Another :class:`DataFrame` that needs to be combined.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          Combined DataFrame.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      This is equivalent to `INTERSECT` in SQL.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df1 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
            "     |      >>> df2 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
            "     |      >>> df1.intersect(df2).sort(df1.C1.desc()).show()\n",
            "     |      +---+---+\n",
            "     |      | C1| C2|\n",
            "     |      +---+---+\n",
            "     |      |  b|  3|\n",
            "     |      |  a|  1|\n",
            "     |      +---+---+\n",
            "     |  \n",
            "     |  intersectAll(self, other: 'DataFrame') -> 'DataFrame'\n",
            "     |      Return a new :class:`DataFrame` containing rows in both this :class:`DataFrame`\n",
            "     |      and another :class:`DataFrame` while preserving duplicates.\n",
            "     |      \n",
            "     |      This is equivalent to `INTERSECT ALL` in SQL. As standard in SQL, this function\n",
            "     |      resolves columns by position (not by name).\n",
            "     |      \n",
            "     |      .. versionadded:: 2.4.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      other : :class:`DataFrame`\n",
            "     |          Another :class:`DataFrame` that needs to be combined.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          Combined DataFrame.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df1 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
            "     |      >>> df2 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
            "     |      >>> df1.intersectAll(df2).sort(\"C1\", \"C2\").show()\n",
            "     |      +---+---+\n",
            "     |      | C1| C2|\n",
            "     |      +---+---+\n",
            "     |      |  a|  1|\n",
            "     |      |  a|  1|\n",
            "     |      |  b|  3|\n",
            "     |      +---+---+\n",
            "     |  \n",
            "     |  isEmpty(self) -> bool\n",
            "     |      Checks if the :class:`DataFrame` is empty and returns a boolean value.\n",
            "     |      \n",
            "     |      .. versionadded:: 3.3.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      bool\n",
            "     |          Returns ``True`` if the DataFrame is empty, ``False`` otherwise.\n",
            "     |      \n",
            "     |      See Also\n",
            "     |      --------\n",
            "     |      DataFrame.count : Counts the number of rows in DataFrame.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      - Unlike `count()`, this method does not trigger any computation.\n",
            "     |      - An empty DataFrame has no rows. It may have columns, but no data.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      Example 1: Checking if an empty DataFrame is empty\n",
            "     |      \n",
            "     |      >>> df_empty = spark.createDataFrame([], 'a STRING')\n",
            "     |      >>> df_empty.isEmpty()\n",
            "     |      True\n",
            "     |      \n",
            "     |      Example 2: Checking if a non-empty DataFrame is empty\n",
            "     |      \n",
            "     |      >>> df_non_empty = spark.createDataFrame([\"a\"], 'STRING')\n",
            "     |      >>> df_non_empty.isEmpty()\n",
            "     |      False\n",
            "     |      \n",
            "     |      Example 3: Checking if a DataFrame with null values is empty\n",
            "     |      \n",
            "     |      >>> df_nulls = spark.createDataFrame([(None, None)], 'a STRING, b INT')\n",
            "     |      >>> df_nulls.isEmpty()\n",
            "     |      False\n",
            "     |      \n",
            "     |      Example 4: Checking if a DataFrame with no rows but with columns is empty\n",
            "     |      \n",
            "     |      >>> df_no_rows = spark.createDataFrame([], 'id INT, value STRING')\n",
            "     |      >>> df_no_rows.isEmpty()\n",
            "     |      True\n",
            "     |  \n",
            "     |  isLocal(self) -> bool\n",
            "     |      Returns ``True`` if the :func:`collect` and :func:`take` methods can be run locally\n",
            "     |      (without any Spark executors).\n",
            "     |      \n",
            "     |      .. versionadded:: 1.3.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      bool\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.sql(\"SHOW TABLES\")\n",
            "     |      >>> df.isLocal()\n",
            "     |      True\n",
            "     |  \n",
            "     |  join(self, other: 'DataFrame', on: Union[str, List[str], pyspark.sql.column.Column, List[pyspark.sql.column.Column], NoneType] = None, how: Optional[str] = None) -> 'DataFrame'\n",
            "     |      Joins with another :class:`DataFrame`, using the given join expression.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.3.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      other : :class:`DataFrame`\n",
            "     |          Right side of the join\n",
            "     |      on : str, list or :class:`Column`, optional\n",
            "     |          a string for the join column name, a list of column names,\n",
            "     |          a join expression (Column), or a list of Columns.\n",
            "     |          If `on` is a string or a list of strings indicating the name of the join column(s),\n",
            "     |          the column(s) must exist on both sides, and this performs an equi-join.\n",
            "     |      how : str, optional\n",
            "     |          default ``inner``. Must be one of: ``inner``, ``cross``, ``outer``,\n",
            "     |          ``full``, ``fullouter``, ``full_outer``, ``left``, ``leftouter``, ``left_outer``,\n",
            "     |          ``right``, ``rightouter``, ``right_outer``, ``semi``, ``leftsemi``, ``left_semi``,\n",
            "     |          ``anti``, ``leftanti`` and ``left_anti``.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          Joined DataFrame.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      The following performs a full outer join between ``df1`` and ``df2``.\n",
            "     |      \n",
            "     |      >>> from pyspark.sql import Row\n",
            "     |      >>> from pyspark.sql.functions import desc\n",
            "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")]).toDF(\"age\", \"name\")\n",
            "     |      >>> df2 = spark.createDataFrame([Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])\n",
            "     |      >>> df3 = spark.createDataFrame([Row(age=2, name=\"Alice\"), Row(age=5, name=\"Bob\")])\n",
            "     |      >>> df4 = spark.createDataFrame([\n",
            "     |      ...     Row(age=10, height=80, name=\"Alice\"),\n",
            "     |      ...     Row(age=5, height=None, name=\"Bob\"),\n",
            "     |      ...     Row(age=None, height=None, name=\"Tom\"),\n",
            "     |      ...     Row(age=None, height=None, name=None),\n",
            "     |      ... ])\n",
            "     |      \n",
            "     |      Inner join on columns (default)\n",
            "     |      \n",
            "     |      >>> df.join(df2, 'name').select(df.name, df2.height).show()\n",
            "     |      +----+------+\n",
            "     |      |name|height|\n",
            "     |      +----+------+\n",
            "     |      | Bob|    85|\n",
            "     |      +----+------+\n",
            "     |      >>> df.join(df4, ['name', 'age']).select(df.name, df.age).show()\n",
            "     |      +----+---+\n",
            "     |      |name|age|\n",
            "     |      +----+---+\n",
            "     |      | Bob|  5|\n",
            "     |      +----+---+\n",
            "     |      \n",
            "     |      Outer join for both DataFrames on the 'name' column.\n",
            "     |      \n",
            "     |      >>> df.join(df2, df.name == df2.name, 'outer').select(\n",
            "     |      ...     df.name, df2.height).sort(desc(\"name\")).show()\n",
            "     |      +-----+------+\n",
            "     |      | name|height|\n",
            "     |      +-----+------+\n",
            "     |      |  Bob|    85|\n",
            "     |      |Alice|  NULL|\n",
            "     |      | NULL|    80|\n",
            "     |      +-----+------+\n",
            "     |      >>> df.join(df2, 'name', 'outer').select('name', 'height').sort(desc(\"name\")).show()\n",
            "     |      +-----+------+\n",
            "     |      | name|height|\n",
            "     |      +-----+------+\n",
            "     |      |  Tom|    80|\n",
            "     |      |  Bob|    85|\n",
            "     |      |Alice|  NULL|\n",
            "     |      +-----+------+\n",
            "     |      \n",
            "     |      Outer join for both DataFrams with multiple columns.\n",
            "     |      \n",
            "     |      >>> df.join(\n",
            "     |      ...     df3,\n",
            "     |      ...     [df.name == df3.name, df.age == df3.age],\n",
            "     |      ...     'outer'\n",
            "     |      ... ).select(df.name, df3.age).show()\n",
            "     |      +-----+---+\n",
            "     |      | name|age|\n",
            "     |      +-----+---+\n",
            "     |      |Alice|  2|\n",
            "     |      |  Bob|  5|\n",
            "     |      +-----+---+\n",
            "     |  \n",
            "     |  limit(self, num: int) -> 'DataFrame'\n",
            "     |      Limits the result count to the number specified.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.3.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      num : int\n",
            "     |          Number of records to return. Will return this number of records\n",
            "     |          or all records if the DataFrame contains less than this number of records.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          Subset of the records\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame(\n",
            "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            "     |      >>> df.limit(1).show()\n",
            "     |      +---+----+\n",
            "     |      |age|name|\n",
            "     |      +---+----+\n",
            "     |      | 14| Tom|\n",
            "     |      +---+----+\n",
            "     |      >>> df.limit(0).show()\n",
            "     |      +---+----+\n",
            "     |      |age|name|\n",
            "     |      +---+----+\n",
            "     |      +---+----+\n",
            "     |  \n",
            "     |  localCheckpoint(self, eager: bool = True) -> 'DataFrame'\n",
            "     |      Returns a locally checkpointed version of this :class:`DataFrame`. Checkpointing can be\n",
            "     |      used to truncate the logical plan of this :class:`DataFrame`, which is especially useful in\n",
            "     |      iterative algorithms where the plan may grow exponentially. Local checkpoints are\n",
            "     |      stored in the executors using the caching subsystem and therefore they are not reliable.\n",
            "     |      \n",
            "     |      .. versionadded:: 2.3.0\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      eager : bool, optional, default True\n",
            "     |          Whether to checkpoint this :class:`DataFrame` immediately.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          Checkpointed DataFrame.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      This API is experimental.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame([\n",
            "     |      ...     (14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            "     |      >>> df.localCheckpoint(False)\n",
            "     |      DataFrame[age: bigint, name: string]\n",
            "     |  \n",
            "     |  melt(self, ids: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName')], Tuple[ForwardRef('ColumnOrName'), ...]], values: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName')], Tuple[ForwardRef('ColumnOrName'), ...], NoneType], variableColumnName: str, valueColumnName: str) -> 'DataFrame'\n",
            "     |      Unpivot a DataFrame from wide format to long format, optionally leaving\n",
            "     |      identifier columns set. This is the reverse to `groupBy(...).pivot(...).agg(...)`,\n",
            "     |      except for the aggregation, which cannot be reversed.\n",
            "     |      \n",
            "     |      :func:`melt` is an alias for :func:`unpivot`.\n",
            "     |      \n",
            "     |      .. versionadded:: 3.4.0\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      ids : str, Column, tuple, list, optional\n",
            "     |          Column(s) to use as identifiers. Can be a single column or column name,\n",
            "     |          or a list or tuple for multiple columns.\n",
            "     |      values : str, Column, tuple, list, optional\n",
            "     |          Column(s) to unpivot. Can be a single column or column name, or a list or tuple\n",
            "     |          for multiple columns. If not specified or empty, use all columns that\n",
            "     |          are not set as `ids`.\n",
            "     |      variableColumnName : str\n",
            "     |          Name of the variable column.\n",
            "     |      valueColumnName : str\n",
            "     |          Name of the value column.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          Unpivoted DataFrame.\n",
            "     |      \n",
            "     |      See Also\n",
            "     |      --------\n",
            "     |      DataFrame.unpivot\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      Supports Spark Connect.\n",
            "     |  \n",
            "     |  observe(self, observation: Union[ForwardRef('Observation'), str], *exprs: pyspark.sql.column.Column) -> 'DataFrame'\n",
            "     |      Define (named) metrics to observe on the DataFrame. This method returns an 'observed'\n",
            "     |      DataFrame that returns the same result as the input, with the following guarantees:\n",
            "     |      \n",
            "     |      * It will compute the defined aggregates (metrics) on all the data that is flowing through\n",
            "     |          the Dataset at that point.\n",
            "     |      \n",
            "     |      * It will report the value of the defined aggregate columns as soon as we reach a completion\n",
            "     |          point. A completion point is either the end of a query (batch mode) or the end of a\n",
            "     |          streaming epoch. The value of the aggregates only reflects the data processed since\n",
            "     |          the previous completion point.\n",
            "     |      \n",
            "     |      The metrics columns must either contain a literal (e.g. lit(42)), or should contain one or\n",
            "     |      more aggregate functions (e.g. sum(a) or sum(a + b) + avg(c) - lit(1)). Expressions that\n",
            "     |      contain references to the input Dataset's columns must always be wrapped in an aggregate\n",
            "     |      function.\n",
            "     |      \n",
            "     |      A user can observe these metrics by adding\n",
            "     |      Python's :class:`~pyspark.sql.streaming.StreamingQueryListener`,\n",
            "     |      Scala/Java's ``org.apache.spark.sql.streaming.StreamingQueryListener`` or Scala/Java's\n",
            "     |      ``org.apache.spark.sql.util.QueryExecutionListener`` to the spark session.\n",
            "     |      \n",
            "     |      .. versionadded:: 3.3.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.5.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      observation : :class:`Observation` or str\n",
            "     |          `str` to specify the name, or an :class:`Observation` instance to obtain the metric.\n",
            "     |      \n",
            "     |          .. versionchanged:: 3.4.0\n",
            "     |             Added support for `str` in this parameter.\n",
            "     |      exprs : :class:`Column`\n",
            "     |          column expressions (:class:`Column`).\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          the observed :class:`DataFrame`.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      When ``observation`` is :class:`Observation`, this method only supports batch queries.\n",
            "     |      When ``observation`` is a string, this method works for both batch and streaming queries.\n",
            "     |      Continuous execution is currently not supported yet.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      When ``observation`` is :class:`Observation`, only batch queries work as below.\n",
            "     |      \n",
            "     |      >>> from pyspark.sql.functions import col, count, lit, max\n",
            "     |      >>> from pyspark.sql import Observation\n",
            "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            "     |      >>> observation = Observation(\"my metrics\")\n",
            "     |      >>> observed_df = df.observe(observation, count(lit(1)).alias(\"count\"), max(col(\"age\")))\n",
            "     |      >>> observed_df.count()\n",
            "     |      2\n",
            "     |      >>> observation.get\n",
            "     |      {'count': 2, 'max(age)': 5}\n",
            "     |      \n",
            "     |      When ``observation`` is a string, streaming queries also work as below.\n",
            "     |      \n",
            "     |      >>> from pyspark.sql.streaming import StreamingQueryListener\n",
            "     |      >>> class MyErrorListener(StreamingQueryListener):\n",
            "     |      ...    def onQueryStarted(self, event):\n",
            "     |      ...        pass\n",
            "     |      ...\n",
            "     |      ...    def onQueryProgress(self, event):\n",
            "     |      ...        row = event.progress.observedMetrics.get(\"my_event\")\n",
            "     |      ...        # Trigger if the number of errors exceeds 5 percent\n",
            "     |      ...        num_rows = row.rc\n",
            "     |      ...        num_error_rows = row.erc\n",
            "     |      ...        ratio = num_error_rows / num_rows\n",
            "     |      ...        if ratio > 0.05:\n",
            "     |      ...            # Trigger alert\n",
            "     |      ...            pass\n",
            "     |      ...\n",
            "     |      ...    def onQueryIdle(self, event):\n",
            "     |      ...        pass\n",
            "     |      ...\n",
            "     |      ...    def onQueryTerminated(self, event):\n",
            "     |      ...        pass\n",
            "     |      ...\n",
            "     |      >>> spark.streams.addListener(MyErrorListener())\n",
            "     |      >>> # Observe row count (rc) and error row count (erc) in the streaming Dataset\n",
            "     |      ... observed_ds = df.observe(\n",
            "     |      ...     \"my_event\",\n",
            "     |      ...     count(lit(1)).alias(\"rc\"),\n",
            "     |      ...     count(col(\"error\")).alias(\"erc\"))  # doctest: +SKIP\n",
            "     |      >>> observed_ds.writeStream.format(\"console\").start()  # doctest: +SKIP\n",
            "     |  \n",
            "     |  offset(self, num: int) -> 'DataFrame'\n",
            "     |      Returns a new :class: `DataFrame` by skipping the first `n` rows.\n",
            "     |      \n",
            "     |      .. versionadded:: 3.4.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.5.0\n",
            "     |          Supports vanilla PySpark.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      num : int\n",
            "     |          Number of records to skip.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          Subset of the records\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame(\n",
            "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            "     |      >>> df.offset(1).show()\n",
            "     |      +---+-----+\n",
            "     |      |age| name|\n",
            "     |      +---+-----+\n",
            "     |      | 23|Alice|\n",
            "     |      | 16|  Bob|\n",
            "     |      +---+-----+\n",
            "     |      >>> df.offset(10).show()\n",
            "     |      +---+----+\n",
            "     |      |age|name|\n",
            "     |      +---+----+\n",
            "     |      +---+----+\n",
            "     |  \n",
            "     |  orderBy = sort(self, *cols: Union[str, pyspark.sql.column.Column, List[Union[str, pyspark.sql.column.Column]]], **kwargs: Any) -> 'DataFrame'\n",
            "     |  \n",
            "     |  pandas_api(self, index_col: Union[str, List[str], NoneType] = None) -> 'PandasOnSparkDataFrame'\n",
            "     |      Converts the existing DataFrame into a pandas-on-Spark DataFrame.\n",
            "     |      \n",
            "     |      .. versionadded:: 3.2.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.5.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      If a pandas-on-Spark DataFrame is converted to a Spark DataFrame and then back\n",
            "     |      to pandas-on-Spark, it will lose the index information and the original index\n",
            "     |      will be turned into a normal column.\n",
            "     |      \n",
            "     |      This is only available if Pandas is installed and available.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      index_col: str or list of str, optional, default: None\n",
            "     |          Index column of table in Spark.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`PandasOnSparkDataFrame`\n",
            "     |      \n",
            "     |      See Also\n",
            "     |      --------\n",
            "     |      pyspark.pandas.frame.DataFrame.to_spark\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame(\n",
            "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            "     |      \n",
            "     |      >>> df.pandas_api()  # doctest: +SKIP\n",
            "     |         age   name\n",
            "     |      0   14    Tom\n",
            "     |      1   23  Alice\n",
            "     |      2   16    Bob\n",
            "     |      \n",
            "     |      We can specify the index columns.\n",
            "     |      \n",
            "     |      >>> df.pandas_api(index_col=\"age\")  # doctest: +SKIP\n",
            "     |            name\n",
            "     |      age\n",
            "     |      14     Tom\n",
            "     |      23   Alice\n",
            "     |      16     Bob\n",
            "     |  \n",
            "     |  persist(self, storageLevel: pyspark.storagelevel.StorageLevel = StorageLevel(True, True, False, True, 1)) -> 'DataFrame'\n",
            "     |      Sets the storage level to persist the contents of the :class:`DataFrame` across\n",
            "     |      operations after the first time it is computed. This can only be used to assign\n",
            "     |      a new storage level if the :class:`DataFrame` does not have a storage level set yet.\n",
            "     |      If no storage level is specified defaults to (`MEMORY_AND_DISK_DESER`)\n",
            "     |      \n",
            "     |      .. versionadded:: 1.3.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      The default storage level has changed to `MEMORY_AND_DISK_DESER` to match Scala in 3.0.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      storageLevel : :class:`StorageLevel`\n",
            "     |          Storage level to set for persistence. Default is MEMORY_AND_DISK_DESER.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          Persisted DataFrame.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.range(1)\n",
            "     |      >>> df.persist()\n",
            "     |      DataFrame[id: bigint]\n",
            "     |      \n",
            "     |      >>> df.explain()\n",
            "     |      == Physical Plan ==\n",
            "     |      AdaptiveSparkPlan isFinalPlan=false\n",
            "     |      +- InMemoryTableScan ...\n",
            "     |      \n",
            "     |      Persists the data in the disk by specifying the storage level.\n",
            "     |      \n",
            "     |      >>> from pyspark.storagelevel import StorageLevel\n",
            "     |      >>> df.persist(StorageLevel.DISK_ONLY)\n",
            "     |      DataFrame[id: bigint]\n",
            "     |  \n",
            "     |  printSchema(self, level: Optional[int] = None) -> None\n",
            "     |      Prints out the schema in the tree format.\n",
            "     |      Optionally allows to specify how many levels to print if schema is nested.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.3.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      level : int, optional, default None\n",
            "     |          How many levels to print for nested schemas.\n",
            "     |      \n",
            "     |          .. versionchanged:: 3.5.0\n",
            "     |              Added Level parameter.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame(\n",
            "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            "     |      >>> df.printSchema()\n",
            "     |      root\n",
            "     |       |-- age: long (nullable = true)\n",
            "     |       |-- name: string (nullable = true)\n",
            "     |      \n",
            "     |      >>> df = spark.createDataFrame([(1, (2,2))], [\"a\", \"b\"])\n",
            "     |      >>> df.printSchema(1)\n",
            "     |      root\n",
            "     |       |-- a: long (nullable = true)\n",
            "     |       |-- b: struct (nullable = true)\n",
            "     |      \n",
            "     |      >>> df.printSchema(2)\n",
            "     |      root\n",
            "     |       |-- a: long (nullable = true)\n",
            "     |       |-- b: struct (nullable = true)\n",
            "     |       |    |-- _1: long (nullable = true)\n",
            "     |       |    |-- _2: long (nullable = true)\n",
            "     |  \n",
            "     |  randomSplit(self, weights: List[float], seed: Optional[int] = None) -> List[ForwardRef('DataFrame')]\n",
            "     |      Randomly splits this :class:`DataFrame` with the provided weights.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.4.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      weights : list\n",
            "     |          list of doubles as weights with which to split the :class:`DataFrame`.\n",
            "     |          Weights will be normalized if they don't sum up to 1.0.\n",
            "     |      seed : int, optional\n",
            "     |          The seed for sampling.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      list\n",
            "     |          List of DataFrames.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> from pyspark.sql import Row\n",
            "     |      >>> df = spark.createDataFrame([\n",
            "     |      ...     Row(age=10, height=80, name=\"Alice\"),\n",
            "     |      ...     Row(age=5, height=None, name=\"Bob\"),\n",
            "     |      ...     Row(age=None, height=None, name=\"Tom\"),\n",
            "     |      ...     Row(age=None, height=None, name=None),\n",
            "     |      ... ])\n",
            "     |      \n",
            "     |      >>> splits = df.randomSplit([1.0, 2.0], 24)\n",
            "     |      >>> splits[0].count()\n",
            "     |      2\n",
            "     |      >>> splits[1].count()\n",
            "     |      2\n",
            "     |  \n",
            "     |  registerTempTable(self, name: str) -> None\n",
            "     |      Registers this :class:`DataFrame` as a temporary table using the given name.\n",
            "     |      \n",
            "     |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
            "     |      that was used to create this :class:`DataFrame`.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.3.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      .. deprecated:: 2.0.0\n",
            "     |          Use :meth:`DataFrame.createOrReplaceTempView` instead.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      name : str\n",
            "     |          Name of the temporary table to register.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            "     |      >>> df.registerTempTable(\"people\")\n",
            "     |      >>> df2 = spark.sql(\"SELECT * FROM people\")\n",
            "     |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
            "     |      True\n",
            "     |      >>> spark.catalog.dropTempView(\"people\")\n",
            "     |      True\n",
            "     |  \n",
            "     |  repartition(self, numPartitions: Union[int, ForwardRef('ColumnOrName')], *cols: 'ColumnOrName') -> 'DataFrame'\n",
            "     |      Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The\n",
            "     |      resulting :class:`DataFrame` is hash partitioned.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.3.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      numPartitions : int\n",
            "     |          can be an int to specify the target number of partitions or a Column.\n",
            "     |          If it is a Column, it will be used as the first partitioning column. If not specified,\n",
            "     |          the default number of partitions is used.\n",
            "     |      cols : str or :class:`Column`\n",
            "     |          partitioning columns.\n",
            "     |      \n",
            "     |          .. versionchanged:: 1.6.0\n",
            "     |             Added optional arguments to specify the partitioning columns. Also made numPartitions\n",
            "     |             optional if partitioning columns are specified.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          Repartitioned DataFrame.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame(\n",
            "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            "     |      \n",
            "     |      Repartition the data into 10 partitions.\n",
            "     |      \n",
            "     |      >>> df.repartition(10).rdd.getNumPartitions()\n",
            "     |      10\n",
            "     |      \n",
            "     |      Repartition the data into 7 partitions by 'age' column.\n",
            "     |      \n",
            "     |      >>> df.repartition(7, \"age\").rdd.getNumPartitions()\n",
            "     |      7\n",
            "     |      \n",
            "     |      Repartition the data into 7 partitions by 'age' and 'name columns.\n",
            "     |      \n",
            "     |      >>> df.repartition(3, \"name\", \"age\").rdd.getNumPartitions()\n",
            "     |      3\n",
            "     |  \n",
            "     |  repartitionByRange(self, numPartitions: Union[int, ForwardRef('ColumnOrName')], *cols: 'ColumnOrName') -> 'DataFrame'\n",
            "     |      Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The\n",
            "     |      resulting :class:`DataFrame` is range partitioned.\n",
            "     |      \n",
            "     |      .. versionadded:: 2.4.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      numPartitions : int\n",
            "     |          can be an int to specify the target number of partitions or a Column.\n",
            "     |          If it is a Column, it will be used as the first partitioning column. If not specified,\n",
            "     |          the default number of partitions is used.\n",
            "     |      cols : str or :class:`Column`\n",
            "     |          partitioning columns.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          Repartitioned DataFrame.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      At least one partition-by expression must be specified.\n",
            "     |      When no explicit sort order is specified, \"ascending nulls first\" is assumed.\n",
            "     |      \n",
            "     |      Due to performance reasons this method uses sampling to estimate the ranges.\n",
            "     |      Hence, the output may not be consistent, since sampling can return different values.\n",
            "     |      The sample size can be controlled by the config\n",
            "     |      `spark.sql.execution.rangeExchange.sampleSizePerPartition`.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame(\n",
            "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            "     |      \n",
            "     |      Repartition the data into 2 partitions by range in 'age' column.\n",
            "     |      For example, the first partition can have ``(14, \"Tom\")``, and the second\n",
            "     |      partition would have ``(16, \"Bob\")`` and ``(23, \"Alice\")``.\n",
            "     |      \n",
            "     |      >>> df.repartitionByRange(2, \"age\").rdd.getNumPartitions()\n",
            "     |      2\n",
            "     |  \n",
            "     |  replace(self, to_replace: Union[ForwardRef('LiteralType'), List[ForwardRef('LiteralType')], Dict[ForwardRef('LiteralType'), ForwardRef('OptionalPrimitiveType')]], value: Union[ForwardRef('OptionalPrimitiveType'), List[ForwardRef('OptionalPrimitiveType')], pyspark._globals._NoValueType, NoneType] = <no value>, subset: Optional[List[str]] = None) -> 'DataFrame'\n",
            "     |      Returns a new :class:`DataFrame` replacing a value with another value.\n",
            "     |      :func:`DataFrame.replace` and :func:`DataFrameNaFunctions.replace` are\n",
            "     |      aliases of each other.\n",
            "     |      Values to_replace and value must have the same type and can only be numerics, booleans,\n",
            "     |      or strings. Value can have None. When replacing, the new value will be cast\n",
            "     |      to the type of the existing column.\n",
            "     |      For numeric replacements all values to be replaced should have unique\n",
            "     |      floating point representation. In case of conflicts (for example with `{42: -1, 42.0: 1}`)\n",
            "     |      and arbitrary replacement will be used.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.4.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      to_replace : bool, int, float, string, list or dict\n",
            "     |          Value to be replaced.\n",
            "     |          If the value is a dict, then `value` is ignored or can be omitted, and `to_replace`\n",
            "     |          must be a mapping between a value and a replacement.\n",
            "     |      value : bool, int, float, string or None, optional\n",
            "     |          The replacement value must be a bool, int, float, string or None. If `value` is a\n",
            "     |          list, `value` should be of the same length and type as `to_replace`.\n",
            "     |          If `value` is a scalar and `to_replace` is a sequence, then `value` is\n",
            "     |          used as a replacement for each item in `to_replace`.\n",
            "     |      subset : list, optional\n",
            "     |          optional list of column names to consider.\n",
            "     |          Columns specified in subset that do not have matching data types are ignored.\n",
            "     |          For example, if `value` is a string, and subset contains a non-string column,\n",
            "     |          then the non-string column is simply ignored.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          DataFrame with replaced values.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame([\n",
            "     |      ...     (10, 80, \"Alice\"),\n",
            "     |      ...     (5, None, \"Bob\"),\n",
            "     |      ...     (None, 10, \"Tom\"),\n",
            "     |      ...     (None, None, None)],\n",
            "     |      ...     schema=[\"age\", \"height\", \"name\"])\n",
            "     |      \n",
            "     |      Replace 10 to 20 in all columns.\n",
            "     |      \n",
            "     |      >>> df.na.replace(10, 20).show()\n",
            "     |      +----+------+-----+\n",
            "     |      | age|height| name|\n",
            "     |      +----+------+-----+\n",
            "     |      |  20|    80|Alice|\n",
            "     |      |   5|  NULL|  Bob|\n",
            "     |      |NULL|    20|  Tom|\n",
            "     |      |NULL|  NULL| NULL|\n",
            "     |      +----+------+-----+\n",
            "     |      \n",
            "     |      Replace 'Alice' to null in all columns.\n",
            "     |      \n",
            "     |      >>> df.na.replace('Alice', None).show()\n",
            "     |      +----+------+----+\n",
            "     |      | age|height|name|\n",
            "     |      +----+------+----+\n",
            "     |      |  10|    80|NULL|\n",
            "     |      |   5|  NULL| Bob|\n",
            "     |      |NULL|    10| Tom|\n",
            "     |      |NULL|  NULL|NULL|\n",
            "     |      +----+------+----+\n",
            "     |      \n",
            "     |      Replace 'Alice' to 'A', and 'Bob' to 'B' in the 'name' column.\n",
            "     |      \n",
            "     |      >>> df.na.replace(['Alice', 'Bob'], ['A', 'B'], 'name').show()\n",
            "     |      +----+------+----+\n",
            "     |      | age|height|name|\n",
            "     |      +----+------+----+\n",
            "     |      |  10|    80|   A|\n",
            "     |      |   5|  NULL|   B|\n",
            "     |      |NULL|    10| Tom|\n",
            "     |      |NULL|  NULL|NULL|\n",
            "     |      +----+------+----+\n",
            "     |  \n",
            "     |  rollup(self, *cols: 'ColumnOrName') -> 'GroupedData'\n",
            "     |      Create a multi-dimensional rollup for the current :class:`DataFrame` using\n",
            "     |      the specified columns, so we can run aggregation on them.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.4.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      cols : list, str or :class:`Column`\n",
            "     |          Columns to roll-up by.\n",
            "     |          Each element should be a column name (string) or an expression (:class:`Column`)\n",
            "     |          or list of them.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`GroupedData`\n",
            "     |          Rolled-up data by given columns.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            "     |      >>> df.rollup(\"name\", df.age).count().orderBy(\"name\", \"age\").show()\n",
            "     |      +-----+----+-----+\n",
            "     |      | name| age|count|\n",
            "     |      +-----+----+-----+\n",
            "     |      | NULL|NULL|    2|\n",
            "     |      |Alice|NULL|    1|\n",
            "     |      |Alice|   2|    1|\n",
            "     |      |  Bob|NULL|    1|\n",
            "     |      |  Bob|   5|    1|\n",
            "     |      +-----+----+-----+\n",
            "     |  \n",
            "     |  sameSemantics(self, other: 'DataFrame') -> bool\n",
            "     |      Returns `True` when the logical query plans inside both :class:`DataFrame`\\s are equal and\n",
            "     |      therefore return the same results.\n",
            "     |      \n",
            "     |      .. versionadded:: 3.1.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.5.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      The equality comparison here is simplified by tolerating the cosmetic differences\n",
            "     |      such as attribute names.\n",
            "     |      \n",
            "     |      This API can compare both :class:`DataFrame`\\s very fast but can still return\n",
            "     |      `False` on the :class:`DataFrame` that return the same results, for instance, from\n",
            "     |      different plans. Such false negative semantic can be useful when caching as an example.\n",
            "     |      \n",
            "     |      This API is a developer API.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      other : :class:`DataFrame`\n",
            "     |          The other DataFrame to compare against.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      bool\n",
            "     |          Whether these two DataFrames are similar.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df1 = spark.range(10)\n",
            "     |      >>> df2 = spark.range(10)\n",
            "     |      >>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col1\", df2.id * 2))\n",
            "     |      True\n",
            "     |      >>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col1\", df2.id + 2))\n",
            "     |      False\n",
            "     |      >>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col0\", df2.id * 2))\n",
            "     |      True\n",
            "     |  \n",
            "     |  sample(self, withReplacement: Union[float, bool, NoneType] = None, fraction: Union[int, float, NoneType] = None, seed: Optional[int] = None) -> 'DataFrame'\n",
            "     |      Returns a sampled subset of this :class:`DataFrame`.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.3.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      withReplacement : bool, optional\n",
            "     |          Sample with replacement or not (default ``False``).\n",
            "     |      fraction : float, optional\n",
            "     |          Fraction of rows to generate, range [0.0, 1.0].\n",
            "     |      seed : int, optional\n",
            "     |          Seed for sampling (default a random seed).\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          Sampled rows from given DataFrame.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      This is not guaranteed to provide exactly the fraction specified of the total\n",
            "     |      count of the given :class:`DataFrame`.\n",
            "     |      \n",
            "     |      `fraction` is required and, `withReplacement` and `seed` are optional.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.range(10)\n",
            "     |      >>> df.sample(0.5, 3).count() # doctest: +SKIP\n",
            "     |      7\n",
            "     |      >>> df.sample(fraction=0.5, seed=3).count() # doctest: +SKIP\n",
            "     |      7\n",
            "     |      >>> df.sample(withReplacement=True, fraction=0.5, seed=3).count() # doctest: +SKIP\n",
            "     |      1\n",
            "     |      >>> df.sample(1.0).count()\n",
            "     |      10\n",
            "     |      >>> df.sample(fraction=1.0).count()\n",
            "     |      10\n",
            "     |      >>> df.sample(False, fraction=1.0).count()\n",
            "     |      10\n",
            "     |  \n",
            "     |  sampleBy(self, col: 'ColumnOrName', fractions: Dict[Any, float], seed: Optional[int] = None) -> 'DataFrame'\n",
            "     |      Returns a stratified sample without replacement based on the\n",
            "     |      fraction given on each stratum.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.5.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      col : :class:`Column` or str\n",
            "     |          column that defines strata\n",
            "     |      \n",
            "     |          .. versionchanged:: 3.0.0\n",
            "     |             Added sampling by a column of :class:`Column`\n",
            "     |      fractions : dict\n",
            "     |          sampling fraction for each stratum. If a stratum is not\n",
            "     |          specified, we treat its fraction as zero.\n",
            "     |      seed : int, optional\n",
            "     |          random seed\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      a new :class:`DataFrame` that represents the stratified sample\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> from pyspark.sql.functions import col\n",
            "     |      >>> dataset = spark.range(0, 100).select((col(\"id\") % 3).alias(\"key\"))\n",
            "     |      >>> sampled = dataset.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=0)\n",
            "     |      >>> sampled.groupBy(\"key\").count().orderBy(\"key\").show()\n",
            "     |      +---+-----+\n",
            "     |      |key|count|\n",
            "     |      +---+-----+\n",
            "     |      |  0|    3|\n",
            "     |      |  1|    6|\n",
            "     |      +---+-----+\n",
            "     |      >>> dataset.sampleBy(col(\"key\"), fractions={2: 1.0}, seed=0).count()\n",
            "     |      33\n",
            "     |  \n",
            "     |  select(self, *cols: 'ColumnOrName') -> 'DataFrame'\n",
            "     |      Projects a set of expressions and returns a new :class:`DataFrame`.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.3.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      cols : str, :class:`Column`, or list\n",
            "     |          column names (string) or expressions (:class:`Column`).\n",
            "     |          If one of the column names is '*', that column is expanded to include all columns\n",
            "     |          in the current :class:`DataFrame`.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          A DataFrame with subset (or all) of columns.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame([\n",
            "     |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            "     |      \n",
            "     |      Select all columns in the DataFrame.\n",
            "     |      \n",
            "     |      >>> df.select('*').show()\n",
            "     |      +---+-----+\n",
            "     |      |age| name|\n",
            "     |      +---+-----+\n",
            "     |      |  2|Alice|\n",
            "     |      |  5|  Bob|\n",
            "     |      +---+-----+\n",
            "     |      \n",
            "     |      Select a column with other expressions in the DataFrame.\n",
            "     |      \n",
            "     |      >>> df.select(df.name, (df.age + 10).alias('age')).show()\n",
            "     |      +-----+---+\n",
            "     |      | name|age|\n",
            "     |      +-----+---+\n",
            "     |      |Alice| 12|\n",
            "     |      |  Bob| 15|\n",
            "     |      +-----+---+\n",
            "     |  \n",
            "     |  selectExpr(self, *expr: Union[str, List[str]]) -> 'DataFrame'\n",
            "     |      Projects a set of SQL expressions and returns a new :class:`DataFrame`.\n",
            "     |      \n",
            "     |      This is a variant of :func:`select` that accepts SQL expressions.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.3.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          A DataFrame with new/old columns transformed by expressions.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame([\n",
            "     |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            "     |      >>> df.selectExpr(\"age * 2\", \"abs(age)\").show()\n",
            "     |      +---------+--------+\n",
            "     |      |(age * 2)|abs(age)|\n",
            "     |      +---------+--------+\n",
            "     |      |        4|       2|\n",
            "     |      |       10|       5|\n",
            "     |      +---------+--------+\n",
            "     |  \n",
            "     |  semanticHash(self) -> int\n",
            "     |      Returns a hash code of the logical query plan against this :class:`DataFrame`.\n",
            "     |      \n",
            "     |      .. versionadded:: 3.1.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.5.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      Unlike the standard hash code, the hash is calculated against the query plan\n",
            "     |      simplified by tolerating the cosmetic differences such as attribute names.\n",
            "     |      \n",
            "     |      This API is a developer API.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      int\n",
            "     |          Hash value.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> spark.range(10).selectExpr(\"id as col0\").semanticHash()  # doctest: +SKIP\n",
            "     |      1855039936\n",
            "     |      >>> spark.range(10).selectExpr(\"id as col1\").semanticHash()  # doctest: +SKIP\n",
            "     |      1855039936\n",
            "     |  \n",
            "     |  show(self, n: int = 20, truncate: Union[bool, int] = True, vertical: bool = False) -> None\n",
            "     |      Prints the first ``n`` rows to the console.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.3.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      n : int, optional\n",
            "     |          Number of rows to show.\n",
            "     |      truncate : bool or int, optional\n",
            "     |          If set to ``True``, truncate strings longer than 20 chars by default.\n",
            "     |          If set to a number greater than one, truncates long strings to length ``truncate``\n",
            "     |          and align cells right.\n",
            "     |      vertical : bool, optional\n",
            "     |          If set to ``True``, print output rows vertically (one line\n",
            "     |          per column value).\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame([\n",
            "     |      ...     (14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            "     |      \n",
            "     |      Show only top 2 rows.\n",
            "     |      \n",
            "     |      >>> df.show(2)\n",
            "     |      +---+-----+\n",
            "     |      |age| name|\n",
            "     |      +---+-----+\n",
            "     |      | 14|  Tom|\n",
            "     |      | 23|Alice|\n",
            "     |      +---+-----+\n",
            "     |      only showing top 2 rows\n",
            "     |      \n",
            "     |      Show :class:`DataFrame` where the maximum number of characters is 3.\n",
            "     |      \n",
            "     |      >>> df.show(truncate=3)\n",
            "     |      +---+----+\n",
            "     |      |age|name|\n",
            "     |      +---+----+\n",
            "     |      | 14| Tom|\n",
            "     |      | 23| Ali|\n",
            "     |      | 16| Bob|\n",
            "     |      +---+----+\n",
            "     |      \n",
            "     |      Show :class:`DataFrame` vertically.\n",
            "     |      \n",
            "     |      >>> df.show(vertical=True)\n",
            "     |      -RECORD 0-----\n",
            "     |      age  | 14\n",
            "     |      name | Tom\n",
            "     |      -RECORD 1-----\n",
            "     |      age  | 23\n",
            "     |      name | Alice\n",
            "     |      -RECORD 2-----\n",
            "     |      age  | 16\n",
            "     |      name | Bob\n",
            "     |  \n",
            "     |  sort(self, *cols: Union[str, pyspark.sql.column.Column, List[Union[str, pyspark.sql.column.Column]]], **kwargs: Any) -> 'DataFrame'\n",
            "     |      Returns a new :class:`DataFrame` sorted by the specified column(s).\n",
            "     |      \n",
            "     |      .. versionadded:: 1.3.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      cols : str, list, or :class:`Column`, optional\n",
            "     |           list of :class:`Column` or column names to sort by.\n",
            "     |      \n",
            "     |      Other Parameters\n",
            "     |      ----------------\n",
            "     |      ascending : bool or list, optional, default True\n",
            "     |          boolean or list of boolean.\n",
            "     |          Sort ascending vs. descending. Specify list for multiple sort orders.\n",
            "     |          If a list is specified, the length of the list must equal the length of the `cols`.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          Sorted DataFrame.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> from pyspark.sql.functions import desc, asc\n",
            "     |      >>> df = spark.createDataFrame([\n",
            "     |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            "     |      \n",
            "     |      Sort the DataFrame in ascending order.\n",
            "     |      \n",
            "     |      >>> df.sort(asc(\"age\")).show()\n",
            "     |      +---+-----+\n",
            "     |      |age| name|\n",
            "     |      +---+-----+\n",
            "     |      |  2|Alice|\n",
            "     |      |  5|  Bob|\n",
            "     |      +---+-----+\n",
            "     |      \n",
            "     |      Sort the DataFrame in descending order.\n",
            "     |      \n",
            "     |      >>> df.sort(df.age.desc()).show()\n",
            "     |      +---+-----+\n",
            "     |      |age| name|\n",
            "     |      +---+-----+\n",
            "     |      |  5|  Bob|\n",
            "     |      |  2|Alice|\n",
            "     |      +---+-----+\n",
            "     |      >>> df.orderBy(df.age.desc()).show()\n",
            "     |      +---+-----+\n",
            "     |      |age| name|\n",
            "     |      +---+-----+\n",
            "     |      |  5|  Bob|\n",
            "     |      |  2|Alice|\n",
            "     |      +---+-----+\n",
            "     |      >>> df.sort(\"age\", ascending=False).show()\n",
            "     |      +---+-----+\n",
            "     |      |age| name|\n",
            "     |      +---+-----+\n",
            "     |      |  5|  Bob|\n",
            "     |      |  2|Alice|\n",
            "     |      +---+-----+\n",
            "     |      \n",
            "     |      Specify multiple columns\n",
            "     |      \n",
            "     |      >>> df = spark.createDataFrame([\n",
            "     |      ...     (2, \"Alice\"), (2, \"Bob\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            "     |      >>> df.orderBy(desc(\"age\"), \"name\").show()\n",
            "     |      +---+-----+\n",
            "     |      |age| name|\n",
            "     |      +---+-----+\n",
            "     |      |  5|  Bob|\n",
            "     |      |  2|Alice|\n",
            "     |      |  2|  Bob|\n",
            "     |      +---+-----+\n",
            "     |      \n",
            "     |      Specify multiple columns for sorting order at `ascending`.\n",
            "     |      \n",
            "     |      >>> df.orderBy([\"age\", \"name\"], ascending=[False, False]).show()\n",
            "     |      +---+-----+\n",
            "     |      |age| name|\n",
            "     |      +---+-----+\n",
            "     |      |  5|  Bob|\n",
            "     |      |  2|  Bob|\n",
            "     |      |  2|Alice|\n",
            "     |      +---+-----+\n",
            "     |  \n",
            "     |  sortWithinPartitions(self, *cols: Union[str, pyspark.sql.column.Column, List[Union[str, pyspark.sql.column.Column]]], **kwargs: Any) -> 'DataFrame'\n",
            "     |      Returns a new :class:`DataFrame` with each partition sorted by the specified column(s).\n",
            "     |      \n",
            "     |      .. versionadded:: 1.6.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      cols : str, list or :class:`Column`, optional\n",
            "     |          list of :class:`Column` or column names to sort by.\n",
            "     |      \n",
            "     |      Other Parameters\n",
            "     |      ----------------\n",
            "     |      ascending : bool or list, optional, default True\n",
            "     |          boolean or list of boolean.\n",
            "     |          Sort ascending vs. descending. Specify list for multiple sort orders.\n",
            "     |          If a list is specified, the length of the list must equal the length of the `cols`.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          DataFrame sorted by partitions.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            "     |      >>> df.sortWithinPartitions(\"age\", ascending=False)\n",
            "     |      DataFrame[age: bigint, name: string]\n",
            "     |  \n",
            "     |  subtract(self, other: 'DataFrame') -> 'DataFrame'\n",
            "     |      Return a new :class:`DataFrame` containing rows in this :class:`DataFrame`\n",
            "     |      but not in another :class:`DataFrame`.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.3.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      other : :class:`DataFrame`\n",
            "     |          Another :class:`DataFrame` that needs to be subtracted.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          Subtracted DataFrame.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      This is equivalent to `EXCEPT DISTINCT` in SQL.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df1 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
            "     |      >>> df2 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
            "     |      >>> df1.subtract(df2).show()\n",
            "     |      +---+---+\n",
            "     |      | C1| C2|\n",
            "     |      +---+---+\n",
            "     |      |  c|  4|\n",
            "     |      +---+---+\n",
            "     |  \n",
            "     |  summary(self, *statistics: str) -> 'DataFrame'\n",
            "     |      Computes specified statistics for numeric and string columns. Available statistics are:\n",
            "     |      - count\n",
            "     |      - mean\n",
            "     |      - stddev\n",
            "     |      - min\n",
            "     |      - max\n",
            "     |      - arbitrary approximate percentiles specified as a percentage (e.g., 75%)\n",
            "     |      \n",
            "     |      If no statistics are given, this function computes count, mean, stddev, min,\n",
            "     |      approximate quartiles (percentiles at 25%, 50%, and 75%), and max.\n",
            "     |      \n",
            "     |      .. versionadded:: 2.3.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      statistics : str, optional\n",
            "     |           Column names to calculate statistics by (default All columns).\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          A new DataFrame that provides statistics for the given DataFrame.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      This function is meant for exploratory data analysis, as we make no\n",
            "     |      guarantee about the backward compatibility of the schema of the resulting\n",
            "     |      :class:`DataFrame`.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame(\n",
            "     |      ...     [(\"Bob\", 13, 40.3, 150.5), (\"Alice\", 12, 37.8, 142.3), (\"Tom\", 11, 44.1, 142.2)],\n",
            "     |      ...     [\"name\", \"age\", \"weight\", \"height\"],\n",
            "     |      ... )\n",
            "     |      >>> df.select(\"age\", \"weight\", \"height\").summary().show()\n",
            "     |      +-------+----+------------------+-----------------+\n",
            "     |      |summary| age|            weight|           height|\n",
            "     |      +-------+----+------------------+-----------------+\n",
            "     |      |  count|   3|                 3|                3|\n",
            "     |      |   mean|12.0| 40.73333333333333|            145.0|\n",
            "     |      | stddev| 1.0|3.1722757341273704|4.763402145525822|\n",
            "     |      |    min|  11|              37.8|            142.2|\n",
            "     |      |    25%|  11|              37.8|            142.2|\n",
            "     |      |    50%|  12|              40.3|            142.3|\n",
            "     |      |    75%|  13|              44.1|            150.5|\n",
            "     |      |    max|  13|              44.1|            150.5|\n",
            "     |      +-------+----+------------------+-----------------+\n",
            "     |      \n",
            "     |      >>> df.select(\"age\", \"weight\", \"height\").summary(\"count\", \"min\", \"25%\", \"75%\", \"max\").show()\n",
            "     |      +-------+---+------+------+\n",
            "     |      |summary|age|weight|height|\n",
            "     |      +-------+---+------+------+\n",
            "     |      |  count|  3|     3|     3|\n",
            "     |      |    min| 11|  37.8| 142.2|\n",
            "     |      |    25%| 11|  37.8| 142.2|\n",
            "     |      |    75%| 13|  44.1| 150.5|\n",
            "     |      |    max| 13|  44.1| 150.5|\n",
            "     |      +-------+---+------+------+\n",
            "     |      \n",
            "     |      See Also\n",
            "     |      --------\n",
            "     |      DataFrame.display\n",
            "     |  \n",
            "     |  tail(self, num: int) -> List[pyspark.sql.types.Row]\n",
            "     |      Returns the last ``num`` rows as a :class:`list` of :class:`Row`.\n",
            "     |      \n",
            "     |      Running tail requires moving data into the application's driver process, and doing so with\n",
            "     |      a very large ``num`` can crash the driver process with OutOfMemoryError.\n",
            "     |      \n",
            "     |      .. versionadded:: 3.0.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      num : int\n",
            "     |          Number of records to return. Will return this number of records\n",
            "     |          or all records if the DataFrame contains less than this number of records.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      list\n",
            "     |          List of rows\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame(\n",
            "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            "     |      \n",
            "     |      >>> df.tail(2)\n",
            "     |      [Row(age=23, name='Alice'), Row(age=16, name='Bob')]\n",
            "     |  \n",
            "     |  take(self, num: int) -> List[pyspark.sql.types.Row]\n",
            "     |      Returns the first ``num`` rows as a :class:`list` of :class:`Row`.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.3.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      num : int\n",
            "     |          Number of records to return. Will return this number of records\n",
            "     |          or all records if the DataFrame contains less than this number of records..\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      list\n",
            "     |          List of rows\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame(\n",
            "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            "     |      \n",
            "     |      Return the first 2 rows of the :class:`DataFrame`.\n",
            "     |      \n",
            "     |      >>> df.take(2)\n",
            "     |      [Row(age=14, name='Tom'), Row(age=23, name='Alice')]\n",
            "     |  \n",
            "     |  to(self, schema: pyspark.sql.types.StructType) -> 'DataFrame'\n",
            "     |      Returns a new :class:`DataFrame` where each row is reconciled to match the specified\n",
            "     |      schema.\n",
            "     |      \n",
            "     |      .. versionadded:: 3.4.0\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      schema : :class:`StructType`\n",
            "     |          Specified schema.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          Reconciled DataFrame.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      * Reorder columns and/or inner fields by name to match the specified schema.\n",
            "     |      \n",
            "     |      * Project away columns and/or inner fields that are not needed by the specified schema.\n",
            "     |          Missing columns and/or inner fields (present in the specified schema but not input\n",
            "     |          DataFrame) lead to failures.\n",
            "     |      \n",
            "     |      * Cast the columns and/or inner fields to match the data types in the specified schema,\n",
            "     |          if the types are compatible, e.g., numeric to numeric (error if overflows), but\n",
            "     |          not string to int.\n",
            "     |      \n",
            "     |      * Carry over the metadata from the specified schema, while the columns and/or inner fields\n",
            "     |          still keep their own metadata if not overwritten by the specified schema.\n",
            "     |      \n",
            "     |      * Fail if the nullability is not compatible. For example, the column and/or inner field\n",
            "     |          is nullable but the specified schema requires them to be not nullable.\n",
            "     |      \n",
            "     |      Supports Spark Connect.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> from pyspark.sql.types import StructField, StringType\n",
            "     |      >>> df = spark.createDataFrame([(\"a\", 1)], [\"i\", \"j\"])\n",
            "     |      >>> df.schema\n",
            "     |      StructType([StructField('i', StringType(), True), StructField('j', LongType(), True)])\n",
            "     |      \n",
            "     |      >>> schema = StructType([StructField(\"j\", StringType()), StructField(\"i\", StringType())])\n",
            "     |      >>> df2 = df.to(schema)\n",
            "     |      >>> df2.schema\n",
            "     |      StructType([StructField('j', StringType(), True), StructField('i', StringType(), True)])\n",
            "     |      >>> df2.show()\n",
            "     |      +---+---+\n",
            "     |      |  j|  i|\n",
            "     |      +---+---+\n",
            "     |      |  1|  a|\n",
            "     |      +---+---+\n",
            "     |  \n",
            "     |  toDF(self, *cols: str) -> 'DataFrame'\n",
            "     |      Returns a new :class:`DataFrame` that with new specified column names\n",
            "     |      \n",
            "     |      .. versionadded:: 1.6.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      *cols : tuple\n",
            "     |          a tuple of string new column name. The length of the\n",
            "     |          list needs to be the same as the number of columns in the initial\n",
            "     |          :class:`DataFrame`\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          DataFrame with new column names.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame([(14, \"Tom\"), (23, \"Alice\"),\n",
            "     |      ...     (16, \"Bob\")], [\"age\", \"name\"])\n",
            "     |      >>> df.toDF('f1', 'f2').show()\n",
            "     |      +---+-----+\n",
            "     |      | f1|   f2|\n",
            "     |      +---+-----+\n",
            "     |      | 14|  Tom|\n",
            "     |      | 23|Alice|\n",
            "     |      | 16|  Bob|\n",
            "     |      +---+-----+\n",
            "     |  \n",
            "     |  toJSON(self, use_unicode: bool = True) -> pyspark.rdd.RDD[str]\n",
            "     |      Converts a :class:`DataFrame` into a :class:`RDD` of string.\n",
            "     |      \n",
            "     |      Each row is turned into a JSON document as one element in the returned RDD.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.3.0\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      use_unicode : bool, optional, default True\n",
            "     |          Whether to convert to unicode or not.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`RDD`\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            "     |      >>> df.toJSON().first()\n",
            "     |      '{\"age\":2,\"name\":\"Alice\"}'\n",
            "     |  \n",
            "     |  toLocalIterator(self, prefetchPartitions: bool = False) -> Iterator[pyspark.sql.types.Row]\n",
            "     |      Returns an iterator that contains all of the rows in this :class:`DataFrame`.\n",
            "     |      The iterator will consume as much memory as the largest partition in this\n",
            "     |      :class:`DataFrame`. With prefetch it may consume up to the memory of the 2 largest\n",
            "     |      partitions.\n",
            "     |      \n",
            "     |      .. versionadded:: 2.0.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      prefetchPartitions : bool, optional\n",
            "     |          If Spark should pre-fetch the next partition before it is needed.\n",
            "     |      \n",
            "     |          .. versionchanged:: 3.4.0\n",
            "     |              This argument does not take effect for Spark Connect.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      Iterator\n",
            "     |          Iterator of rows.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame(\n",
            "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            "     |      >>> list(df.toLocalIterator())\n",
            "     |      [Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\n",
            "     |  \n",
            "     |  to_koalas(self, index_col: Union[str, List[str], NoneType] = None) -> 'PandasOnSparkDataFrame'\n",
            "     |      # Keep to_koalas for backward compatibility for now.\n",
            "     |  \n",
            "     |  to_pandas_on_spark(self, index_col: Union[str, List[str], NoneType] = None) -> 'PandasOnSparkDataFrame'\n",
            "     |      # Keep to_pandas_on_spark for backward compatibility for now.\n",
            "     |  \n",
            "     |  transform(self, func: Callable[..., ForwardRef('DataFrame')], *args: Any, **kwargs: Any) -> 'DataFrame'\n",
            "     |      Returns a new :class:`DataFrame`. Concise syntax for chaining custom transformations.\n",
            "     |      \n",
            "     |      .. versionadded:: 3.0.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      func : function\n",
            "     |          a function that takes and returns a :class:`DataFrame`.\n",
            "     |      *args\n",
            "     |          Positional arguments to pass to func.\n",
            "     |      \n",
            "     |          .. versionadded:: 3.3.0\n",
            "     |      **kwargs\n",
            "     |          Keyword arguments to pass to func.\n",
            "     |      \n",
            "     |          .. versionadded:: 3.3.0\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          Transformed DataFrame.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> from pyspark.sql.functions import col\n",
            "     |      >>> df = spark.createDataFrame([(1, 1.0), (2, 2.0)], [\"int\", \"float\"])\n",
            "     |      >>> def cast_all_to_int(input_df):\n",
            "     |      ...     return input_df.select([col(col_name).cast(\"int\") for col_name in input_df.columns])\n",
            "     |      ...\n",
            "     |      >>> def sort_columns_asc(input_df):\n",
            "     |      ...     return input_df.select(*sorted(input_df.columns))\n",
            "     |      ...\n",
            "     |      >>> df.transform(cast_all_to_int).transform(sort_columns_asc).show()\n",
            "     |      +-----+---+\n",
            "     |      |float|int|\n",
            "     |      +-----+---+\n",
            "     |      |    1|  1|\n",
            "     |      |    2|  2|\n",
            "     |      +-----+---+\n",
            "     |      \n",
            "     |      >>> def add_n(input_df, n):\n",
            "     |      ...     return input_df.select([(col(col_name) + n).alias(col_name)\n",
            "     |      ...                             for col_name in input_df.columns])\n",
            "     |      >>> df.transform(add_n, 1).transform(add_n, n=10).show()\n",
            "     |      +---+-----+\n",
            "     |      |int|float|\n",
            "     |      +---+-----+\n",
            "     |      | 12| 12.0|\n",
            "     |      | 13| 13.0|\n",
            "     |      +---+-----+\n",
            "     |  \n",
            "     |  union(self, other: 'DataFrame') -> 'DataFrame'\n",
            "     |      Return a new :class:`DataFrame` containing the union of rows in this and another\n",
            "     |      :class:`DataFrame`.\n",
            "     |      \n",
            "     |      .. versionadded:: 2.0.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      other : :class:`DataFrame`\n",
            "     |          Another :class:`DataFrame` that needs to be unioned.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          A new :class:`DataFrame` containing the combined rows with corresponding columns.\n",
            "     |      \n",
            "     |      See Also\n",
            "     |      --------\n",
            "     |      DataFrame.unionAll\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      This method performs a SQL-style set union of the rows from both `DataFrame` objects,\n",
            "     |      with no automatic deduplication of elements.\n",
            "     |      \n",
            "     |      Use the `distinct()` method to perform deduplication of rows.\n",
            "     |      \n",
            "     |      The method resolves columns by position (not by name), following the standard behavior\n",
            "     |      in SQL.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      Example 1: Combining two DataFrames with the same schema\n",
            "     |      \n",
            "     |      >>> df1 = spark.createDataFrame([(1, 'A'), (2, 'B')], ['id', 'value'])\n",
            "     |      >>> df2 = spark.createDataFrame([(3, 'C'), (4, 'D')], ['id', 'value'])\n",
            "     |      >>> df3 = df1.union(df2)\n",
            "     |      >>> df3.show()\n",
            "     |      +---+-----+\n",
            "     |      | id|value|\n",
            "     |      +---+-----+\n",
            "     |      |  1|    A|\n",
            "     |      |  2|    B|\n",
            "     |      |  3|    C|\n",
            "     |      |  4|    D|\n",
            "     |      +---+-----+\n",
            "     |      \n",
            "     |      Example 2: Combining two DataFrames with different schemas\n",
            "     |      \n",
            "     |      >>> from pyspark.sql.functions import lit\n",
            "     |      >>> df1 = spark.createDataFrame([(\"Alice\", 1), (\"Bob\", 2)], [\"name\", \"id\"])\n",
            "     |      >>> df2 = spark.createDataFrame([(3, \"Charlie\"), (4, \"Dave\")], [\"id\", \"name\"])\n",
            "     |      >>> df1 = df1.withColumn(\"age\", lit(30))\n",
            "     |      >>> df2 = df2.withColumn(\"age\", lit(40))\n",
            "     |      >>> df3 = df1.union(df2)\n",
            "     |      >>> df3.show()\n",
            "     |      +-----+-------+---+\n",
            "     |      | name|     id|age|\n",
            "     |      +-----+-------+---+\n",
            "     |      |Alice|      1| 30|\n",
            "     |      |  Bob|      2| 30|\n",
            "     |      |    3|Charlie| 40|\n",
            "     |      |    4|   Dave| 40|\n",
            "     |      +-----+-------+---+\n",
            "     |      \n",
            "     |      Example 3: Combining two DataFrames with mismatched columns\n",
            "     |      \n",
            "     |      >>> df1 = spark.createDataFrame([(1, 2)], [\"A\", \"B\"])\n",
            "     |      >>> df2 = spark.createDataFrame([(3, 4)], [\"C\", \"D\"])\n",
            "     |      >>> df3 = df1.union(df2)\n",
            "     |      >>> df3.show()\n",
            "     |      +---+---+\n",
            "     |      |  A|  B|\n",
            "     |      +---+---+\n",
            "     |      |  1|  2|\n",
            "     |      |  3|  4|\n",
            "     |      +---+---+\n",
            "     |      \n",
            "     |      Example 4: Combining duplicate rows from two different DataFrames\n",
            "     |      \n",
            "     |      >>> df1 = spark.createDataFrame([(1, 'A'), (2, 'B'), (3, 'C')], ['id', 'value'])\n",
            "     |      >>> df2 = spark.createDataFrame([(3, 'C'), (4, 'D')], ['id', 'value'])\n",
            "     |      >>> df3 = df1.union(df2).distinct().sort(\"id\")\n",
            "     |      >>> df3.show()\n",
            "     |      +---+-----+\n",
            "     |      | id|value|\n",
            "     |      +---+-----+\n",
            "     |      |  1|    A|\n",
            "     |      |  2|    B|\n",
            "     |      |  3|    C|\n",
            "     |      |  4|    D|\n",
            "     |      +---+-----+\n",
            "     |  \n",
            "     |  unionAll(self, other: 'DataFrame') -> 'DataFrame'\n",
            "     |      Return a new :class:`DataFrame` containing the union of rows in this and another\n",
            "     |      :class:`DataFrame`.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.3.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      other : :class:`DataFrame`\n",
            "     |          Another :class:`DataFrame` that needs to be combined\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          A new :class:`DataFrame` containing combined rows from both dataframes.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      This method combines all rows from both `DataFrame` objects with no automatic\n",
            "     |      deduplication of elements.\n",
            "     |      \n",
            "     |      Use the `distinct()` method to perform deduplication of rows.\n",
            "     |      \n",
            "     |      :func:`unionAll` is an alias to :func:`union`\n",
            "     |      \n",
            "     |      See Also\n",
            "     |      --------\n",
            "     |      DataFrame.union\n",
            "     |  \n",
            "     |  unionByName(self, other: 'DataFrame', allowMissingColumns: bool = False) -> 'DataFrame'\n",
            "     |      Returns a new :class:`DataFrame` containing union of rows in this and another\n",
            "     |      :class:`DataFrame`.\n",
            "     |      \n",
            "     |      This method performs a union operation on both input DataFrames, resolving columns by\n",
            "     |      name (rather than position). When `allowMissingColumns` is True, missing columns will\n",
            "     |      be filled with null.\n",
            "     |      \n",
            "     |      .. versionadded:: 2.3.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      other : :class:`DataFrame`\n",
            "     |          Another :class:`DataFrame` that needs to be combined.\n",
            "     |      allowMissingColumns : bool, optional, default False\n",
            "     |         Specify whether to allow missing columns.\n",
            "     |      \n",
            "     |         .. versionadded:: 3.1.0\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          A new :class:`DataFrame` containing the combined rows with corresponding\n",
            "     |          columns of the two given DataFrames.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      Example 1: Union of two DataFrames with same columns in different order.\n",
            "     |      \n",
            "     |      >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
            "     |      >>> df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col0\"])\n",
            "     |      >>> df1.unionByName(df2).show()\n",
            "     |      +----+----+----+\n",
            "     |      |col0|col1|col2|\n",
            "     |      +----+----+----+\n",
            "     |      |   1|   2|   3|\n",
            "     |      |   6|   4|   5|\n",
            "     |      +----+----+----+\n",
            "     |      \n",
            "     |      Example 2: Union with missing columns and setting `allowMissingColumns=True`.\n",
            "     |      \n",
            "     |      >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
            "     |      >>> df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col3\"])\n",
            "     |      >>> df1.unionByName(df2, allowMissingColumns=True).show()\n",
            "     |      +----+----+----+----+\n",
            "     |      |col0|col1|col2|col3|\n",
            "     |      +----+----+----+----+\n",
            "     |      |   1|   2|   3|NULL|\n",
            "     |      |NULL|   4|   5|   6|\n",
            "     |      +----+----+----+----+\n",
            "     |      \n",
            "     |      Example 3: Union of two DataFrames with few common columns.\n",
            "     |      \n",
            "     |      >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
            "     |      >>> df2 = spark.createDataFrame([[4, 5, 6, 7]], [\"col1\", \"col2\", \"col3\", \"col4\"])\n",
            "     |      >>> df1.unionByName(df2, allowMissingColumns=True).show()\n",
            "     |      +----+----+----+----+----+\n",
            "     |      |col0|col1|col2|col3|col4|\n",
            "     |      +----+----+----+----+----+\n",
            "     |      |   1|   2|   3|NULL|NULL|\n",
            "     |      |NULL|   4|   5|   6|   7|\n",
            "     |      +----+----+----+----+----+\n",
            "     |      \n",
            "     |      Example 4: Union of two DataFrames with completely different columns.\n",
            "     |      \n",
            "     |      >>> df1 = spark.createDataFrame([[0, 1, 2]], [\"col0\", \"col1\", \"col2\"])\n",
            "     |      >>> df2 = spark.createDataFrame([[3, 4, 5]], [\"col3\", \"col4\", \"col5\"])\n",
            "     |      >>> df1.unionByName(df2, allowMissingColumns=True).show()\n",
            "     |      +----+----+----+----+----+----+\n",
            "     |      |col0|col1|col2|col3|col4|col5|\n",
            "     |      +----+----+----+----+----+----+\n",
            "     |      |   0|   1|   2|NULL|NULL|NULL|\n",
            "     |      |NULL|NULL|NULL|   3|   4|   5|\n",
            "     |      +----+----+----+----+----+----+\n",
            "     |  \n",
            "     |  unpersist(self, blocking: bool = False) -> 'DataFrame'\n",
            "     |      Marks the :class:`DataFrame` as non-persistent, and remove all blocks for it from\n",
            "     |      memory and disk.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.3.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      `blocking` default has changed to ``False`` to match Scala in 2.0.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      blocking : bool\n",
            "     |          Whether to block until all blocks are deleted.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          Unpersisted DataFrame.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.range(1)\n",
            "     |      >>> df.persist()\n",
            "     |      DataFrame[id: bigint]\n",
            "     |      >>> df.unpersist()\n",
            "     |      DataFrame[id: bigint]\n",
            "     |      >>> df = spark.range(1)\n",
            "     |      >>> df.unpersist(True)\n",
            "     |      DataFrame[id: bigint]\n",
            "     |  \n",
            "     |  unpivot(self, ids: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName')], Tuple[ForwardRef('ColumnOrName'), ...]], values: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName')], Tuple[ForwardRef('ColumnOrName'), ...], NoneType], variableColumnName: str, valueColumnName: str) -> 'DataFrame'\n",
            "     |      Unpivot a DataFrame from wide format to long format, optionally leaving\n",
            "     |      identifier columns set. This is the reverse to `groupBy(...).pivot(...).agg(...)`,\n",
            "     |      except for the aggregation, which cannot be reversed.\n",
            "     |      \n",
            "     |      This function is useful to massage a DataFrame into a format where some\n",
            "     |      columns are identifier columns (\"ids\"), while all other columns (\"values\")\n",
            "     |      are \"unpivoted\" to the rows, leaving just two non-id columns, named as given\n",
            "     |      by `variableColumnName` and `valueColumnName`.\n",
            "     |      \n",
            "     |      When no \"id\" columns are given, the unpivoted DataFrame consists of only the\n",
            "     |      \"variable\" and \"value\" columns.\n",
            "     |      \n",
            "     |      The `values` columns must not be empty so at least one value must be given to be unpivoted.\n",
            "     |      When `values` is `None`, all non-id columns will be unpivoted.\n",
            "     |      \n",
            "     |      All \"value\" columns must share a least common data type. Unless they are the same data type,\n",
            "     |      all \"value\" columns are cast to the nearest common data type. For instance, types\n",
            "     |      `IntegerType` and `LongType` are cast to `LongType`, while `IntegerType` and `StringType`\n",
            "     |      do not have a common data type and `unpivot` fails.\n",
            "     |      \n",
            "     |      .. versionadded:: 3.4.0\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      ids : str, Column, tuple, list\n",
            "     |          Column(s) to use as identifiers. Can be a single column or column name,\n",
            "     |          or a list or tuple for multiple columns.\n",
            "     |      values : str, Column, tuple, list, optional\n",
            "     |          Column(s) to unpivot. Can be a single column or column name, or a list or tuple\n",
            "     |          for multiple columns. If specified, must not be empty. If not specified, uses all\n",
            "     |          columns that are not set as `ids`.\n",
            "     |      variableColumnName : str\n",
            "     |          Name of the variable column.\n",
            "     |      valueColumnName : str\n",
            "     |          Name of the value column.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          Unpivoted DataFrame.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      Supports Spark Connect.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame(\n",
            "     |      ...     [(1, 11, 1.1), (2, 12, 1.2)],\n",
            "     |      ...     [\"id\", \"int\", \"double\"],\n",
            "     |      ... )\n",
            "     |      >>> df.show()\n",
            "     |      +---+---+------+\n",
            "     |      | id|int|double|\n",
            "     |      +---+---+------+\n",
            "     |      |  1| 11|   1.1|\n",
            "     |      |  2| 12|   1.2|\n",
            "     |      +---+---+------+\n",
            "     |      \n",
            "     |      >>> df.unpivot(\"id\", [\"int\", \"double\"], \"var\", \"val\").show()\n",
            "     |      +---+------+----+\n",
            "     |      | id|   var| val|\n",
            "     |      +---+------+----+\n",
            "     |      |  1|   int|11.0|\n",
            "     |      |  1|double| 1.1|\n",
            "     |      |  2|   int|12.0|\n",
            "     |      |  2|double| 1.2|\n",
            "     |      +---+------+----+\n",
            "     |      \n",
            "     |      See Also\n",
            "     |      --------\n",
            "     |      DataFrame.melt\n",
            "     |  \n",
            "     |  where = filter(self, condition)\n",
            "     |      :func:`where` is an alias for :func:`filter`.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.3\n",
            "     |  \n",
            "     |  withColumn(self, colName: str, col: pyspark.sql.column.Column) -> 'DataFrame'\n",
            "     |      Returns a new :class:`DataFrame` by adding a column or replacing the\n",
            "     |      existing column that has the same name.\n",
            "     |      \n",
            "     |      The column expression must be an expression over this :class:`DataFrame`; attempting to add\n",
            "     |      a column from some other :class:`DataFrame` will raise an error.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.3.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      colName : str\n",
            "     |          string, name of the new column.\n",
            "     |      col : :class:`Column`\n",
            "     |          a :class:`Column` expression for the new column.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          DataFrame with new or replaced column.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      This method introduces a projection internally. Therefore, calling it multiple\n",
            "     |      times, for instance, via loops in order to add multiple columns can generate big\n",
            "     |      plans which can cause performance issues and even `StackOverflowException`.\n",
            "     |      To avoid this, use :func:`select` with multiple columns at once.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            "     |      >>> df.withColumn('age2', df.age + 2).show()\n",
            "     |      +---+-----+----+\n",
            "     |      |age| name|age2|\n",
            "     |      +---+-----+----+\n",
            "     |      |  2|Alice|   4|\n",
            "     |      |  5|  Bob|   7|\n",
            "     |      +---+-----+----+\n",
            "     |  \n",
            "     |  withColumnRenamed(self, existing: str, new: str) -> 'DataFrame'\n",
            "     |      Returns a new :class:`DataFrame` by renaming an existing column.\n",
            "     |      This is a no-op if the schema doesn't contain the given column name.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.3.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      existing : str\n",
            "     |          string, name of the existing column to rename.\n",
            "     |      new : str\n",
            "     |          string, new name of the column.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          DataFrame with renamed column.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            "     |      >>> df.withColumnRenamed('age', 'age2').show()\n",
            "     |      +----+-----+\n",
            "     |      |age2| name|\n",
            "     |      +----+-----+\n",
            "     |      |   2|Alice|\n",
            "     |      |   5|  Bob|\n",
            "     |      +----+-----+\n",
            "     |  \n",
            "     |  withColumns(self, *colsMap: Dict[str, pyspark.sql.column.Column]) -> 'DataFrame'\n",
            "     |      Returns a new :class:`DataFrame` by adding multiple columns or replacing the\n",
            "     |      existing columns that have the same names.\n",
            "     |      \n",
            "     |      The colsMap is a map of column name and column, the column must only refer to attributes\n",
            "     |      supplied by this Dataset. It is an error to add columns that refer to some other Dataset.\n",
            "     |      \n",
            "     |      .. versionadded:: 3.3.0\n",
            "     |         Added support for multiple columns adding\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      colsMap : dict\n",
            "     |          a dict of column name and :class:`Column`. Currently, only a single map is supported.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          DataFrame with new or replaced columns.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            "     |      >>> df.withColumns({'age2': df.age + 2, 'age3': df.age + 3}).show()\n",
            "     |      +---+-----+----+----+\n",
            "     |      |age| name|age2|age3|\n",
            "     |      +---+-----+----+----+\n",
            "     |      |  2|Alice|   4|   5|\n",
            "     |      |  5|  Bob|   7|   8|\n",
            "     |      +---+-----+----+----+\n",
            "     |  \n",
            "     |  withColumnsRenamed(self, colsMap: Dict[str, str]) -> 'DataFrame'\n",
            "     |      Returns a new :class:`DataFrame` by renaming multiple columns.\n",
            "     |      This is a no-op if the schema doesn't contain the given column names.\n",
            "     |      \n",
            "     |      .. versionadded:: 3.4.0\n",
            "     |         Added support for multiple columns renaming\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      colsMap : dict\n",
            "     |          a dict of existing column names and corresponding desired column names.\n",
            "     |          Currently, only a single map is supported.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          DataFrame with renamed columns.\n",
            "     |      \n",
            "     |      See Also\n",
            "     |      --------\n",
            "     |      :meth:`withColumnRenamed`\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      Support Spark Connect\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            "     |      >>> df = df.withColumns({'age2': df.age + 2, 'age3': df.age + 3})\n",
            "     |      >>> df.withColumnsRenamed({'age2': 'age4', 'age3': 'age5'}).show()\n",
            "     |      +---+-----+----+----+\n",
            "     |      |age| name|age4|age5|\n",
            "     |      +---+-----+----+----+\n",
            "     |      |  2|Alice|   4|   5|\n",
            "     |      |  5|  Bob|   7|   8|\n",
            "     |      +---+-----+----+----+\n",
            "     |  \n",
            "     |  withMetadata(self, columnName: str, metadata: Dict[str, Any]) -> 'DataFrame'\n",
            "     |      Returns a new :class:`DataFrame` by updating an existing column with metadata.\n",
            "     |      \n",
            "     |      .. versionadded:: 3.3.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      columnName : str\n",
            "     |          string, name of the existing column to update the metadata.\n",
            "     |      metadata : dict\n",
            "     |          dict, new metadata to be assigned to df.schema[columnName].metadata\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          DataFrame with updated metadata column.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            "     |      >>> df_meta = df.withMetadata('age', {'foo': 'bar'})\n",
            "     |      >>> df_meta.schema['age'].metadata\n",
            "     |      {'foo': 'bar'}\n",
            "     |  \n",
            "     |  withWatermark(self, eventTime: str, delayThreshold: str) -> 'DataFrame'\n",
            "     |      Defines an event time watermark for this :class:`DataFrame`. A watermark tracks a point\n",
            "     |      in time before which we assume no more late data is going to arrive.\n",
            "     |      \n",
            "     |      Spark will use this watermark for several purposes:\n",
            "     |        - To know when a given time window aggregation can be finalized and thus can be emitted\n",
            "     |          when using output modes that do not allow updates.\n",
            "     |      \n",
            "     |        - To minimize the amount of state that we need to keep for on-going aggregations.\n",
            "     |      \n",
            "     |      The current watermark is computed by looking at the `MAX(eventTime)` seen across\n",
            "     |      all of the partitions in the query minus a user specified `delayThreshold`.  Due to the cost\n",
            "     |      of coordinating this value across partitions, the actual watermark used is only guaranteed\n",
            "     |      to be at least `delayThreshold` behind the actual event time.  In some cases we may still\n",
            "     |      process records that arrive more than `delayThreshold` late.\n",
            "     |      \n",
            "     |      .. versionadded:: 2.1.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.5.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      eventTime : str\n",
            "     |          the name of the column that contains the event time of the row.\n",
            "     |      delayThreshold : str\n",
            "     |          the minimum delay to wait to data to arrive late, relative to the\n",
            "     |          latest record that has been processed in the form of an interval\n",
            "     |          (e.g. \"1 minute\" or \"5 hours\").\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          Watermarked DataFrame\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      This is a feature only for Structured Streaming.\n",
            "     |      \n",
            "     |      This API is evolving.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> from pyspark.sql import Row\n",
            "     |      >>> from pyspark.sql.functions import timestamp_seconds\n",
            "     |      >>> df = spark.readStream.format(\"rate\").load().selectExpr(\n",
            "     |      ...     \"value % 5 AS value\", \"timestamp\")\n",
            "     |      >>> df.select(\"value\", df.timestamp.alias(\"time\")).withWatermark(\"time\", '10 minutes')\n",
            "     |      DataFrame[value: bigint, time: timestamp]\n",
            "     |      \n",
            "     |      Group the data by window and value (0 - 4), and compute the count of each group.\n",
            "     |      \n",
            "     |      >>> import time\n",
            "     |      >>> from pyspark.sql.functions import window\n",
            "     |      >>> query = (df\n",
            "     |      ...     .withWatermark(\"timestamp\", \"10 minutes\")\n",
            "     |      ...     .groupBy(\n",
            "     |      ...         window(df.timestamp, \"10 minutes\", \"5 minutes\"),\n",
            "     |      ...         df.value)\n",
            "     |      ...     ).count().writeStream.outputMode(\"complete\").format(\"console\").start()\n",
            "     |      >>> time.sleep(3)\n",
            "     |      >>> query.stop()\n",
            "     |  \n",
            "     |  writeTo(self, table: str) -> pyspark.sql.readwriter.DataFrameWriterV2\n",
            "     |      Create a write configuration builder for v2 sources.\n",
            "     |      \n",
            "     |      This builder is used to configure and execute write operations.\n",
            "     |      \n",
            "     |      For example, to append or create or replace existing tables.\n",
            "     |      \n",
            "     |      .. versionadded:: 3.1.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      table : str\n",
            "     |          Target table name to write to.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrameWriterV2`\n",
            "     |          DataFrameWriterV2 to use further to specify how to save the data\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame(\n",
            "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            "     |      >>> df.writeTo(\"catalog.db.table\").append()  # doctest: +SKIP\n",
            "     |      >>> df.writeTo(                              # doctest: +SKIP\n",
            "     |      ...     \"catalog.db.table\"\n",
            "     |      ... ).partitionedBy(\"col\").createOrReplace()\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Readonly properties defined here:\n",
            "     |  \n",
            "     |  columns\n",
            "     |      Retrieves the names of all columns in the :class:`DataFrame` as a list.\n",
            "     |      \n",
            "     |      The order of the column names in the list reflects their order in the DataFrame.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.3.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      list\n",
            "     |          List of column names in the DataFrame.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      Example 1: Retrieve column names of a DataFrame\n",
            "     |      \n",
            "     |      >>> df = spark.createDataFrame(\n",
            "     |      ...     [(14, \"Tom\", \"CA\"), (23, \"Alice\", \"NY\"), (16, \"Bob\", \"TX\")],\n",
            "     |      ...     [\"age\", \"name\", \"state\"]\n",
            "     |      ... )\n",
            "     |      >>> df.columns\n",
            "     |      ['age', 'name', 'state']\n",
            "     |      \n",
            "     |      Example 2: Using column names to project specific columns\n",
            "     |      \n",
            "     |      >>> selected_cols = [col for col in df.columns if col != \"age\"]\n",
            "     |      >>> df.select(selected_cols).show()\n",
            "     |      +-----+-----+\n",
            "     |      | name|state|\n",
            "     |      +-----+-----+\n",
            "     |      |  Tom|   CA|\n",
            "     |      |Alice|   NY|\n",
            "     |      |  Bob|   TX|\n",
            "     |      +-----+-----+\n",
            "     |      \n",
            "     |      Example 3: Checking if a specific column exists in a DataFrame\n",
            "     |      \n",
            "     |      >>> \"state\" in df.columns\n",
            "     |      True\n",
            "     |      >>> \"salary\" in df.columns\n",
            "     |      False\n",
            "     |      \n",
            "     |      Example 4: Iterating over columns to apply a transformation\n",
            "     |      \n",
            "     |      >>> import pyspark.sql.functions as f\n",
            "     |      >>> for col_name in df.columns:\n",
            "     |      ...     df = df.withColumn(col_name, f.upper(f.col(col_name)))\n",
            "     |      >>> df.show()\n",
            "     |      +---+-----+-----+\n",
            "     |      |age| name|state|\n",
            "     |      +---+-----+-----+\n",
            "     |      | 14|  TOM|   CA|\n",
            "     |      | 23|ALICE|   NY|\n",
            "     |      | 16|  BOB|   TX|\n",
            "     |      +---+-----+-----+\n",
            "     |      \n",
            "     |      Example 5: Renaming columns and checking the updated column names\n",
            "     |      \n",
            "     |      >>> df = df.withColumnRenamed(\"name\", \"first_name\")\n",
            "     |      >>> df.columns\n",
            "     |      ['age', 'first_name', 'state']\n",
            "     |      \n",
            "     |      Example 6: Using the `columns` property to ensure two DataFrames have the\n",
            "     |      same columns before a union\n",
            "     |      \n",
            "     |      >>> df2 = spark.createDataFrame(\n",
            "     |      ...     [(30, \"Eve\", \"FL\"), (40, \"Sam\", \"WA\")], [\"age\", \"name\", \"location\"])\n",
            "     |      >>> df.columns == df2.columns\n",
            "     |      False\n",
            "     |  \n",
            "     |  dtypes\n",
            "     |      Returns all column names and their data types as a list.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.3.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      list\n",
            "     |          List of columns as tuple pairs.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame(\n",
            "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            "     |      >>> df.dtypes\n",
            "     |      [('age', 'bigint'), ('name', 'string')]\n",
            "     |  \n",
            "     |  isStreaming\n",
            "     |      Returns ``True`` if this :class:`DataFrame` contains one or more sources that\n",
            "     |      continuously return data as it arrives. A :class:`DataFrame` that reads data from a\n",
            "     |      streaming source must be executed as a :class:`StreamingQuery` using the :func:`start`\n",
            "     |      method in :class:`DataStreamWriter`.  Methods that return a single answer, (e.g.,\n",
            "     |      :func:`count` or :func:`collect`) will throw an :class:`AnalysisException` when there\n",
            "     |      is a streaming source present.\n",
            "     |      \n",
            "     |      .. versionadded:: 2.0.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      This API is evolving.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      bool\n",
            "     |          Whether it's streaming DataFrame or not.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.readStream.format(\"rate\").load()\n",
            "     |      >>> df.isStreaming\n",
            "     |      True\n",
            "     |  \n",
            "     |  na\n",
            "     |      Returns a :class:`DataFrameNaFunctions` for handling missing values.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.3.1\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrameNaFunctions`\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.sql(\"SELECT 1 AS c1, int(NULL) AS c2\")\n",
            "     |      >>> type(df.na)\n",
            "     |      <class '...dataframe.DataFrameNaFunctions'>\n",
            "     |      \n",
            "     |      Replace the missing values as 2.\n",
            "     |      \n",
            "     |      >>> df.na.fill(2).show()\n",
            "     |      +---+---+\n",
            "     |      | c1| c2|\n",
            "     |      +---+---+\n",
            "     |      |  1|  2|\n",
            "     |      +---+---+\n",
            "     |  \n",
            "     |  rdd\n",
            "     |      Returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.3.0\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`RDD`\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.range(1)\n",
            "     |      >>> type(df.rdd)\n",
            "     |      <class 'pyspark.rdd.RDD'>\n",
            "     |  \n",
            "     |  schema\n",
            "     |      Returns the schema of this :class:`DataFrame` as a :class:`pyspark.sql.types.StructType`.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.3.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`StructType`\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame(\n",
            "     |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            "     |      \n",
            "     |      Retrieve the schema of the current DataFrame.\n",
            "     |      \n",
            "     |      >>> df.schema\n",
            "     |      StructType([StructField('age', LongType(), True),\n",
            "     |                  StructField('name', StringType(), True)])\n",
            "     |  \n",
            "     |  sparkSession\n",
            "     |      Returns Spark session that created this :class:`DataFrame`.\n",
            "     |      \n",
            "     |      .. versionadded:: 3.3.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`SparkSession`\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.range(1)\n",
            "     |      >>> type(df.sparkSession)\n",
            "     |      <class '...session.SparkSession'>\n",
            "     |  \n",
            "     |  sql_ctx\n",
            "     |  \n",
            "     |  stat\n",
            "     |      Returns a :class:`DataFrameStatFunctions` for statistic functions.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.4.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrameStatFunctions`\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> import pyspark.sql.functions as f\n",
            "     |      >>> df = spark.range(3).withColumn(\"c\", f.expr(\"id + 1\"))\n",
            "     |      >>> type(df.stat)\n",
            "     |      <class '...dataframe.DataFrameStatFunctions'>\n",
            "     |      >>> df.stat.corr(\"id\", \"c\")\n",
            "     |      1.0\n",
            "     |  \n",
            "     |  storageLevel\n",
            "     |      Get the :class:`DataFrame`'s current storage level.\n",
            "     |      \n",
            "     |      .. versionadded:: 2.1.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`StorageLevel`\n",
            "     |          Currently defined storage level.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df1 = spark.range(10)\n",
            "     |      >>> df1.storageLevel\n",
            "     |      StorageLevel(False, False, False, False, 1)\n",
            "     |      >>> df1.cache().storageLevel\n",
            "     |      StorageLevel(True, True, False, True, 1)\n",
            "     |      \n",
            "     |      >>> df2 = spark.range(5)\n",
            "     |      >>> df2.persist(StorageLevel.DISK_ONLY_2).storageLevel\n",
            "     |      StorageLevel(True, False, False, False, 2)\n",
            "     |  \n",
            "     |  write\n",
            "     |      Interface for saving the content of the non-streaming :class:`DataFrame` out into external\n",
            "     |      storage.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.4.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrameWriter`\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            "     |      >>> type(df.write)\n",
            "     |      <class '...readwriter.DataFrameWriter'>\n",
            "     |      \n",
            "     |      Write the DataFrame as a table.\n",
            "     |      \n",
            "     |      >>> _ = spark.sql(\"DROP TABLE IF EXISTS tab2\")\n",
            "     |      >>> df.write.saveAsTable(\"tab2\")\n",
            "     |      >>> _ = spark.sql(\"DROP TABLE tab2\")\n",
            "     |  \n",
            "     |  writeStream\n",
            "     |      Interface for saving the content of the streaming :class:`DataFrame` out into external\n",
            "     |      storage.\n",
            "     |      \n",
            "     |      .. versionadded:: 2.0.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.5.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      This API is evolving.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataStreamWriter`\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> import tempfile\n",
            "     |      >>> df = spark.readStream.format(\"rate\").load()\n",
            "     |      >>> type(df.writeStream)\n",
            "     |      <class '...streaming.readwriter.DataStreamWriter'>\n",
            "     |      \n",
            "     |      >>> with tempfile.TemporaryDirectory() as d:\n",
            "     |      ...     # Create a table with Rate source.\n",
            "     |      ...     df.writeStream.toTable(\n",
            "     |      ...         \"my_table\", checkpointLocation=d)\n",
            "     |      <...streaming.query.StreamingQuery object at 0x...>\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from pyspark.sql.pandas.map_ops.PandasMapOpsMixin:\n",
            "     |  \n",
            "     |  mapInArrow(self, func: 'ArrowMapIterFunction', schema: Union[pyspark.sql.types.StructType, str], barrier: bool = False) -> 'DataFrame'\n",
            "     |      Maps an iterator of batches in the current :class:`DataFrame` using a Python native\n",
            "     |      function that takes and outputs a PyArrow's `RecordBatch`, and returns the result as a\n",
            "     |      :class:`DataFrame`.\n",
            "     |      \n",
            "     |      The function should take an iterator of `pyarrow.RecordBatch`\\s and return\n",
            "     |      another iterator of `pyarrow.RecordBatch`\\s. All columns are passed\n",
            "     |      together as an iterator of `pyarrow.RecordBatch`\\s to the function and the\n",
            "     |      returned iterator of `pyarrow.RecordBatch`\\s are combined as a :class:`DataFrame`.\n",
            "     |      Each `pyarrow.RecordBatch` size can be controlled by\n",
            "     |      `spark.sql.execution.arrow.maxRecordsPerBatch`. The size of the function's input and\n",
            "     |      output can be different.\n",
            "     |      \n",
            "     |      .. versionadded:: 3.3.0\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      func : function\n",
            "     |          a Python native function that takes an iterator of `pyarrow.RecordBatch`\\s, and\n",
            "     |          outputs an iterator of `pyarrow.RecordBatch`\\s.\n",
            "     |      schema : :class:`pyspark.sql.types.DataType` or str\n",
            "     |          the return type of the `func` in PySpark. The value can be either a\n",
            "     |          :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
            "     |      barrier : bool, optional, default False\n",
            "     |          Use barrier mode execution.\n",
            "     |      \n",
            "     |          .. versionadded: 3.5.0\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> import pyarrow  # doctest: +SKIP\n",
            "     |      >>> df = spark.createDataFrame([(1, 21), (2, 30)], (\"id\", \"age\"))\n",
            "     |      >>> def filter_func(iterator):\n",
            "     |      ...     for batch in iterator:\n",
            "     |      ...         pdf = batch.to_pandas()\n",
            "     |      ...         yield pyarrow.RecordBatch.from_pandas(pdf[pdf.id == 1])\n",
            "     |      >>> df.mapInArrow(filter_func, df.schema).show()  # doctest: +SKIP\n",
            "     |      +---+---+\n",
            "     |      | id|age|\n",
            "     |      +---+---+\n",
            "     |      |  1| 21|\n",
            "     |      +---+---+\n",
            "     |      \n",
            "     |      Set ``barrier`` to ``True`` to force the ``mapInArrow`` stage running in the\n",
            "     |      barrier mode, it ensures all Python workers in the stage will be\n",
            "     |      launched concurrently.\n",
            "     |      \n",
            "     |      >>> df.mapInArrow(filter_func, df.schema, barrier=True).show()  # doctest: +SKIP\n",
            "     |      +---+---+\n",
            "     |      | id|age|\n",
            "     |      +---+---+\n",
            "     |      |  1| 21|\n",
            "     |      +---+---+\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      This API is unstable, and for developers.\n",
            "     |      \n",
            "     |      See Also\n",
            "     |      --------\n",
            "     |      pyspark.sql.functions.pandas_udf\n",
            "     |      pyspark.sql.DataFrame.mapInPandas\n",
            "     |  \n",
            "     |  mapInPandas(self, func: 'PandasMapIterFunction', schema: Union[pyspark.sql.types.StructType, str], barrier: bool = False) -> 'DataFrame'\n",
            "     |      Maps an iterator of batches in the current :class:`DataFrame` using a Python native\n",
            "     |      function that takes and outputs a pandas DataFrame, and returns the result as a\n",
            "     |      :class:`DataFrame`.\n",
            "     |      \n",
            "     |      The function should take an iterator of `pandas.DataFrame`\\s and return\n",
            "     |      another iterator of `pandas.DataFrame`\\s. All columns are passed\n",
            "     |      together as an iterator of `pandas.DataFrame`\\s to the function and the\n",
            "     |      returned iterator of `pandas.DataFrame`\\s are combined as a :class:`DataFrame`.\n",
            "     |      Each `pandas.DataFrame` size can be controlled by\n",
            "     |      `spark.sql.execution.arrow.maxRecordsPerBatch`. The size of the function's input and\n",
            "     |      output can be different.\n",
            "     |      \n",
            "     |      .. versionadded:: 3.0.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      func : function\n",
            "     |          a Python native function that takes an iterator of `pandas.DataFrame`\\s, and\n",
            "     |          outputs an iterator of `pandas.DataFrame`\\s.\n",
            "     |      schema : :class:`pyspark.sql.types.DataType` or str\n",
            "     |          the return type of the `func` in PySpark. The value can be either a\n",
            "     |          :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
            "     |      barrier : bool, optional, default False\n",
            "     |          Use barrier mode execution.\n",
            "     |      \n",
            "     |          .. versionadded: 3.5.0\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> from pyspark.sql.functions import pandas_udf\n",
            "     |      >>> df = spark.createDataFrame([(1, 21), (2, 30)], (\"id\", \"age\"))\n",
            "     |      >>> def filter_func(iterator):\n",
            "     |      ...     for pdf in iterator:\n",
            "     |      ...         yield pdf[pdf.id == 1]\n",
            "     |      ...\n",
            "     |      >>> df.mapInPandas(filter_func, df.schema).show()  # doctest: +SKIP\n",
            "     |      +---+---+\n",
            "     |      | id|age|\n",
            "     |      +---+---+\n",
            "     |      |  1| 21|\n",
            "     |      +---+---+\n",
            "     |      \n",
            "     |      Set ``barrier`` to ``True`` to force the ``mapInPandas`` stage running in the\n",
            "     |      barrier mode, it ensures all Python workers in the stage will be\n",
            "     |      launched concurrently.\n",
            "     |      \n",
            "     |      >>> df.mapInPandas(filter_func, df.schema, barrier=True).show()  # doctest: +SKIP\n",
            "     |      +---+---+\n",
            "     |      | id|age|\n",
            "     |      +---+---+\n",
            "     |      |  1| 21|\n",
            "     |      +---+---+\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      This API is experimental\n",
            "     |      \n",
            "     |      See Also\n",
            "     |      --------\n",
            "     |      pyspark.sql.functions.pandas_udf\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from pyspark.sql.pandas.map_ops.PandasMapOpsMixin:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables (if defined)\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object (if defined)\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from pyspark.sql.pandas.conversion.PandasConversionMixin:\n",
            "     |  \n",
            "     |  toPandas(self) -> 'PandasDataFrameLike'\n",
            "     |      Returns the contents of this :class:`DataFrame` as Pandas ``pandas.DataFrame``.\n",
            "     |      \n",
            "     |      This is only available if Pandas is installed and available.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.3.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      This method should only be used if the resulting Pandas ``pandas.DataFrame`` is\n",
            "     |      expected to be small, as all the data is loaded into the driver's memory.\n",
            "     |      \n",
            "     |      Usage with ``spark.sql.execution.arrow.pyspark.enabled=True`` is experimental.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df.toPandas()  # doctest: +SKIP\n",
            "     |         age   name\n",
            "     |      0    2  Alice\n",
            "     |      1    5    Bob\n",
            "    \n",
            "    class DataFrameNaFunctions(builtins.object)\n",
            "     |  DataFrameNaFunctions(df: pyspark.sql.dataframe.DataFrame)\n",
            "     |  \n",
            "     |  Functionality for working with missing data in :class:`DataFrame`.\n",
            "     |  \n",
            "     |  .. versionadded:: 1.4.0\n",
            "     |  \n",
            "     |  .. versionchanged:: 3.4.0\n",
            "     |      Supports Spark Connect.\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __init__(self, df: pyspark.sql.dataframe.DataFrame)\n",
            "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
            "     |  \n",
            "     |  drop(self, how: str = 'any', thresh: Optional[int] = None, subset: Union[str, Tuple[str, ...], List[str], NoneType] = None) -> pyspark.sql.dataframe.DataFrame\n",
            "     |      Returns a new :class:`DataFrame` omitting rows with null values.\n",
            "     |      :func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.3.1\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      how : str, optional\n",
            "     |          'any' or 'all'.\n",
            "     |          If 'any', drop a row if it contains any nulls.\n",
            "     |          If 'all', drop a row only if all its values are null.\n",
            "     |      thresh: int, optional\n",
            "     |          default None\n",
            "     |          If specified, drop rows that have less than `thresh` non-null values.\n",
            "     |          This overwrites the `how` parameter.\n",
            "     |      subset : str, tuple or list, optional\n",
            "     |          optional list of column names to consider.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          DataFrame with null only rows excluded.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> from pyspark.sql import Row\n",
            "     |      >>> df = spark.createDataFrame([\n",
            "     |      ...     Row(age=10, height=80, name=\"Alice\"),\n",
            "     |      ...     Row(age=5, height=None, name=\"Bob\"),\n",
            "     |      ...     Row(age=None, height=None, name=\"Tom\"),\n",
            "     |      ...     Row(age=None, height=None, name=None),\n",
            "     |      ... ])\n",
            "     |      >>> df.na.drop().show()\n",
            "     |      +---+------+-----+\n",
            "     |      |age|height| name|\n",
            "     |      +---+------+-----+\n",
            "     |      | 10|    80|Alice|\n",
            "     |      +---+------+-----+\n",
            "     |  \n",
            "     |  fill(self, value: Union[ForwardRef('LiteralType'), Dict[str, ForwardRef('LiteralType')]], subset: Optional[List[str]] = None) -> pyspark.sql.dataframe.DataFrame\n",
            "     |      Replace null values, alias for ``na.fill()``.\n",
            "     |      :func:`DataFrame.fillna` and :func:`DataFrameNaFunctions.fill` are aliases of each other.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.3.1\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      value : int, float, string, bool or dict\n",
            "     |          Value to replace null values with.\n",
            "     |          If the value is a dict, then `subset` is ignored and `value` must be a mapping\n",
            "     |          from column name (string) to replacement value. The replacement value must be\n",
            "     |          an int, float, boolean, or string.\n",
            "     |      subset : str, tuple or list, optional\n",
            "     |          optional list of column names to consider.\n",
            "     |          Columns specified in subset that do not have matching data types are ignored.\n",
            "     |          For example, if `value` is a string, and subset contains a non-string column,\n",
            "     |          then the non-string column is simply ignored.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          DataFrame with replaced null values.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame([\n",
            "     |      ...     (10, 80.5, \"Alice\", None),\n",
            "     |      ...     (5, None, \"Bob\", None),\n",
            "     |      ...     (None, None, \"Tom\", None),\n",
            "     |      ...     (None, None, None, True)],\n",
            "     |      ...     schema=[\"age\", \"height\", \"name\", \"bool\"])\n",
            "     |      \n",
            "     |      Fill all null values with 50 for numeric columns.\n",
            "     |      \n",
            "     |      >>> df.na.fill(50).show()\n",
            "     |      +---+------+-----+----+\n",
            "     |      |age|height| name|bool|\n",
            "     |      +---+------+-----+----+\n",
            "     |      | 10|  80.5|Alice|NULL|\n",
            "     |      |  5|  50.0|  Bob|NULL|\n",
            "     |      | 50|  50.0|  Tom|NULL|\n",
            "     |      | 50|  50.0| NULL|true|\n",
            "     |      +---+------+-----+----+\n",
            "     |      \n",
            "     |      Fill all null values with ``False`` for boolean columns.\n",
            "     |      \n",
            "     |      >>> df.na.fill(False).show()\n",
            "     |      +----+------+-----+-----+\n",
            "     |      | age|height| name| bool|\n",
            "     |      +----+------+-----+-----+\n",
            "     |      |  10|  80.5|Alice|false|\n",
            "     |      |   5|  NULL|  Bob|false|\n",
            "     |      |NULL|  NULL|  Tom|false|\n",
            "     |      |NULL|  NULL| NULL| true|\n",
            "     |      +----+------+-----+-----+\n",
            "     |      \n",
            "     |      Fill all null values with to 50 and \"unknown\" for 'age' and 'name' column respectively.\n",
            "     |      \n",
            "     |      >>> df.na.fill({'age': 50, 'name': 'unknown'}).show()\n",
            "     |      +---+------+-------+----+\n",
            "     |      |age|height|   name|bool|\n",
            "     |      +---+------+-------+----+\n",
            "     |      | 10|  80.5|  Alice|NULL|\n",
            "     |      |  5|  NULL|    Bob|NULL|\n",
            "     |      | 50|  NULL|    Tom|NULL|\n",
            "     |      | 50|  NULL|unknown|true|\n",
            "     |      +---+------+-------+----+\n",
            "     |  \n",
            "     |  replace(self, to_replace: Union[List[ForwardRef('LiteralType')], Dict[ForwardRef('LiteralType'), ForwardRef('OptionalPrimitiveType')]], value: Union[ForwardRef('OptionalPrimitiveType'), List[ForwardRef('OptionalPrimitiveType')], pyspark._globals._NoValueType, NoneType] = <no value>, subset: Optional[List[str]] = None) -> pyspark.sql.dataframe.DataFrame\n",
            "     |      Returns a new :class:`DataFrame` replacing a value with another value.\n",
            "     |      :func:`DataFrame.replace` and :func:`DataFrameNaFunctions.replace` are\n",
            "     |      aliases of each other.\n",
            "     |      Values to_replace and value must have the same type and can only be numerics, booleans,\n",
            "     |      or strings. Value can have None. When replacing, the new value will be cast\n",
            "     |      to the type of the existing column.\n",
            "     |      For numeric replacements all values to be replaced should have unique\n",
            "     |      floating point representation. In case of conflicts (for example with `{42: -1, 42.0: 1}`)\n",
            "     |      and arbitrary replacement will be used.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.4.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      to_replace : bool, int, float, string, list or dict\n",
            "     |          Value to be replaced.\n",
            "     |          If the value is a dict, then `value` is ignored or can be omitted, and `to_replace`\n",
            "     |          must be a mapping between a value and a replacement.\n",
            "     |      value : bool, int, float, string or None, optional\n",
            "     |          The replacement value must be a bool, int, float, string or None. If `value` is a\n",
            "     |          list, `value` should be of the same length and type as `to_replace`.\n",
            "     |          If `value` is a scalar and `to_replace` is a sequence, then `value` is\n",
            "     |          used as a replacement for each item in `to_replace`.\n",
            "     |      subset : list, optional\n",
            "     |          optional list of column names to consider.\n",
            "     |          Columns specified in subset that do not have matching data types are ignored.\n",
            "     |          For example, if `value` is a string, and subset contains a non-string column,\n",
            "     |          then the non-string column is simply ignored.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          DataFrame with replaced values.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame([\n",
            "     |      ...     (10, 80, \"Alice\"),\n",
            "     |      ...     (5, None, \"Bob\"),\n",
            "     |      ...     (None, 10, \"Tom\"),\n",
            "     |      ...     (None, None, None)],\n",
            "     |      ...     schema=[\"age\", \"height\", \"name\"])\n",
            "     |      \n",
            "     |      Replace 10 to 20 in all columns.\n",
            "     |      \n",
            "     |      >>> df.na.replace(10, 20).show()\n",
            "     |      +----+------+-----+\n",
            "     |      | age|height| name|\n",
            "     |      +----+------+-----+\n",
            "     |      |  20|    80|Alice|\n",
            "     |      |   5|  NULL|  Bob|\n",
            "     |      |NULL|    20|  Tom|\n",
            "     |      |NULL|  NULL| NULL|\n",
            "     |      +----+------+-----+\n",
            "     |      \n",
            "     |      Replace 'Alice' to null in all columns.\n",
            "     |      \n",
            "     |      >>> df.na.replace('Alice', None).show()\n",
            "     |      +----+------+----+\n",
            "     |      | age|height|name|\n",
            "     |      +----+------+----+\n",
            "     |      |  10|    80|NULL|\n",
            "     |      |   5|  NULL| Bob|\n",
            "     |      |NULL|    10| Tom|\n",
            "     |      |NULL|  NULL|NULL|\n",
            "     |      +----+------+----+\n",
            "     |      \n",
            "     |      Replace 'Alice' to 'A', and 'Bob' to 'B' in the 'name' column.\n",
            "     |      \n",
            "     |      >>> df.na.replace(['Alice', 'Bob'], ['A', 'B'], 'name').show()\n",
            "     |      +----+------+----+\n",
            "     |      | age|height|name|\n",
            "     |      +----+------+----+\n",
            "     |      |  10|    80|   A|\n",
            "     |      |   5|  NULL|   B|\n",
            "     |      |NULL|    10| Tom|\n",
            "     |      |NULL|  NULL|NULL|\n",
            "     |      +----+------+----+\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors defined here:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables (if defined)\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object (if defined)\n",
            "    \n",
            "    class DataFrameStatFunctions(builtins.object)\n",
            "     |  DataFrameStatFunctions(df: pyspark.sql.dataframe.DataFrame)\n",
            "     |  \n",
            "     |  Functionality for statistic functions with :class:`DataFrame`.\n",
            "     |  \n",
            "     |  .. versionadded:: 1.4.0\n",
            "     |  \n",
            "     |  .. versionchanged:: 3.4.0\n",
            "     |      Supports Spark Connect.\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __init__(self, df: pyspark.sql.dataframe.DataFrame)\n",
            "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
            "     |  \n",
            "     |  approxQuantile(self, col: Union[str, List[str], Tuple[str]], probabilities: Union[List[float], Tuple[float]], relativeError: float) -> Union[List[float], List[List[float]]]\n",
            "     |      Calculates the approximate quantiles of numerical columns of a\n",
            "     |      :class:`DataFrame`.\n",
            "     |      \n",
            "     |      The result of this algorithm has the following deterministic bound:\n",
            "     |      If the :class:`DataFrame` has N elements and if we request the quantile at\n",
            "     |      probability `p` up to error `err`, then the algorithm will return\n",
            "     |      a sample `x` from the :class:`DataFrame` so that the *exact* rank of `x` is\n",
            "     |      close to (p * N). More precisely,\n",
            "     |      \n",
            "     |        floor((p - err) * N) <= rank(x) <= ceil((p + err) * N).\n",
            "     |      \n",
            "     |      This method implements a variation of the Greenwald-Khanna\n",
            "     |      algorithm (with some speed optimizations). The algorithm was first\n",
            "     |      present in [[https://doi.org/10.1145/375663.375670\n",
            "     |      Space-efficient Online Computation of Quantile Summaries]]\n",
            "     |      by Greenwald and Khanna.\n",
            "     |      \n",
            "     |      .. versionadded:: 2.0.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      col: str, tuple or list\n",
            "     |          Can be a single column name, or a list of names for multiple columns.\n",
            "     |      \n",
            "     |          .. versionchanged:: 2.2.0\n",
            "     |             Added support for multiple columns.\n",
            "     |      probabilities : list or tuple\n",
            "     |          a list of quantile probabilities\n",
            "     |          Each number must belong to [0, 1].\n",
            "     |          For example 0 is the minimum, 0.5 is the median, 1 is the maximum.\n",
            "     |      relativeError : float\n",
            "     |          The relative target precision to achieve\n",
            "     |          (>= 0). If set to zero, the exact quantiles are computed, which\n",
            "     |          could be very expensive. Note that values greater than 1 are\n",
            "     |          accepted but gives the same result as 1.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      list\n",
            "     |          the approximate quantiles at the given probabilities.\n",
            "     |      \n",
            "     |          * If the input `col` is a string, the output is a list of floats.\n",
            "     |      \n",
            "     |          * If the input `col` is a list or tuple of strings, the output is also a\n",
            "     |              list, but each element in it is a list of floats, i.e., the output\n",
            "     |              is a list of list of floats.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      Null values will be ignored in numerical columns before calculation.\n",
            "     |      For columns only containing null values, an empty list is returned.\n",
            "     |  \n",
            "     |  corr(self, col1: str, col2: str, method: Optional[str] = None) -> float\n",
            "     |      Calculates the correlation of two columns of a :class:`DataFrame` as a double value.\n",
            "     |      Currently only supports the Pearson Correlation Coefficient.\n",
            "     |      :func:`DataFrame.corr` and :func:`DataFrameStatFunctions.corr` are aliases of each other.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.4.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      col1 : str\n",
            "     |          The name of the first column\n",
            "     |      col2 : str\n",
            "     |          The name of the second column\n",
            "     |      method : str, optional\n",
            "     |          The correlation method. Currently only supports \"pearson\"\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      float\n",
            "     |          Pearson Correlation Coefficient of two columns.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame([(1, 12), (10, 1), (19, 8)], [\"c1\", \"c2\"])\n",
            "     |      >>> df.corr(\"c1\", \"c2\")\n",
            "     |      -0.3592106040535498\n",
            "     |      >>> df = spark.createDataFrame([(11, 12), (10, 11), (9, 10)], [\"small\", \"bigger\"])\n",
            "     |      >>> df.corr(\"small\", \"bigger\")\n",
            "     |      1.0\n",
            "     |  \n",
            "     |  cov(self, col1: str, col2: str) -> float\n",
            "     |      Calculate the sample covariance for the given columns, specified by their names, as a\n",
            "     |      double value. :func:`DataFrame.cov` and :func:`DataFrameStatFunctions.cov` are aliases.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.4.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      col1 : str\n",
            "     |          The name of the first column\n",
            "     |      col2 : str\n",
            "     |          The name of the second column\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      float\n",
            "     |          Covariance of two columns.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame([(1, 12), (10, 1), (19, 8)], [\"c1\", \"c2\"])\n",
            "     |      >>> df.cov(\"c1\", \"c2\")\n",
            "     |      -18.0\n",
            "     |      >>> df = spark.createDataFrame([(11, 12), (10, 11), (9, 10)], [\"small\", \"bigger\"])\n",
            "     |      >>> df.cov(\"small\", \"bigger\")\n",
            "     |      1.0\n",
            "     |  \n",
            "     |  crosstab(self, col1: str, col2: str) -> pyspark.sql.dataframe.DataFrame\n",
            "     |      Computes a pair-wise frequency table of the given columns. Also known as a contingency\n",
            "     |      table.\n",
            "     |      The first column of each row will be the distinct values of `col1` and the column names\n",
            "     |      will be the distinct values of `col2`. The name of the first column will be `$col1_$col2`.\n",
            "     |      Pairs that have no occurrences will have zero as their counts.\n",
            "     |      :func:`DataFrame.crosstab` and :func:`DataFrameStatFunctions.crosstab` are aliases.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.4.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      col1 : str\n",
            "     |          The name of the first column. Distinct items will make the first item of\n",
            "     |          each row.\n",
            "     |      col2 : str\n",
            "     |          The name of the second column. Distinct items will make the column names\n",
            "     |          of the :class:`DataFrame`.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          Frequency matrix of two columns.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame([(1, 11), (1, 11), (3, 10), (4, 8), (4, 8)], [\"c1\", \"c2\"])\n",
            "     |      >>> df.crosstab(\"c1\", \"c2\").sort(\"c1_c2\").show()\n",
            "     |      +-----+---+---+---+\n",
            "     |      |c1_c2| 10| 11|  8|\n",
            "     |      +-----+---+---+---+\n",
            "     |      |    1|  0|  2|  0|\n",
            "     |      |    3|  1|  0|  0|\n",
            "     |      |    4|  0|  0|  2|\n",
            "     |      +-----+---+---+---+\n",
            "     |  \n",
            "     |  freqItems(self, cols: List[str], support: Optional[float] = None) -> pyspark.sql.dataframe.DataFrame\n",
            "     |      Finding frequent items for columns, possibly with false positives. Using the\n",
            "     |      frequent element count algorithm described in\n",
            "     |      \"https://doi.org/10.1145/762471.762473, proposed by Karp, Schenker, and Papadimitriou\".\n",
            "     |      :func:`DataFrame.freqItems` and :func:`DataFrameStatFunctions.freqItems` are aliases.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.4.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      cols : list or tuple\n",
            "     |          Names of the columns to calculate frequent items for as a list or tuple of\n",
            "     |          strings.\n",
            "     |      support : float, optional\n",
            "     |          The frequency with which to consider an item 'frequent'. Default is 1%.\n",
            "     |          The support must be greater than 1e-4.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      :class:`DataFrame`\n",
            "     |          DataFrame with frequent items.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      This function is meant for exploratory data analysis, as we make no\n",
            "     |      guarantee about the backward compatibility of the schema of the resulting\n",
            "     |      :class:`DataFrame`.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> df = spark.createDataFrame([(1, 11), (1, 11), (3, 10), (4, 8), (4, 8)], [\"c1\", \"c2\"])\n",
            "     |      >>> df.freqItems([\"c1\", \"c2\"]).show()  # doctest: +SKIP\n",
            "     |      +------------+------------+\n",
            "     |      |c1_freqItems|c2_freqItems|\n",
            "     |      +------------+------------+\n",
            "     |      |   [4, 1, 3]| [8, 11, 10]|\n",
            "     |      +------------+------------+\n",
            "     |  \n",
            "     |  sampleBy(self, col: str, fractions: Dict[Any, float], seed: Optional[int] = None) -> pyspark.sql.dataframe.DataFrame\n",
            "     |      Returns a stratified sample without replacement based on the\n",
            "     |      fraction given on each stratum.\n",
            "     |      \n",
            "     |      .. versionadded:: 1.5.0\n",
            "     |      \n",
            "     |      .. versionchanged:: 3.4.0\n",
            "     |          Supports Spark Connect.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      col : :class:`Column` or str\n",
            "     |          column that defines strata\n",
            "     |      \n",
            "     |          .. versionchanged:: 3.0.0\n",
            "     |             Added sampling by a column of :class:`Column`\n",
            "     |      fractions : dict\n",
            "     |          sampling fraction for each stratum. If a stratum is not\n",
            "     |          specified, we treat its fraction as zero.\n",
            "     |      seed : int, optional\n",
            "     |          random seed\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      a new :class:`DataFrame` that represents the stratified sample\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> from pyspark.sql.functions import col\n",
            "     |      >>> dataset = spark.range(0, 100).select((col(\"id\") % 3).alias(\"key\"))\n",
            "     |      >>> sampled = dataset.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=0)\n",
            "     |      >>> sampled.groupBy(\"key\").count().orderBy(\"key\").show()\n",
            "     |      +---+-----+\n",
            "     |      |key|count|\n",
            "     |      +---+-----+\n",
            "     |      |  0|    3|\n",
            "     |      |  1|    6|\n",
            "     |      +---+-----+\n",
            "     |      >>> dataset.sampleBy(col(\"key\"), fractions={2: 1.0}, seed=0).count()\n",
            "     |      33\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors defined here:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables (if defined)\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object (if defined)\n",
            "\n",
            "DATA\n",
            "    __all__ = ['DataFrame', 'DataFrameNaFunctions', 'DataFrameStatFunction...\n",
            "\n",
            "FILE\n",
            "    /usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(1, 'Stephen'), (2, 'Ayden')]\n",
        "schema = ['id', 'name']\n",
        "\n",
        "df= spark.createDataFrame(data=data, schema=schema)\n",
        "df.show()\n",
        "display(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "SZ6cBrV83OFn",
        "outputId": "29416288-9778-4b8b-a6b9-1cd246b40df6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "| id|   name|\n",
            "+---+-------+\n",
            "|  1|Stephen|\n",
            "|  2|  Ayden|\n",
            "+---+-------+\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, name: string]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(df.write)\n",
        "df.write.csv(path='/empdata.csv',header=True)"
      ],
      "metadata": {
        "id": "rSlCNird4QFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.read.csv(path='/empdata.csv', header=True, inferSchema=True).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEXOCDay61Ob",
        "outputId": "f6e6bb60-1625-4196-fea4-8f8fa7a3f04c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "| id|   name|\n",
            "+---+-------+\n",
            "|  1|Stephen|\n",
            "|  1|Stephen|\n",
            "|  2|  Ayden|\n",
            "|  2|  Ayden|\n",
            "+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(df.write)\n",
        "df.write.csv(path='/empdata.csv',header=True,mode ='ignore')"
      ],
      "metadata": {
        "id": "qv2RN9iz7lrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(df.write)\n",
        "df.write.csv(path='/empdata.csv',header=True,mode ='overwrite')"
      ],
      "metadata": {
        "id": "CToA7FKO8NPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(df.write)\n",
        "df.write.csv(path='/empdata.csv',header=True,mode ='append')"
      ],
      "metadata": {
        "id": "jaCK99y_8fJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "    {\"id\": 1, \"neighborhood\": \"Back Bay\", \"price\": 750000, \"bedrooms\": 2, \"bathrooms\": 2},\n",
        "    {\"id\": 2, \"neighborhood\": \"Beacon Hill\", \"price\": 1200000, \"bedrooms\": 3, \"bathrooms\": 3},\n",
        "    {\"id\": 3, \"neighborhood\": \"North End\", \"price\": 850000, \"bedrooms\": 2, \"bathrooms\": 1},\n",
        "    {\"id\": 4, \"neighborhood\": \"South Boston\", \"price\": 650000, \"bedrooms\": 2, \"bathrooms\": 2},\n",
        "    {\"id\": 5, \"neighborhood\": \"West End\", \"price\": 700000, \"bedrooms\": 2, \"bathrooms\": 1}\n",
        "]\n",
        "\n",
        "# Save data to a JSON file\n",
        "with open('sample_data.json', 'w') as file:\n",
        "    json.dump(data, file, indent=4)\n",
        "\n",
        "print(\"JSON data has been saved to sample_data.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFXhf_K-_Up5",
        "outputId": "a7b18a17-a03f-4b1d-b270-58d8c97dbad2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JSON data has been saved to sample_data.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Step 1: Create and Save JSON Data\n",
        "data = [\n",
        "    {\"id\": 1, \"neighborhood\": \"Back Bay\", \"price\": 750000, \"bedrooms\": 2, \"bathrooms\": 2},\n",
        "    {\"id\": 2, \"neighborhood\": \"Beacon Hill\", \"price\": 1200000, \"bedrooms\": 3, \"bathrooms\": 3},\n",
        "    {\"id\": 3, \"neighborhood\": \"North End\", \"price\": 850000, \"bedrooms\": 2, \"bathrooms\": 1},\n",
        "    {\"id\": 4, \"neighborhood\": \"South Boston\", \"price\": 650000, \"bedrooms\": 2, \"bathrooms\": 2},\n",
        "    {\"id\": 5, \"neighborhood\": \"West End\", \"price\": 700000, \"bedrooms\": 2, \"bathrooms\": 1}\n",
        "]\n",
        "\n",
        "# Save data to a JSON file in the Colab environment\n",
        "with open('/content/sample_data.json', 'w') as file:\n",
        "    json.dump(data, file, indent=4)\n",
        "\n",
        "print(\"JSON data has been saved to /content/sample_data.json\")\n",
        "\n",
        "# Step 2: Load and Display JSON Data\n",
        "with open('/content/sample_data.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Display the data\n",
        "print(data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlChBOXPAJY5",
        "outputId": "f4db5605-72b5-4513-8dd2-57ae8916f3b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JSON data has been saved to /content/sample_data.json\n",
            "[{'id': 1, 'neighborhood': 'Back Bay', 'price': 750000, 'bedrooms': 2, 'bathrooms': 2}, {'id': 2, 'neighborhood': 'Beacon Hill', 'price': 1200000, 'bedrooms': 3, 'bathrooms': 3}, {'id': 3, 'neighborhood': 'North End', 'price': 850000, 'bedrooms': 2, 'bathrooms': 1}, {'id': 4, 'neighborhood': 'South Boston', 'price': 650000, 'bedrooms': 2, 'bathrooms': 2}, {'id': 5, 'neighborhood': 'West End', 'price': 700000, 'bedrooms': 2, 'bathrooms': 1}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Read and inspect the JSON file\n",
        "with open('/content/sample_data.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Print the data to check for issues\n",
        "print(data)\n"
      ],
      "metadata": {
        "id": "VcC5fzu1KvXN",
        "outputId": "799c2fcb-ddef-4f6e-f924-9b984376a4de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'id': 1, 'neighborhood': 'Back Bay', 'price': 750000, 'bedrooms': 2, 'bathrooms': 2}, {'id': 2, 'neighborhood': 'Beacon Hill', 'price': 1200000, 'bedrooms': 3, 'bathrooms': 3}, {'id': 3, 'neighborhood': 'North End', 'price': 850000, 'bedrooms': 2, 'bathrooms': 1}, {'id': 4, 'neighborhood': 'South Boston', 'price': 650000, 'bedrooms': 2, 'bathrooms': 2}, {'id': 5, 'neighborhood': 'West End', 'price': 700000, 'bedrooms': 2, 'bathrooms': 1}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.json('/content/simple.json',multiLine=True)\n",
        "df.printSchema\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7rVia2JBu5G",
        "outputId": "349a6c62-2f81-4b8a-ec2a-29c51b6cc34a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------+---+-------+\n",
            "|age|         city| id|   name|\n",
            "+---+-------------+---+-------+\n",
            "| 30|     New York|  1|  Alice|\n",
            "| 25|San Francisco|  2|    Bob|\n",
            "| 35|  Los Angeles|  3|Charlie|\n",
            "+---+-------------+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.json('/content/sample_data1.json',multiLine=True)\n",
        "df.printSchema\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64uRinMgCj72",
        "outputId": "11d1e475-380b-426c-949b-af3045b8310a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+---+------------+-------+\n",
            "|bathrooms|bedrooms| id|neighborhood|  price|\n",
            "+---------+--------+---+------------+-------+\n",
            "|        2|       2|  1|    Back Bay| 750000|\n",
            "|        3|       3|  2| Beacon Hill|1200000|\n",
            "|        1|       2|  3|   North End| 850000|\n",
            "|        2|       2|  4|South Boston| 650000|\n",
            "|        1|       2|  5|    West End| 700000|\n",
            "+---------+--------+---+------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.json('/content/test2.json',multiLine=True)\n",
        "df.printSchema()\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64L_LCCgCuRt",
        "outputId": "84e7236e-eafd-4702-e09f-df8935312a9f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- bio: string (nullable = true)\n",
            " |-- id: string (nullable = true)\n",
            " |-- language: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- version: double (nullable = true)\n",
            "\n",
            "+--------------------+----------------+----------------+--------------------+-------+\n",
            "|                 bio|              id|        language|                name|version|\n",
            "+--------------------+----------------+----------------+--------------------+-------+\n",
            "|Dec lobortis elei...|V59OF92YF627HFY0|          Sindhi|       Adeel Solangi|    6.1|\n",
            "|Some concern befo...|ENTOCR13RSCLZ6KU|          Sindhi|       Afzal Ghaffar|   1.88|\n",
            "|Vestibulum pharet...|IAKPO3R4761JDRVG|          Sindhi|       Aamir Solangi|   7.27|\n",
            "|Until the story o...|5ZVOEPMJUI4MB4EN|          Uighur|       Abla Dilmurat|   2.53|\n",
            "|Let's live what t...|6VTI8X6LL0MMPJCC|          Uighur|            Adil Eli|   6.49|\n",
            "|You have the adva...|F2KEU5L7EHYSYFTT|          Uighur|         Adile Qadir|    1.9|\n",
            "|Let's live what t...|LO6DVTZLRK68528I|          Uighur|I will abduct Ibr...|    5.9|\n",
            "|orcs just sometim...|LJRIULRNJFCNZJAJ|          Sindhi|           Adil Abro|   9.32|\n",
            "|Fusce eu ultrices...|JMCL0CXNXHPL1GBC|        Galician|    Afonso Vilarchán|   5.21|\n",
            "|but, as the great...|KU4T500C830697CW|         Maltese|       Mark Schembri|   3.17|\n",
            "|           eleifend.|XOF91ZR7MHV1TXRS|        Galician|       Antía Sixirei|   6.44|\n",
            "|You have a medica...|FTSNV411G5MKLPDT|          Uighur|      Aygul Mutellip|    9.1|\n",
            "|Nunc aquet member...|OJMWMEEQWMLDU29P|          Sindhi|        Awais Shaikh|   1.59|\n",
            "|The vestibule bef...|5G646V7E6TJW8X2M|          Sindhi|       Ambreen Ahmed|   2.35|\n",
            "|Nulla and members...|Z53AJY7WUYPLAWC9|        Galician|         Celtia Anes|   8.34|\n",
            "|Phasellus tincidu...|N1AS6UFULO6WGTLB|         Maltese|       George Mifsud|   7.47|\n",
            "|It will be treate...|70RODUVRD95CLOJL|          Uighur|        Aytürk Qasim|   1.32|\n",
            "|Maecenas does not...|VBLI24FKF7VV6BWE|Sesotho sa Leboa|          Dialè Meso|   6.29|\n",
            "|Integer vehicles,...|4VRLON0GPEZYFCVL|        Galician|       Breixo Galáns|   1.62|\n",
            "|Ut viverra quis e...|5DRDI1QLRGLP29RC|        Galician|         Bieto Lorme|   4.45|\n",
            "+--------------------+----------------+----------------+--------------------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "uploading multiple files with identical schema"
      ],
      "metadata": {
        "id": "LDhyrr7UMSER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.json(path=['/content/test3.json','/content/test4.json'],multiLine=True)\n",
        "df.printSchema()\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ma_4Q9QfKsut",
        "outputId": "2df372ee-1a37-42f8-b435-06ab04f37c45"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- age: long (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n",
            "+---+-------------+---+-------+\n",
            "|age|         city| id|   name|\n",
            "+---+-------------+---+-------+\n",
            "| 30|     New York|  1|  Alice|\n",
            "| 25|San Francisco|  2|    Bob|\n",
            "| 35|  Los Angeles|  3|Charlie|\n",
            "| 28|      Seattle|  4|  David|\n",
            "| 22|      Chicago|  5|    Eve|\n",
            "| 40|        Miami|  6|  Frank|\n",
            "+---+-------------+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**changing the datatype of columns when importing datasets**"
      ],
      "metadata": {
        "id": "8HZeIOJPPMg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, IntegerType, StringType\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"ReadJSON\").getOrCreate()\n",
        "\n",
        "# Define the schema\n",
        "schema = StructType().add('age', IntegerType())\\\n",
        "                     .add('city', StringType())\\\n",
        "                     .add('id', IntegerType())\\\n",
        "                     .add('name', StringType())\n",
        "\n",
        "# Read the JSON files with the defined schema and multiLine option\n",
        "df = spark.read.schema(schema).json(path=['/content/test3.json', '/content/test4.json'], multiLine=True)\n",
        "\n",
        "# Print the schema of the DataFrame\n",
        "df.printSchema()\n",
        "\n",
        "# Show the contents of the DataFrame\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvPmflysO-U_",
        "outputId": "36a7d201-ac6c-4915-dc25-bea9ab3a0876"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- age: integer (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n",
            "+---+-------------+---+-------+\n",
            "|age|         city| id|   name|\n",
            "+---+-------------+---+-------+\n",
            "| 30|     New York|  1|  Alice|\n",
            "| 25|San Francisco|  2|    Bob|\n",
            "| 35|  Los Angeles|  3|Charlie|\n",
            "| 28|      Seattle|  4|  David|\n",
            "| 22|      Chicago|  5|    Eve|\n",
            "| 40|        Miami|  6|  Frank|\n",
            "+---+-------------+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "write Datafrom in json file\n",
        "**bold text**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SsYTfFtHP5nJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(1,'Stephen'),(2,'Ayden')]\n",
        "schema = ['id','name']\n",
        "\n",
        "df = spark.createDataFrame(data=data,schema=schema)\n",
        "df.show()\n",
        "display(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "XCdxo4ZLP_F8",
        "outputId": "9c06258c-a0d3-4d42-9620-8fe153bc906f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "| id|   name|\n",
            "+---+-------+\n",
            "|  1|Stephen|\n",
            "|  2|  Ayden|\n",
            "+---+-------+\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, name: string]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.json(path='/empdata1.json')\n",
        "display(spark.read.json('/empdata1.json'))\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "hYBfF__zQlGM",
        "outputId": "2dd6ae67-5585-4120-ff56-3ebf41a32414"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, name: string]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "| id|   name|\n",
            "+---+-------+\n",
            "|  1|Stephen|\n",
            "|  2|  Ayden|\n",
            "+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**using parquet file formats**"
      ],
      "metadata": {
        "id": "YfiReHyvUTZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"CreateParquet\").getOrCreate()\n",
        "\n",
        "# Define the schema\n",
        "schema = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"age\", IntegerType(), True),\n",
        "    StructField(\"city\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Create a DataFrame with sample data\n",
        "data = [\n",
        "    (1, \"Alice\", 30, \"New York\"),\n",
        "    (2, \"Bob\", 25, \"San Francisco\"),\n",
        "    (3, \"Charlie\", 35, \"Los Angeles\")\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data, schema)\n",
        "\n",
        "# Write the DataFrame to a Parquet file\n",
        "df.write.parquet(\"/content/pdata.parquet\")\n",
        "\n",
        "# Verify by reading the Parquet file back\n",
        "df_read = spark.read.parquet(\"/content/pdata.parquet\")\n",
        "df_read.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifDoUZHRToe_",
        "outputId": "6b717af0-df46-4cd4-948c-2c3b8e5e40d2"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+---+-------------+\n",
            "| id|   name|age|         city|\n",
            "+---+-------+---+-------------+\n",
            "|  2|    Bob| 25|San Francisco|\n",
            "|  3|Charlie| 35|  Los Angeles|\n",
            "|  1|  Alice| 30|     New York|\n",
            "+---+-------+---+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_read = spark.read.parquet(\"/userdata1.parquet\")\n",
        "print(df.count())\n",
        "df_read.show()\n",
        "display(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        },
        "id": "YOVBmzRrUGMJ",
        "outputId": "db1d963f-7177-4c31-910b-5efda43b87c0"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n",
            "+-------------------+---+----------+---------+--------------------+------+---------------+-------------------+--------------------+----------+---------+--------------------+--------------------+\n",
            "|  registration_dttm| id|first_name|last_name|               email|gender|     ip_address|                 cc|             country| birthdate|   salary|               title|            comments|\n",
            "+-------------------+---+----------+---------+--------------------+------+---------------+-------------------+--------------------+----------+---------+--------------------+--------------------+\n",
            "|2016-02-03 07:55:29|  1|    Amanda|   Jordan|    ajordan0@com.com|Female|    1.197.201.2|   6759521864920116|           Indonesia|  3/8/1971| 49756.53|    Internal Auditor|               1E+02|\n",
            "|2016-02-03 17:04:03|  2|    Albert|  Freeman|     afreeman1@is.gd|  Male| 218.111.175.34|                   |              Canada| 1/16/1968|150280.17|       Accountant IV|                    |\n",
            "|2016-02-03 01:09:31|  3|    Evelyn|   Morgan|emorgan2@altervis...|Female|   7.161.136.94|   6767119071901597|              Russia|  2/1/1960|144972.51| Structural Engineer|                    |\n",
            "|2016-02-03 00:36:21|  4|    Denise|    Riley|    driley3@gmpg.org|Female|  140.35.109.83|   3576031598965625|               China|  4/8/1997| 90263.05|Senior Cost Accou...|                    |\n",
            "|2016-02-03 05:05:31|  5|    Carlos|    Burns|cburns4@miitbeian...|      | 169.113.235.40|   5602256255204850|        South Africa|          |     NULL|                    |                    |\n",
            "|2016-02-03 07:22:34|  6|   Kathryn|    White|  kwhite5@google.com|Female| 195.131.81.179|   3583136326049310|           Indonesia| 2/25/1983| 69227.11|   Account Executive|                    |\n",
            "|2016-02-03 08:33:08|  7|    Samuel|   Holmes|sholmes6@foxnews.com|  Male| 232.234.81.197|   3582641366974690|            Portugal|12/18/1987| 14247.62|Senior Financial ...|                    |\n",
            "|2016-02-03 06:47:06|  8|     Harry|   Howell| hhowell7@eepurl.com|  Male|   91.235.51.73|                   |Bosnia and Herzeg...|  3/1/1962|186469.43|    Web Developer IV|                    |\n",
            "|2016-02-03 03:52:53|  9|      Jose|   Foster|   jfoster8@yelp.com|  Male|   132.31.53.61|                   |         South Korea| 3/27/1992|231067.84|Software Test Eng...|               1E+02|\n",
            "|2016-02-03 18:29:47| 10|     Emily|  Stewart|estewart9@opensou...|Female| 143.28.251.245|   3574254110301671|             Nigeria| 1/28/1997| 27234.28|     Health Coach IV|                    |\n",
            "|2016-02-03 00:10:42| 11|     Susan|  Perkins| sperkinsa@patch.com|Female|    180.85.0.62|   3573823609854134|              Russia|          |210001.95|                    |                    |\n",
            "|2016-02-03 18:04:34| 12|     Alice|    Berry|aberryb@wikipedia...|Female| 246.225.12.189|   4917830851454417|               China| 8/12/1968| 22944.53|    Quality Engineer|                    |\n",
            "|2016-02-03 18:48:17| 13|    Justin|    Berry|jberryc@usatoday.com|  Male|   157.7.146.43|6331109912871813274|              Zambia| 8/15/1975| 44165.46|Structural Analys...|                    |\n",
            "|2016-02-03 21:46:52| 14|     Kathy| Reynolds|kreynoldsd@redcro...|Female|  81.254.172.13|   5537178462965976|Bosnia and Herzeg...| 6/27/1970|286592.99|           Librarian|                    |\n",
            "|2016-02-03 08:53:23| 15|   Dorothy|   Hudson|dhudsone@blogger.com|Female|       8.59.7.0|   3542586858224170|               Japan|12/20/1989|157099.71|  Nurse Practicioner|<script>alert('hi...|\n",
            "|2016-02-03 00:44:01| 16|     Bruce|   Willis|bwillisf@bluehost...|  Male|239.182.219.189|   3573030625927601|              Brazil|          |239100.65|                    |                    |\n",
            "|2016-02-03 00:57:45| 17|     Emily|  Andrews|eandrewsg@cornell...|Female| 29.231.180.172|     30271790537626|              Russia| 4/13/1990|116800.65|        Food Chemist|                    |\n",
            "|2016-02-03 16:44:24| 18|   Stephen|  Wallace|swallaceh@netvibe...|  Male|  152.49.213.62|   5433943468526428|             Ukraine| 1/15/1978|248877.99|Account Represent...|                    |\n",
            "|2016-02-03 11:45:54| 19|  Clarence|   Lawson|clawsoni@vkontakt...|  Male| 107.175.15.152|   3544052814080964|              Russia|          |177122.99|                    |                    |\n",
            "|2016-02-03 10:30:36| 20|   Rebecca|     Bell| rbellj@bandcamp.com|Female|172.215.104.127|                   |               China|          |137251.19|                    |                    |\n",
            "+-------------------+---+----------+---------+--------------------+------+---------------+-------------------+--------------------+----------+---------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[registration_dttm: timestamp, id: int, first_name: string, last_name: string, email: string, gender: string, ip_address: string, cc: string, country: string, birthdate: string, salary: double, title: string, comments: string]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading multiple parquet datasets"
      ],
      "metadata": {
        "id": "pNosJLzWcDbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_read = spark.read.parquet('/userdata1.parquet', '/userdata2.parquet')\n",
        "print(df.count())\n",
        "df_read.show()\n",
        "display(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        },
        "id": "18khwaj9aeHY",
        "outputId": "99aa5caa-ceb6-4751-c6a2-db7f248ea139"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n",
            "+-------------------+---+----------+---------+--------------------+------+---------------+-------------------+--------------------+----------+---------+--------------------+--------------------+\n",
            "|  registration_dttm| id|first_name|last_name|               email|gender|     ip_address|                 cc|             country| birthdate|   salary|               title|            comments|\n",
            "+-------------------+---+----------+---------+--------------------+------+---------------+-------------------+--------------------+----------+---------+--------------------+--------------------+\n",
            "|2016-02-03 07:55:29|  1|    Amanda|   Jordan|    ajordan0@com.com|Female|    1.197.201.2|   6759521864920116|           Indonesia|  3/8/1971| 49756.53|    Internal Auditor|               1E+02|\n",
            "|2016-02-03 17:04:03|  2|    Albert|  Freeman|     afreeman1@is.gd|  Male| 218.111.175.34|                   |              Canada| 1/16/1968|150280.17|       Accountant IV|                    |\n",
            "|2016-02-03 01:09:31|  3|    Evelyn|   Morgan|emorgan2@altervis...|Female|   7.161.136.94|   6767119071901597|              Russia|  2/1/1960|144972.51| Structural Engineer|                    |\n",
            "|2016-02-03 00:36:21|  4|    Denise|    Riley|    driley3@gmpg.org|Female|  140.35.109.83|   3576031598965625|               China|  4/8/1997| 90263.05|Senior Cost Accou...|                    |\n",
            "|2016-02-03 05:05:31|  5|    Carlos|    Burns|cburns4@miitbeian...|      | 169.113.235.40|   5602256255204850|        South Africa|          |     NULL|                    |                    |\n",
            "|2016-02-03 07:22:34|  6|   Kathryn|    White|  kwhite5@google.com|Female| 195.131.81.179|   3583136326049310|           Indonesia| 2/25/1983| 69227.11|   Account Executive|                    |\n",
            "|2016-02-03 08:33:08|  7|    Samuel|   Holmes|sholmes6@foxnews.com|  Male| 232.234.81.197|   3582641366974690|            Portugal|12/18/1987| 14247.62|Senior Financial ...|                    |\n",
            "|2016-02-03 06:47:06|  8|     Harry|   Howell| hhowell7@eepurl.com|  Male|   91.235.51.73|                   |Bosnia and Herzeg...|  3/1/1962|186469.43|    Web Developer IV|                    |\n",
            "|2016-02-03 03:52:53|  9|      Jose|   Foster|   jfoster8@yelp.com|  Male|   132.31.53.61|                   |         South Korea| 3/27/1992|231067.84|Software Test Eng...|               1E+02|\n",
            "|2016-02-03 18:29:47| 10|     Emily|  Stewart|estewart9@opensou...|Female| 143.28.251.245|   3574254110301671|             Nigeria| 1/28/1997| 27234.28|     Health Coach IV|                    |\n",
            "|2016-02-03 00:10:42| 11|     Susan|  Perkins| sperkinsa@patch.com|Female|    180.85.0.62|   3573823609854134|              Russia|          |210001.95|                    |                    |\n",
            "|2016-02-03 18:04:34| 12|     Alice|    Berry|aberryb@wikipedia...|Female| 246.225.12.189|   4917830851454417|               China| 8/12/1968| 22944.53|    Quality Engineer|                    |\n",
            "|2016-02-03 18:48:17| 13|    Justin|    Berry|jberryc@usatoday.com|  Male|   157.7.146.43|6331109912871813274|              Zambia| 8/15/1975| 44165.46|Structural Analys...|                    |\n",
            "|2016-02-03 21:46:52| 14|     Kathy| Reynolds|kreynoldsd@redcro...|Female|  81.254.172.13|   5537178462965976|Bosnia and Herzeg...| 6/27/1970|286592.99|           Librarian|                    |\n",
            "|2016-02-03 08:53:23| 15|   Dorothy|   Hudson|dhudsone@blogger.com|Female|       8.59.7.0|   3542586858224170|               Japan|12/20/1989|157099.71|  Nurse Practicioner|<script>alert('hi...|\n",
            "|2016-02-03 00:44:01| 16|     Bruce|   Willis|bwillisf@bluehost...|  Male|239.182.219.189|   3573030625927601|              Brazil|          |239100.65|                    |                    |\n",
            "|2016-02-03 00:57:45| 17|     Emily|  Andrews|eandrewsg@cornell...|Female| 29.231.180.172|     30271790537626|              Russia| 4/13/1990|116800.65|        Food Chemist|                    |\n",
            "|2016-02-03 16:44:24| 18|   Stephen|  Wallace|swallaceh@netvibe...|  Male|  152.49.213.62|   5433943468526428|             Ukraine| 1/15/1978|248877.99|Account Represent...|                    |\n",
            "|2016-02-03 11:45:54| 19|  Clarence|   Lawson|clawsoni@vkontakt...|  Male| 107.175.15.152|   3544052814080964|              Russia|          |177122.99|                    |                    |\n",
            "|2016-02-03 10:30:36| 20|   Rebecca|     Bell| rbellj@bandcamp.com|Female|172.215.104.127|                   |               China|          |137251.19|                    |                    |\n",
            "+-------------------+---+----------+---------+--------------------+------+---------------+-------------------+--------------------+----------+---------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[registration_dttm: timestamp, id: int, first_name: string, last_name: string, email: string, gender: string, ip_address: string, cc: string, country: string, birthdate: string, salary: double, title: string, comments: string]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "source": [
        "!pip install pyspark\n",
        "# Import the necessary Spark libraries\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "data = [(1, 'Stephen'), (2, 'Ayden')]\n",
        "schema = ['id', 'name']\n",
        "\n",
        "df= spark.createDataFrame(data=data, schema=schema)\n",
        "df.show()\n",
        "display(df)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "ies4aYKgnYTG",
        "outputId": "f9d26077-1089-4eec-b78e-87fd381bc8b7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=0a1973457460015c1906c0c3f4cd2d90370ed5503ff87260eb21f60cdba7e670\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n",
            "+---+-------+\n",
            "| id|   name|\n",
            "+---+-------+\n",
            "|  1|Stephen|\n",
            "|  2|  Ayden|\n",
            "+---+-------+\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, name: string]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.parquet(path='/paquetsample.parquet')\n"
      ],
      "metadata": {
        "id": "iU4edfjvplM1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df =spark.read.parquet('/paquetsample.parquet')\n",
        "df.show()\n",
        "display(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "i3mPzaGJq0s5",
        "outputId": "8091329f-d0c0-4142-b623-4924a459cd81"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "| id|   name|\n",
            "+---+-------+\n",
            "|  1|Stephen|\n",
            "|  2|  Ayden|\n",
            "+---+-------+\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, name: string]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_read = spark.read.parquet('/userdata2.parquet')\n",
        "print(df.count())\n",
        "display(df)\n",
        "df.show(truncate=False, vertical=True)\n",
        "#df.show(n=1, vertical=True)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "ZvFDVhAzsAYE",
        "outputId": "3373de6a-d352-44e1-d79b-9a47bab3e120"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, name: string]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-RECORD 0-------\n",
            " id   | 1       \n",
            " name | Stephen \n",
            "-RECORD 1-------\n",
            " id   | 2       \n",
            " name | Ayden   \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "withColumn()"
      ],
      "metadata": {
        "id": "QRXau-1tvrZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col,lit\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"WithColumn\").getOrCreate()\n",
        "\n",
        "# Sample data\n",
        "data = [(1, 'Stephen','20000'), (2, 'Ayden','30000')]\n",
        "schema = ['id', 'name','salary']\n",
        "\n",
        "df = spark.createDataFrame(data=data, schema=schema)\n",
        "\n",
        "df1=df.withColumn(colName='salary', col=col ('salary').cast('Integer'))\n",
        "\n",
        "\n",
        "\n",
        "df2=df1.withColumn('salary', col('salary')*2) # updating values in existing  records\n",
        "\n",
        "\n",
        "df3=df2.withColumn('country', lit('Uganda')) # adding a new column to the dataframe\n",
        "\n",
        "df3.show()\n",
        "\n",
        "df4= df3.withColumn('copiedsalary', col('salary')) # copying an existing column and appending it to the dataframe\n",
        "df4.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmvOd1mpxfqD",
        "outputId": "8511e9dc-6945-4574-d2aa-9c0141bc8443"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+------+-------+\n",
            "| id|   name|salary|country|\n",
            "+---+-------+------+-------+\n",
            "|  1|Stephen| 40000| Uganda|\n",
            "|  2|  Ayden| 60000| Uganda|\n",
            "+---+-------+------+-------+\n",
            "\n",
            "+---+-------+------+-------+------------+\n",
            "| id|   name|salary|country|copiedsalary|\n",
            "+---+-------+------+-------+------------+\n",
            "|  1|Stephen| 40000| Uganda|       40000|\n",
            "|  2|  Ayden| 60000| Uganda|       60000|\n",
            "+---+-------+------+-------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "withColumnRenamed() - usage"
      ],
      "metadata": {
        "id": "uAwAOcI_3oqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(1, 'Stephen','20000'), (2, 'Ayden','30000')]\n",
        "columns= ['id', 'name','salary']\n",
        "\n",
        "spark.createDataFrame(data=data, schema=columns)\n",
        "df.show()\n",
        "df.show"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWerIKC53u1S",
        "outputId": "1bf366d8-221c-4aa2-c0a5-b78274203bb2"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+------+\n",
            "| id|   name|salary|\n",
            "+---+-------+------+\n",
            "|  1|Stephen| 20000|\n",
            "|  2|  Ayden| 30000|\n",
            "+---+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# help(df.withColumnRenamed)\n",
        "df.withColumnRenamed('salary','salary_amount').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GinmTFkS4wIV",
        "outputId": "106fcd05-877e-49ad-a10c-3507a587ad8a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+-------------+\n",
            "| id|   name|salary_amount|\n",
            "+---+-------+-------------+\n",
            "|  1|Stephen|        20000|\n",
            "|  2|  Ayden|        30000|\n",
            "+---+-------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "StrucType() & StructField()\n"
      ],
      "metadata": {
        "id": "rxQQUbh854I1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(1, 'Stephen','20000'), (2, 'Ayden','30000')]\n",
        "schema = ['id','name','salary']\n",
        "\n",
        "df = spark.createDataFrame(data,schema)\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lyqC-bg73Vj",
        "outputId": "3570826f-82ba-4281-de50-b5232d5e2e2b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+------+\n",
            "| id|   name|salary|\n",
            "+---+-------+------+\n",
            "|  1|Stephen| 20000|\n",
            "|  2|  Ayden| 30000|\n",
            "+---+-------+------+\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- salary: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "# Define the schema\n",
        "schema = StructType([\n",
        "                    StructField(name=\"id\",dataType=IntegerType()),\\\n",
        "                    StructField(name=\"name\",dataType=StringType()),\\\n",
        "                    StructField(name=\"salary\",dataType=IntegerType())\n",
        "                    ])\n",
        "\n",
        "#\n",
        "data = [(1, 'Stephen','20000'), (2, 'Ayden','30000')]\n",
        "schema = ['id','name','salary']\n",
        "\n",
        "df = spark.createDataFrame(data,schema)\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tjnfsNM8Rjc",
        "outputId": "03047fc5-6ca3-4a54-f596-5407d70f7630"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+------+\n",
            "| id|   name|salary|\n",
            "+---+-------+------+\n",
            "|  1|Stephen| 20000|\n",
            "|  2|  Ayden| 30000|\n",
            "+---+-------+------+\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- salary: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "complex columns"
      ],
      "metadata": {
        "id": "T0WnEjNbBx_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "data = [(1, ('Stephen', 'Waigi'), 20000), (2, ('Ayden', 'Kanja'), 30000)]\n",
        "\n",
        "structName = StructType([\n",
        "    StructField('firstName', StringType()),\n",
        "    StructField('lastName', StringType())\n",
        "])\n",
        "\n",
        "schema = StructType([\n",
        "                    StructField(name=\"id\",dataType=IntegerType()),\\\n",
        "                    StructField(name=\"name\",dataType=structName),\\\n",
        "                    StructField(name=\"salary\",dataType=IntegerType())\n",
        "                    ])\n",
        "\n",
        "\n",
        "df = spark.createDataFrame(data,schema)\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bR_3ZdY-Oyv",
        "outputId": "cde6cd6c-b6dd-41ef-deb5-2e5c0f67bcb6"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------------+------+\n",
            "| id|            name|salary|\n",
            "+---+----------------+------+\n",
            "|  1|{Stephen, Waigi}| 20000|\n",
            "|  2|  {Ayden, Kanja}| 30000|\n",
            "+---+----------------+------+\n",
            "\n",
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: struct (nullable = true)\n",
            " |    |-- firstName: string (nullable = true)\n",
            " |    |-- lastName: string (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ArrayType"
      ],
      "metadata": {
        "id": "ms41EFACCstD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [('abc', [1,2]), ('efg', [3,4]), ('hjk', [5,6])]\n",
        "schema = ['id', 'numbers']\n",
        "\n",
        "df = spark.createDataFrame(data,schema)\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iz_gtEPqCxOJ",
        "outputId": "b1e289a4-020f-4b23-ec9c-8cc4ad1e2be5"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "| id|numbers|\n",
            "+---+-------+\n",
            "|abc| [1, 2]|\n",
            "|efg| [3, 4]|\n",
            "|hjk| [5, 6]|\n",
            "+---+-------+\n",
            "\n",
            "root\n",
            " |-- id: string (nullable = true)\n",
            " |-- numbers: array (nullable = true)\n",
            " |    |-- element: long (containsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, ArrayType,StringType\n",
        "data = [('abc', [1,2]), ('efg', [3,4]), ('hjk', [5,6])]\n",
        "schema = StructType([\n",
        "                    StructField('id',StringType()),\\\n",
        "                    StructField('numbers',ArrayType(IntegerType()))\n",
        "                    ])\n",
        "\n",
        "\n",
        "df = spark.createDataFrame(data,schema)\n",
        "df.show()\n",
        "df.printSchema()\n",
        "display(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "Y4ILQSNOELSx",
        "outputId": "431a9762-d904-4b7d-f6ef-cc21c689ecbd"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "| id|numbers|\n",
            "+---+-------+\n",
            "|abc| [1, 2]|\n",
            "|efg| [3, 4]|\n",
            "|hjk| [5, 6]|\n",
            "+---+-------+\n",
            "\n",
            "root\n",
            " |-- id: string (nullable = true)\n",
            " |-- numbers: array (nullable = true)\n",
            " |    |-- element: integer (containsNull = true)\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[id: string, numbers: array<int>]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumn('firstNumber',col('numbers')[0]).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38L7hc11FWCX",
        "outputId": "e0c7a46f-ea3e-4b2b-a469-fb0068cbe4eb"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+-----------+\n",
            "| id|numbers|firstNumber|\n",
            "+---+-------+-----------+\n",
            "|abc| [1, 2]|          1|\n",
            "|efg| [3, 4]|          3|\n",
            "|hjk| [5, 6]|          5|\n",
            "+---+-------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col,array\n",
        "data = [(1,2),(3,4),(5,6)]\n",
        "schema = ['num1','num2']\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "df1=df.withColumn('numbers',array(col('num1'),col('num2')))\n",
        "df1.show()\n",
        "df1.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xp7kd4l_HTU_",
        "outputId": "45b38d2c-4a19-46fa-a9b3-a8ec1b7da056"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+-------+\n",
            "|num1|num2|numbers|\n",
            "+----+----+-------+\n",
            "|   1|   2| [1, 2]|\n",
            "|   3|   4| [3, 4]|\n",
            "|   5|   6| [5, 6]|\n",
            "+----+----+-------+\n",
            "\n",
            "root\n",
            " |-- num1: long (nullable = true)\n",
            " |-- num2: long (nullable = true)\n",
            " |-- numbers: array (nullable = false)\n",
            " |    |-- element: long (containsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "explode()"
      ],
      "metadata": {
        "id": "BU0SNOTqJay_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(1,'Stephen',['Sql','python']), (2,'Ayden',['R','java'])]\n",
        "schema = ['id','name','skills']\n",
        "\n",
        "df = spark.createDataFrame(data,schema)\n",
        "df.show()\n",
        "df.printSchema()\n",
        "display(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "jYInVm5JJcT1",
        "outputId": "b79a3833-f688-4f5b-dd8a-35a025914b6e"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+-------------+\n",
            "| id|   name|       skills|\n",
            "+---+-------+-------------+\n",
            "|  1|Stephen|[Sql, python]|\n",
            "|  2|  Ayden|    [R, java]|\n",
            "+---+-------+-------------+\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- skills: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, name: string, skills: array<string>]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode,col\n",
        "df.show\n",
        "df1=df.withColumn('skills',explode(col('skills'))).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LH_9vzNYK9ER",
        "outputId": "f0d8984c-13b1-41a7-c43d-672c26f88e6f"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+------+\n",
            "| id|   name|skills|\n",
            "+---+-------+------+\n",
            "|  1|Stephen|   Sql|\n",
            "|  1|Stephen|python|\n",
            "|  2|  Ayden|     R|\n",
            "|  2|  Ayden|  java|\n",
            "+---+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "split"
      ],
      "metadata": {
        "id": "-zJ2J2_9L5IG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import split,col\n",
        "df.show()\n",
        "df1=df.withColumn('skillsArray',split(col('skills'),',')).show()\n"
      ],
      "metadata": {
        "id": "CILt-At9L61X",
        "outputId": "170f3757-5fbd-4406-9374-c1c5f491db62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        }
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+------------+\n",
            "| id|   name|      skills|\n",
            "+---+-------+------------+\n",
            "|  1|Stephen|[Sql,python]|\n",
            "|  2|  Ayden|    [R,java]|\n",
            "+---+-------+------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve \"split(skills, ,, -1)\" due to data type mismatch: Parameter 1 requires the \"STRING\" type, however \"skills\" has the type \"ARRAY<STRING>\".;\n'Project [id#1065L, name#1066, skills#1067, split(skills#1067, ,, -1) AS skillsArray#1142]\n+- LogicalRDD [id#1065L, name#1066, skills#1067], false\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-5d3bc329e889>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skillsArray'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skills'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   5172\u001b[0m                 \u001b[0mmessage_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"arg_name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"col\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"arg_type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5173\u001b[0m             )\n\u001b[0;32m-> 5174\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve \"split(skills, ,, -1)\" due to data type mismatch: Parameter 1 requires the \"STRING\" type, however \"skills\" has the type \"ARRAY<STRING>\".;\n'Project [id#1065L, name#1066, skills#1067, split(skills#1067, ,, -1) AS skillsArray#1142]\n+- LogicalRDD [id#1065L, name#1066, skills#1067], false\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(1,'Stephen','Sql','python'), (2,'Ayden','R','java')]\n",
        "schema = ['id','name','primarySkills','secondarySkills']\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "df.show()\n",
        "df.printSchema()\n",
        "display(df)"
      ],
      "metadata": {
        "id": "HF2RTS8dOlTh",
        "outputId": "799a0b72-5cb6-4d16-d62c-81524c81a4b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+-------------+---------------+\n",
            "| id|   name|primarySkills|secondarySkills|\n",
            "+---+-------+-------------+---------------+\n",
            "|  1|Stephen|          Sql|         python|\n",
            "|  2|  Ayden|            R|           java|\n",
            "+---+-------+-------------+---------------+\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- primarySkills: string (nullable = true)\n",
            " |-- secondarySkills: string (nullable = true)\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, name: string, primarySkills: string, secondarySkills: string]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "source": [
        "from pyspark.sql.functions import array, col\n",
        "\n",
        "df1 = df.withColumn('skillsArray', array(col('primarySkills'), col('secondarySkills'))).show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "1Z6zGzN4QCP8",
        "outputId": "3690e2b7-2e57-4156-dcce-7e7511e76638",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+-------------+---------------+-------------+\n",
            "| id|   name|primarySkills|secondarySkills|  skillsArray|\n",
            "+---+-------+-------------+---------------+-------------+\n",
            "|  1|Stephen|          Sql|         python|[Sql, python]|\n",
            "|  2|  Ayden|            R|           java|    [R, java]|\n",
            "+---+-------+-------------+---------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-_3eWSNAPk_x"
      }
    }
  ]
}