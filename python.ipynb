{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5VamUPXQHqhr9nlc081Fy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waigisteve/ProforlioProject/blob/main/python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"its that time again\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QspLkQsquURo",
        "outputId": "cd608f10-aeca-43ce-d451-7f23cc8b1801"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "its that time again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mhW10otguI_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x=14"
      ],
      "metadata": {
        "id": "lxz9YapBvThF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CA8iFk0suDeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCO3-i2mveEO",
        "outputId": "64339aa1-d43d-4374-a337-59e3bae32c86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y=34\n",
        "sum =x+y\n",
        "print(sum)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQL25I_xv5lF",
        "outputId": "01da5ebb-2b9e-4bcb-e431-010bbf730167"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: A script that prints the average of three subjects  scores Maths, Science , Geography  add an extra 5 marks for each subject and generate the answer\n",
        "\n",
        "# Scores for each subject\n",
        "maths = 70\n",
        "science = 80\n",
        "geography = 90\n",
        "\n",
        "# Adding bonus marks\n",
        "maths += 5\n",
        "science += 5\n",
        "geography += 5\n",
        "\n",
        "# Calculate the total score\n",
        "total_score = maths + science + geography\n",
        "\n",
        "# Calculate the average score\n",
        "average_score = total_score / 3\n",
        "\n",
        "# Print the average score\n",
        "print(\"The average score is:\", average_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXklWEZ4wYtl",
        "outputId": "84a02704-4de2-4b87-eeff-25756eb9f906"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "its that time again\n",
            "14\n",
            "48\n",
            "The average score is: 82.33333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maths = \"good marks\"\n",
        "science =\"excellent\"\n",
        "total =maths + science\n",
        "print(total)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYNYOB021k2q",
        "outputId": "13d6bee7-2e26-4b4c-d606-02520bfac142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "good marksexcellent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "science =100\n",
        "maths =\"120\"\n",
        "total =int(maths) + science\n",
        "print(total)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIizEu6k5jxy",
        "outputId": "5a502cbe-387a-465b-8d52-bbdc8d4544ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "220\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maths = int(input(\"enter maths marks\"))\n",
        "science = int(input(\"enter science marks\"))\n",
        "total =maths + science\n",
        "total_marks = maths + science\n",
        "print(total/200*100)\n",
        "\n",
        "if total >89:\n",
        "  print(\"A\")\n",
        "elif total >79:\n",
        "  print(\"B\")\n",
        "elif total >69:\n",
        "  print(\"C\")\n",
        "elif total >59:\n",
        "  print(\"D\")\n",
        "else:\n",
        "  print(\"F\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9hhTD2k8GX-",
        "outputId": "183c3249-626e-4697-a02f-c0b25f982eaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter maths marks43\n",
            "enter science marks23\n",
            "33.0\n",
            "D\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "from os import PRIO_PROCESS\n",
        "Movie Ticket Pricing\n",
        "Scenario: a cinema has different  ticket prices based on age.\n",
        "Task: Write a program that asks for the customers age adn prints the ticket PRIO_PROCESS\n",
        "\n",
        "child (0-12): 200\n",
        "Teen (13-17): 400\n",
        "Adult (18-64): 600\n",
        "Senior (65 and above): 800"
      ],
      "metadata": {
        "id": "mN8lG5uiCGtx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LYsy5jLHCJ3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "age = int(input(\"Enter your age: \"))\n",
        "\n",
        "if age <= 12:\n",
        "    price = 200\n",
        "    category = \"Child\"\n",
        "elif age <= 17:\n",
        "    price = 400\n",
        "    category = \"Teen\"\n",
        "elif age <= 64:\n",
        "    price = 600\n",
        "    category = \"Adult\"\n",
        "else:\n",
        "    price = 800\n",
        "    category = \"Senior\"\n",
        "\n",
        "print(f\"Ticket price for {category}: {price}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WA8I9FfpCLFa",
        "outputId": "06a5aa21-c562-4900-d1ca-32c338a32c04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your age: 8\n",
            "Ticket price for Child: 200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "a country has a progressive tax system based on annual income. create a program that calculates tax based on the following brackets up to 350000 no tax 351000 to 500000 5%\n",
        "501,000 to 1000000 10% above 1000000 20% tax"
      ],
      "metadata": {
        "id": "YO72BmxxGQ2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_tax(income):\n",
        "    if income <= 350000:\n",
        "        tax = 0\n",
        "        tax_percent = 0\n",
        "    elif income <= 500000:\n",
        "        tax = (income - 350000) * 0.05\n",
        "        tax_percent = 5\n",
        "    elif income <= 1000000:\n",
        "        tax = (150000 * 0.05) + (income - 500000) * 0.10\n",
        "        tax_percent = (tax / income) * 100  # Calculate effective tax percentage\n",
        "    else:\n",
        "        tax = (150000 * 0.05) + (500000 * 0.10) + (income - 1000000) * 0.20\n",
        "        tax_percent = (tax / income) * 100  # Calculate effective tax percentage\n",
        "\n",
        "    return tax, tax_percent\n",
        "\n",
        "income = float(input(\"Enter your annual income: \"))\n",
        "tax_amount, tax_percentage = calculate_tax(income)\n",
        "\n",
        "print(f\"Tax payable on income of ${income:,.2f} is:\")\n",
        "print(f\"- Amount: ${tax_amount:,.2f}\")\n",
        "print(f\"- Effective Tax Percentage: {tax_percentage:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVAGYH_XGZvW",
        "outputId": "c95ec28f-5b88-477e-898f-67e14cff4d0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your annual income: 1300000\n",
            "Tax payable on income of $1,300,000.00 is:\n",
            "- Amount: $117,500.00\n",
            "- Effective Tax Percentage: 9.04%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YXaFwkTzKpLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "546PymiJKsi8",
        "outputId": "87fae08c-469a-4b6d-a3ec-b669a43a7a6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your exam score: 45\n",
            "Admission unlikely\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Advanced Loan Approval System\n",
        "\n",
        "Scenario: A bank has a complex loan approval process based on multiple factors.\n",
        "Task: Create a program that determines loan approval and interest rate based on:\n",
        "\n",
        "Credit Score (Excellent: 800+, Good: 700-799, Fair: 600-699, Poor: below 600)\n",
        "Annual Income (High: 1,000,000+, Medium: 500,000-999,999, Low: below 500,000)\n",
        "Debt-to-Income Ratio (Low: below 20%, Medium: 20-36%, High: above 36%)\n",
        "Loan Amount (Small: below 1,000,000, Medium: 1,000,000-5,000,000, Large: above 5,000,000)\n",
        "\n",
        "Rules:\n",
        "\n",
        "Approve loan if Credit Score is Good or Excellent, and Debt-to-Income Ratio is Low or Medium\n",
        "Reject loan if Credit Score is Poor, or if Debt-to-Income Ratio is High\n",
        "For Fair Credit Score, approve only if Income is High and Debt-to-Income Ratio is Low\n",
        "Interest rates:\n",
        "\n",
        "8% for Excellent Credit, Low Debt-to-Income, and High Income\n",
        "10% for Good Credit or (Fair Credit with High Income and Low Debt-to-Income)\n",
        "12% for all other approved loans"
      ],
      "metadata": {
        "id": "iPId-LNyLNK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def assess_loan(credit_score, income, dti):\n",
        "    approved = False\n",
        "    interest_rate = None\n",
        "\n",
        "    if credit_score >= 800:\n",
        "        if income >= 1000000 and dti < 0.20:\n",
        "            approved = True\n",
        "            interest_rate = 0.03  # Excellent credit, high income, low DTI\n",
        "        elif income >= 500000 and dti < 0.36:\n",
        "            approved = True\n",
        "            interest_rate = 0.04  # Excellent credit, medium income, reasonable DTI\n",
        "        elif dti < 0.20:\n",
        "            approved = True\n",
        "            interest_rate = 0.05  # Excellent credit, but lower income or higher DTI\n",
        "    elif credit_score >= 700:\n",
        "        if income >= 500000 and dti < 0.20:\n",
        "            approved = True\n",
        "            interest_rate = 0.06  # Good credit, medium income, low DTI\n",
        "        elif dti < 0.30:\n",
        "            approved = True\n",
        "            interest_rate = 0.07  # Good credit, but lower income or higher DTI\n",
        "    elif credit_score >= 600:\n",
        "        if income >= 750000 and dti < 0.20:\n",
        "            approved = True\n",
        "            interest_rate = 0.08  # Fair credit, high income, low DTI\n",
        "    # No approval for poor credit (below 600)\n",
        "\n",
        "    return approved, interest_rate\n",
        "\n",
        "# Get user input\n",
        "credit_score = int(input(\"Enter your credit score: \"))\n",
        "income = float(input(\"Enter your annual income: \"))\n",
        "dti = float(input(\"Enter your debt-to-income ratio (e.g., 0.3 for 30%): \"))\n",
        "\n",
        "# Assess loan application\n",
        "approved, interest_rate = assess_loan(credit_score, income, dti)\n",
        "\n",
        "if approved:\n",
        "    print(\"Congratulations! Your loan is approved.\")\n",
        "    print(f\"Your interest rate is: {interest_rate * 100:.2f}%\")\n",
        "else:\n",
        "    print(\"Unfortunately, your loan application is not approved at this time.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36yrE5wZLTnZ",
        "outputId": "78b13cf3-436b-423b-ab11-12502ae784f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your credit score: 500\n",
            "Enter your annual income: 1200000\n",
            "Enter your debt-to-income ratio (e.g., 0.3 for 30%): 40\n",
            "Unfortunately, your loan application is not approved at this time.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyodbc\n",
        "\n",
        "# Database connection details\n",
        "server = 'STEPHENWAIGI'\n",
        "database = 'Roster'\n",
        "\n",
        "try:\n",
        "    # Connect to the database using Windows Authentication\n",
        "    conn = pyodbc.connect('Driver={SQL Server};'\n",
        "                          'Server=' + server + ';'\n",
        "                          'Database=' + database + ';'\n",
        "                          'Trusted_Connection=yes;')\n",
        "\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Execute the query to fetch data from the Revenue table\n",
        "    cursor.execute(\"SELECT * FROM Revenue\")\n",
        "    rows = cursor.fetchall()\n",
        "\n",
        "    # Display the data\n",
        "    for row in rows:\n",
        "        print(row)\n",
        "\n",
        "except pyodbc.Error as ex:\n",
        "    print(\"Error connecting to database or fetching data:\", ex)\n",
        "\n",
        "finally:\n",
        "    if conn:\n",
        "        conn.close()\n",
        "        print(\"Database connection closed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "mFmJoGQ2P7gP",
        "outputId": "8017b96f-1921-4d74-b9eb-872b233c781f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error connecting to database or fetching data: ('01000', \"[01000] [unixODBC][Driver Manager]Can't open lib 'SQL Server' : file not found (0) (SQLDriverConnect)\")\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'conn' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-31133ea8f978>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Database connection closed.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'conn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyodbc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0EthSO1QMDR",
        "outputId": "ec08d060-042c-4a26-e9f0-c7136a4a2ae3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyodbc\n",
            "  Downloading pyodbc-5.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (334 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m334.7/334.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyodbc\n",
            "Successfully installed pyodbc-5.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install unixodbc unixodbc-dev"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGIrh_A5QyuE",
        "outputId": "0c6c94ef-acb1-4c0c-f18c-ed934e1903fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "unixodbc-dev is already the newest version (2.3.9-5ubuntu0.1).\n",
            "unixodbc-dev set to manually installed.\n",
            "The following NEW packages will be installed:\n",
            "  unixodbc\n",
            "0 upgraded, 1 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 26.7 kB of archives.\n",
            "After this operation, 111 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 unixodbc amd64 2.3.9-5ubuntu0.1 [26.7 kB]\n",
            "Fetched 26.7 kB in 3s (9,726 B/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package unixodbc.\n",
            "(Reading database ... 123586 files and directories currently installed.)\n",
            "Preparing to unpack .../unixodbc_2.3.9-5ubuntu0.1_amd64.deb ...\n",
            "Unpacking unixodbc (2.3.9-5ubuntu0.1) ...\n",
            "Setting up unixodbc (2.3.9-5ubuntu0.1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "odbcinst -j"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "NjOPPI7WRZbR",
        "outputId": "30c3a0c7-f0f3-447d-bb09-f67bef2c7691"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'odbcinst' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-dab819a1a072>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0modbcinst\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'odbcinst' is not defined"
          ]
        }
      ]
    },
    {
      "source": [
        "sudo apt-get install unixodbc unixodbc-dev"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "oUB_ADstReht",
        "outputId": "4c9c19df-e092-4b9f-db92-3720c017b66b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-40-3f4ec9a7584e>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-40-3f4ec9a7584e>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    sudo apt-get install unixodbc unixodbc-dev\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!sudo apt-get install unixodbc unixodbc-dev"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIVK3YxQRzJj",
        "outputId": "a33c8c6f-5343-4eae-f25d-8c501d21ba63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "unixodbc-dev is already the newest version (2.3.9-5ubuntu0.1).\n",
            "unixodbc is already the newest version (2.3.9-5ubuntu0.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FtCFbZbkTKUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comprehensive Health Insurance Premium Calculator\n",
        "\n",
        "Scenario: A health insurance company determines premiums based on multiple health factors.\n",
        "Task: Create a program that calculates the insurance premium based on:\n",
        "\n",
        "Age (Child: 0-18, Young Adult: 19-35, Adult: 36-55, Senior: 56+)\n",
        "BMI Category (Underweight, Normal, Overweight, Obese)\n",
        "Smoking Status (Non-smoker, Occasional, Regular)\n",
        "Pre-existing Conditions (None, Minor, Major)\n",
        "Family History (No risks, Minor risks, Major risks)\n",
        "\n",
        "Rules:\n",
        "\n",
        "Base premium: 5000 for Child, 10000 for Young Adult, 15000 for Adult, 20000 for Senior\n",
        "Increase premium by:\n",
        "\n",
        "20% for Overweight, 50% for Obese\n",
        "30% for Occasional Smoker, 50% for Regular Smoker\n",
        "20% for Minor pre-existing conditions, 50% for Major\n",
        "10% for Minor family history risks, 25% for Major\n",
        "\n",
        "\n",
        "Decrease premium by 10% for Underweight\n",
        "Maximum increase: 150% of base premium\n",
        "Add flat 1000 for each year above 30"
      ],
      "metadata": {
        "id": "S6Tlwv-7UFpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_final_fare(base_fare, peak_season=False, holiday_season=False, weekend=False, booking_time='regular', passenger_type='adult'):\n",
        "    # Start with the base fare\n",
        "    fare = base_fare\n",
        "\n",
        "    # Apply seasonal adjustments\n",
        "    if peak_season:\n",
        "        fare *= 1.20\n",
        "    elif holiday_season:\n",
        "        fare *= 1.10\n",
        "\n",
        "    # Apply weekend adjustment\n",
        "    if weekend:\n",
        "        fare *= 1.10\n",
        "\n",
        "    # Apply booking time adjustments\n",
        "    if booking_time == 'late':\n",
        "        fare *= 1.15\n",
        "    elif booking_time == 'early':\n",
        "        fare *= 0.90\n",
        "\n",
        "    # Apply passenger type adjustments\n",
        "    if passenger_type == 'child':\n",
        "        fare *= 0.75\n",
        "    elif passenger_type == 'senior':\n",
        "        fare *= 0.90\n",
        "\n",
        "    # Ensure maximum combined discount and increase limits\n",
        "    if fare < base_fare * 0.70:\n",
        "        fare = base_fare * 0.70\n",
        "    if fare > base_fare * 1.50:\n",
        "        fare = base_fare * 1.50\n",
        "\n",
        "    return fare\n",
        "\n",
        "# Prompt the user for inputs\n",
        "base_fare = float(input(\"Enter the base fare: \"))\n",
        "peak_season = input(\"Is it peak season? (yes/no): \").lower() == 'yes'\n",
        "holiday_season = input(\"Is it holiday season? (yes/no): \").lower() == 'yes'\n",
        "weekend = input(\"Is the travel on a weekend? (yes/no): \").lower() == 'yes'\n",
        "booking_time = input(\"Enter the booking time (early, regular, late): \").lower()\n",
        "passenger_type = input(\"Enter the passenger type (adult, child, senior): \").lower()\n",
        "\n",
        "# Calculate the final fare\n",
        "final_fare = calculate_final_fare(base_fare, peak_season, holiday_season, weekend, booking_time, passenger_type)\n",
        "\n",
        "print(f\"The final fare is: ${final_fare:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKT1MIhkVMSL",
        "outputId": "8f2a1f86-60ce-476d-f0a4-69f13df02537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the base fare: 5300\n",
            "Is it peak season? (yes/no): yes\n",
            "Is it holiday season? (yes/no): yes\n",
            "Is the travel on a weekend? (yes/no): no\n",
            "Enter the booking time (early, regular, late): late\n",
            "Enter the passenger type (adult, child, senior): senior\n",
            "The final fare is: $6582.60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comprehensive Health Insurance Premium Calculator\n",
        "\n",
        "Scenario: A health insurance company determines premiums based on multiple health factors.\n",
        "Task: Create a program that calculates the insurance premium based on:\n",
        "\n",
        "Age (Child: 0-18, Young Adult: 19-35, Adult: 36-55, Senior: 56+)\n",
        "BMI Category (Underweight, Normal, Overweight, Obese)\n",
        "Smoking Status (Non-smoker, Occasional, Regular)\n",
        "Pre-existing Conditions (None, Minor, Major)\n",
        "Family History (No risks, Minor risks, Major risks)\n",
        "\n",
        "Rules:\n",
        "\n",
        "Base premium: 5000 for Child, 10000 for Young Adult, 15000 for Adult, 20000 for Senior\n",
        "Increase premium by:\n",
        "\n",
        "20% for Overweight, 50% for Obese\n",
        "30% for Occasional Smoker, 50% for Regular Smoker\n",
        "20% for Minor pre-existing conditions, 50% for Major\n",
        "10% for Minor family history risks, 25% for Major\n",
        "\n",
        "\n",
        "Decrease premium by 10% for Underweight\n",
        "Maximum increase: 150% of base premium\n",
        "Add flat 1000 for each year above 30\n"
      ],
      "metadata": {
        "id": "d1iGDCs8ZXG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_premium(age, bmi_category, smoking_status, pre_existing_conditions, family_history):\n",
        "    # Determine base premium based on age\n",
        "    if age <= 18:\n",
        "        base_premium = 5000\n",
        "    elif 19 <= age <= 35:\n",
        "        base_premium = 10000\n",
        "    elif 36 <= age <= 55:\n",
        "        base_premium = 15000\n",
        "    else:\n",
        "        base_premium = 20000\n",
        "\n",
        "    premium = base_premium\n",
        "\n",
        "    # Adjust premium based on BMI category\n",
        "    if bmi_category == 'underweight':\n",
        "        premium *= 0.90\n",
        "    elif bmi_category == 'overweight':\n",
        "        premium *= 1.20\n",
        "    elif bmi_category == 'obese':\n",
        "        premium *= 1.50\n",
        "\n",
        "    # Adjust premium based on smoking status\n",
        "    if smoking_status == 'occasional':\n",
        "        premium *= 1.30\n",
        "    elif smoking_status == 'regular':\n",
        "        premium *= 1.50\n",
        "\n",
        "    # Adjust premium based on pre-existing conditions\n",
        "    if pre_existing_conditions == 'minor':\n",
        "        premium *= 1.20\n",
        "    elif pre_existing_conditions == 'major':\n",
        "        premium *= 1.50\n",
        "\n",
        "    # Adjust premium based on family history\n",
        "    if family_history == 'minor':\n",
        "        premium *= 1.10\n",
        "    elif family_history == 'major':\n",
        "        premium *= 1.25\n",
        "\n",
        "    # Cap the maximum increase\n",
        "    if premium > base_premium * 2.50:\n",
        "        premium = base_premium * 2.50\n",
        "\n",
        "    # Add flat 1000 for each year above 30\n",
        "    if age > 30:\n",
        "        premium += (age - 30) * 1000\n",
        "\n",
        "    return premium\n",
        "\n",
        "# Prompt the user for inputs\n",
        "age = int(input(\"Enter your age: \"))\n",
        "bmi_category = input(\"Enter your BMI category (underweight, normal, overweight, obese): \").lower()\n",
        "smoking_status = input(\"Enter your smoking status (non-smoker, occasional, regular): \").lower()\n",
        "pre_existing_conditions = input(\"Enter your pre-existing conditions status (none, minor, major): \").lower()\n",
        "family_history = input(\"Enter your family history risk (no risks, minor risks, major risks): \").lower()\n",
        "\n",
        "# Calculate the premium\n",
        "final_premium = calculate_premium(age, bmi_category, smoking_status, pre_existing_conditions, family_history)\n",
        "\n",
        "print(f\"The calculated insurance premium is: ${final_premium:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkcqDBK7ZhKl",
        "outputId": "65b59713-00c0-4922-cc45-9da6297fa049"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your age: 56\n",
            "Enter your BMI category (underweight, normal, overweight, obese): overweight\n",
            "Enter your smoking status (non-smoker, occasional, regular): occasional\n",
            "Enter your pre-existing conditions status (none, minor, major): minor\n",
            "Enter your family history risk (no risks, minor risks, major risks): minor risks\n",
            "The calculated insurance premium is: $63440.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Advanced Academic Performance Evaluator\n",
        "\n",
        "Scenario: A school uses a comprehensive system to evaluate student performance and provide tailored advice.\n",
        "Task: Create a program that assesses a student's performance and provides recommendations based on:\n",
        "\n",
        "Grades in core subjects (Math, Science, Language, Social Studies)\n",
        "Extracurricular activities participation (None, Low, Medium, High)\n",
        "Attendance rate (Excellent: >95%, Good: 90-95%, Fair: 80-89%, Poor: <80%)\n",
        "Behavioral record (Excellent, Good, Needs Improvement, Poor)\n",
        "Learning style (Visual, Auditory, Kinesthetic)\n",
        "\n",
        "Rules:\n",
        "\n",
        "Calculate GPA (4.0 scale) from core subject grades\n",
        "Determine overall academic standing:\n",
        "\n",
        "Outstanding: GPA > 3.5, Good attendance, Good or Excellent behavior\n",
        "Good: GPA 3.0-3.5, At least Fair attendance, At least Needs Improvement behavior\n",
        "Average: GPA 2.0-2.9, At least Fair attendance\n",
        "Needs Improvement: GPA < 2.0 or Poor attendance"
      ],
      "metadata": {
        "id": "k5EFwtqNj00V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_gpa(grades):\n",
        "    return sum(grades) / len(grades)\n",
        "\n",
        "def determine_academic_standing(gpa, attendance_rate, behavioral_record):\n",
        "    if gpa > 3.5 and attendance_rate in ['excellent', 'good'] and behavioral_record in ['excellent', 'good']:\n",
        "        return 'Outstanding'\n",
        "    elif 3.0 <= gpa <= 3.5 and attendance_rate in ['excellent', 'good', 'fair'] and behavioral_record in ['excellent', 'good', 'needs improvement']:\n",
        "        return 'Good'\n",
        "    elif 2.0 <= gpa <= 2.9 and attendance_rate in ['excellent', 'good', 'fair']:\n",
        "        return 'Average'\n",
        "    else:\n",
        "        return 'Needs Improvement'\n",
        "\n",
        "def provide_recommendations(academic_standing, learning_style):\n",
        "    recommendations = {\n",
        "        'Outstanding': 'Keep up the great work! Continue to challenge yourself with advanced courses and leadership roles in extracurricular activities.',\n",
        "        'Good': 'You are doing well! Consider seeking additional help in subjects you find challenging and stay involved in extracurricular activities.',\n",
        "        'Average': 'Focus on improving your study habits and consider tutoring for subjects where you struggle. Maintain consistent attendance.',\n",
        "        'Needs Improvement': 'Seek help from teachers and counselors to address academic and attendance issues. Develop a study plan and stick to it.',\n",
        "    }\n",
        "\n",
        "    learning_style_advice = {\n",
        "        'visual': 'Use diagrams, charts, and written notes to enhance your learning.',\n",
        "        'auditory': 'Participate in discussions and use audio recordings to reinforce learning.',\n",
        "        'kinesthetic': 'Engage in hands-on activities and practical exercises to understand concepts better.'\n",
        "    }\n",
        "\n",
        "    return f\"{recommendations[academic_standing]} Additionally, since you are a {learning_style} learner, {learning_style_advice[learning_style]}\"\n",
        "\n",
        "# Prompt the user for inputs\n",
        "grades = []\n",
        "subjects = ['Math', 'Science', 'Language', 'Social Studies']\n",
        "for subject in subjects:\n",
        "    grade = float(input(f\"Enter your grade for {subject} (0.0 - 4.0 scale): \"))\n",
        "    grades.append(grade)\n",
        "\n",
        "extracurricular_activities = input(\"Enter your level of participation in extracurricular activities (none, low, medium, high): \").lower()\n",
        "attendance_rate = input(\"Enter your attendance rate (excellent: >95%, good: 90-95%, fair: 80-89%, poor: <80%): \").lower()\n",
        "behavioral_record = input(\"Enter your behavioral record (excellent, good, needs improvement, poor): \").lower()\n",
        "learning_style = input(\"Enter your learning style (visual, auditory, kinesthetic): \").lower()\n",
        "\n",
        "# Calculate GPA\n",
        "gpa = calculate_gpa(grades)\n",
        "\n",
        "# Determine academic standing\n",
        "academic_standing = determine_academic_standing(gpa, attendance_rate, behavioral_record)\n",
        "\n",
        "# Provide recommendations\n",
        "recommendations = provide_recommendations(academic_standing, learning_style)\n",
        "\n",
        "print(f\"\\nYour GPA is: {gpa:.2f}\")\n",
        "print(f\"Your academic standing is: {academic_standing}\")\n",
        "print(f\"Recommendations: {recommendations}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MoH_BVXlY4N",
        "outputId": "6125ad47-5acc-4233-ccfc-0d2f01edcfd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your grade for Math (0.0 - 4.0 scale): 3.6\n",
            "Enter your grade for Science (0.0 - 4.0 scale): 3.1\n",
            "Enter your grade for Language (0.0 - 4.0 scale): 4\n",
            "Enter your grade for Social Studies (0.0 - 4.0 scale): 3.3\n",
            "Enter your level of participation in extracurricular activities (none, low, medium, high): medium\n",
            "Enter your attendance rate (excellent: >95%, good: 90-95%, fair: 80-89%, poor: <80%): good\n",
            "Enter your behavioral record (excellent, good, needs improvement, poor): poor\n",
            "Enter your learning style (visual, auditory, kinesthetic): visual\n",
            "\n",
            "Your GPA is: 3.50\n",
            "Your academic standing is: Needs Improvement\n",
            "Recommendations: Seek help from teachers and counselors to address academic and attendance issues. Develop a study plan and stick to it. Additionally, since you are a visual learner, Use diagrams, charts, and written notes to enhance your learning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "loops\n"
      ],
      "metadata": {
        "id": "jOpiDf3houZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i=1\n",
        "while i<=5:\n",
        "  print(\"Jump\")\n",
        "  i=i+1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lMe0s1gow5s",
        "outputId": "eb15d829-9e20-48b1-b208-1cfc8877b37f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jump\n",
            "Jump\n",
            "Jump\n",
            "Jump\n",
            "Jump\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "script that calculates the average percentage based on the marks entered for six subjects (Math, English, Physics, Chemistry, Geography, Business):\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AQ4zFXYNtfoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate average percentage and determine result\n",
        "def calculate_average_and_result(marks):\n",
        "    total_marks = sum(marks)\n",
        "    average_percentage = total_marks / len(marks)\n",
        "\n",
        "    if average_percentage < 50:\n",
        "        result = \"Fail\"\n",
        "    elif 50 <= average_percentage < 60:\n",
        "        result = \"Pass\"\n",
        "    elif 60 <= average_percentage < 70:\n",
        "        result = \"Credit\"\n",
        "    else:\n",
        "        result = \"Distinction\"\n",
        "\n",
        "    return average_percentage, result\n",
        "\n",
        "# List of subjects\n",
        "subjects = [\"Maths\", \"English\", \"Physics\", \"Chemistry\", \"Business\"]\n",
        "\n",
        "# List to store marks\n",
        "marks = []\n",
        "\n",
        "# Loop to get marks for each subject\n",
        "for subject in subjects:\n",
        "    while True:\n",
        "        try:\n",
        "            mark = float(input(f\"Enter marks for {subject}: \"))\n",
        "            if 0 <= mark <= 100:\n",
        "                marks.append(mark)\n",
        "                break\n",
        "            else:\n",
        "                print(\"Please enter a valid mark between 0 and 100.\")\n",
        "        except ValueError:\n",
        "            print(\"Please enter a valid number.\")\n",
        "\n",
        "# Calculate average percentage and result\n",
        "average_percentage, result = calculate_average_and_result(marks)\n",
        "\n",
        "# Print the results\n",
        "print(f\"\\nYour average percentage is: {average_percentage:.2f}%\")\n",
        "print(f\"Your result is: {result}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuEpX8pvthk3",
        "outputId": "e1f1952c-9b9c-44e6-bb88-844b8981118d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter marks for Maths: 40\n",
            "Enter marks for English: 60\n",
            "Enter marks for Physics: 57\n",
            "Enter marks for Chemistry: 68\n",
            "Enter marks for Business: 80\n",
            "\n",
            "Your average percentage is: 61.00%\n",
            "Your result is: Credit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "code to use use loop to confirm password three times after which user is prompted to reset password."
      ],
      "metadata": {
        "id": "21vUX_i3yU4I"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S-wRLkSE1HAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actualPass = 'admin'\n",
        "i = 1\n",
        "\n",
        "while i < 4:\n",
        "    password = input(\"Enter the password: \")\n",
        "    if password == actualPass:\n",
        "        print(\"Login Successful\")\n",
        "        break\n",
        "    else:\n",
        "        print(\"Retry\")\n",
        "    i = i + 1\n",
        "\n",
        "if i == 4:\n",
        "    change_password = input(\"You have exhausted all retries. Would you like to change the password? (yes/no): \").lower()\n",
        "    if change_password == 'yes':\n",
        "        new_password = input(\"Enter the new password: \")\n",
        "        confirm_password = input(\"Confirm the new password: \")\n",
        "        if new_password == confirm_password:\n",
        "            actualPass = new_password\n",
        "            print(\"Password changed successfully.\")\n",
        "        else:\n",
        "            print(\"Passwords do not match. Password change failed.\")\n",
        "    else:\n",
        "        print(\"Password change not initiated.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6epEw6yyfqH",
        "outputId": "26e34956-73cc-4932-9009-791b315037db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the password: sdfdsf\n",
            "Retry\n",
            "Enter the password: sdfds\n",
            "Retry\n",
            "Enter the password: sdfsd\n",
            "Retry\n",
            "You have exhausted all retries. Would you like to change the password? (yes/no): yes\n",
            "Enter the new password: admin\n",
            "Confirm the new password: admin\n",
            "Password changed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GUfcJCcn3WEF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Countdown Timer\n",
        "Create a program that simulates a countdown timer.\n",
        "Start with a variable seconds  set to 10.\n",
        "Use a while loop to count down from 10 to 1, printing each number.\n",
        "After the loop ends, print \"Time's up!\".\n",
        "\n"
      ],
      "metadata": {
        "id": "X6kcr3O23Wsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Set the initial countdown time in seconds\n",
        "seconds = 10\n",
        "\n",
        "# Start the countdown\n",
        "while seconds > 0:\n",
        "    print(seconds)\n",
        "    time.sleep(1)  # Wait for 1 second\n",
        "    seconds -= 1\n",
        "\n",
        "# Print the final message\n",
        "print(\"Time's up!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22PAYGfR37Cl",
        "outputId": "ed2effea-1913-4a14-d936-0c5b7b0d69d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "9\n",
            "8\n",
            "7\n",
            "6\n",
            "5\n",
            "4\n",
            "3\n",
            "2\n",
            "1\n",
            "Time's up!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple Interest Calculator\n",
        "Develop a program that calculates simple interest over a period of years.\n",
        "Set initial variables: principal = 1000, rate = 0.05 (5%), years = 5.\n",
        "Use a while loop to calculate and print the balance for each year.\n",
        "For each year, calculate the interest and add it to the principal."
      ],
      "metadata": {
        "id": "UFXNr3c94Jn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set initial variables\n",
        "principal = 1000\n",
        "rate = 0.05  # 5% interest rate\n",
        "years = 5\n",
        "\n",
        "# Initialize a variable to track the current year\n",
        "current_year = 1\n",
        "\n",
        "# Calculate and print the balance for each year\n",
        "while current_year <= years:\n",
        "    # Calculate interest for the current year\n",
        "    interest = principal * rate\n",
        "    # Add the interest to the principal\n",
        "    principal += interest\n",
        "    # Print the balance for the current year\n",
        "    print(f\"Year {current_year}: Balance = ${principal:.2f}\")\n",
        "    # Move to the next year\n",
        "    current_year += 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XlENOip4Xl_",
        "outputId": "cafc88a2-561f-490c-fd93-134494c9d0af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Year 1: Balance = $1050.00\n",
            "Year 2: Balance = $1102.50\n",
            "Year 3: Balance = $1157.62\n",
            "Year 4: Balance = $1215.51\n",
            "Year 5: Balance = $1276.28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set initial variables\n",
        "principal = 1000\n",
        "rate = 0.05  # 5% interest rate\n",
        "\n",
        "# Prompt the user for the number of years\n",
        "years = int(input(\"Enter the number of years: \"))\n",
        "\n",
        "# Initialize a variable to track the current year\n",
        "current_year = 1\n",
        "\n",
        "# Calculate and print the balance for each year\n",
        "while current_year <= years:\n",
        "    # Calculate interest for the current year\n",
        "    interest = principal * rate\n",
        "    # Add the interest to the principal\n",
        "    principal += interest\n",
        "    # Print the balance for the current year\n",
        "    print(f\"Year {current_year}: Balance = ${principal:.2f}\")\n",
        "    # Move to the next year\n",
        "    current_year += 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiTxj9t74rBf",
        "outputId": "8e21ab8f-e0f2-43be-c1e2-210ea84cb695"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the number of years: 3\n",
            "Year 1: Balance = $1050.00\n",
            "Year 2: Balance = $1102.50\n",
            "Year 3: Balance = $1157.62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**list**   ordered changable and allows duplicates"
      ],
      "metadata": {
        "id": "3UoGCxUt8nda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "abc = [1,2,3,4,5]\n",
        "xyz = ['Stephen','Jacinta','Ayden']\n",
        "newlist =[1,'Stephen', 4.5]\n",
        "\n"
      ],
      "metadata": {
        "id": "iVraXRIf8pZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(abc)\n",
        "print(type(abc))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRcIU2-_9WqF",
        "outputId": "54836b77-999b-4c82-9a2e-f333559e7544"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5]\n",
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (abc[0])\n",
        "print (abc[3])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYZ_6Orw9gXV",
        "outputId": "fbdce778-8b49-4755-ea8d-a57cf4aa7220"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xyz[1] = 'Clement'\n",
        "print(xyz[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3AOBWmA-xoM",
        "outputId": "3f14f079-91b9-439f-f198-b8a553d7ee1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clement\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "abc =[1,2,3,2,1,6,5,7]\n",
        "print(len(abc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-lTQznV_62o",
        "outputId": "592b524f-db7f-49e0-c0a5-2231b1b19ea1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (abc[-4])  #index in python starts from zero"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iwq_x16kAeQc",
        "outputId": "ac43bedd-f9e8-45bb-b925-53366b18eb25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(abc[1:4])   # fisrt index and last index excluded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pY2VKr7SAxkG",
        "outputId": "56bbe4d2-248f-4d59-e33d-3e2b9afbbbe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 3, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "abc.append(30)\n",
        "print(abc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5EOE012CR2L",
        "outputId": "f6900e36-4bc9-4327-c489-04a1512d7a53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 2, 1, 6, 5, 7, 30]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(abc)\n",
        "print(xyz)\n",
        "abc.extend(xyz)\n",
        "print(abc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDbgOMIxDgdM",
        "outputId": "218cdfe2-1d26-44c9-9ccf-174b24ef981c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 2, 1, 6, 5, 7, 30, 'Stephen', 'Clement', 'Ayden']\n",
            "['Stephen', 'Clement', 'Ayden']\n",
            "[1, 2, 3, 2, 1, 6, 5, 7, 30, 'Stephen', 'Clement', 'Ayden', 'Stephen', 'Clement', 'Ayden']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "abc.remove(5)  # remove based on item\n",
        "print (abc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBoCI9yDDzxN",
        "outputId": "57906c47-305b-4724-8266-07e70571cef9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 2, 1, 6, 7, 30, 'Stephen', 'Clement', 'Ayden', 'Stephen', 'Clement', 'Ayden']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "abc.pop(7)  # remove based on index\n",
        "print(abc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVdstdvmEgFX",
        "outputId": "58db85a3-bc12-40ab-e2e4-abd34c4b9ab6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 2, 1, 6, 7, 'Clement', 'Ayden', 'Stephen', 'Clement', 'Ayden']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del abc[3]\n",
        "print(abc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NiOUOoCE5_q",
        "outputId": "c21d9877-7554-4311-c875-de6161e8814d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 1, 6, 7, 'Clement', 'Ayden', 'Stephen', 'Clement', 'Ayden']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in abc:   #iterate\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uo8OGR5vFtiv",
        "outputId": "40051fde-4b83-4974-fdc6-aacaafeb7ef9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "1\n",
            "6\n",
            "7\n",
            "Clement\n",
            "Ayden\n",
            "Stephen\n",
            "Clement\n",
            "Ayden\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "abc.clear()\n",
        "print(abc)\n",
        "print(len(abc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zRMmXrwIx0b",
        "outputId": "d4282b60-00a2-40bb-81f3-996642ae4fe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tuple**  ordered, not changeable, uses curved brackets, use square brackets to access"
      ],
      "metadata": {
        "id": "F2waU_fBLcZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customer = ('Stephen','Waigi', 'Data and Cloud')\n",
        "print(customer)\n",
        "print(type(customer))\n",
        "print(len(customer))\n",
        "print(customer[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1R_XT-SuMKpL",
        "outputId": "c87daff2-370b-40ee-f696-8a41271ba7b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Stephen', 'Waigi', 'Data and Cloud')\n",
            "<class 'tuple'>\n",
            "3\n",
            "Data and Cloud\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "customerList = list(customer)\n",
        "customerList.append('Data engineering')\n",
        "customer = tuple(customerList)\n",
        "print(customer)\n"
      ],
      "metadata": {
        "id": "vrlaLDuGOJxx",
        "outputId": "027bb775-ecae-4622-8892-6a94f0c94608",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Stephen', 'Waigi', 'Data and Cloud', 'Data engineering')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**set** uses curly brackets, can't allow duplicate, unordered, addition and delition allowed but update not allowed"
      ],
      "metadata": {
        "id": "wUCN9jiCQ4N-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "countries = {'Rwanda','Rwanda','Tanzania','Kenya'}\n",
        "print(countries)\n",
        "print(type(countries))\n",
        "print(len(countries))"
      ],
      "metadata": {
        "id": "LvrDVKgARtxl",
        "outputId": "bf606e85-68e4-4945-97b8-61901ee964e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Rwanda', 'Kenya', 'Tanzania'}\n",
            "<class 'set'>\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "countries.remove('Rwanda')\n",
        "print(countries)"
      ],
      "metadata": {
        "id": "GovuigxjSgar",
        "outputId": "056abc27-fc39-4659-ab7e-bd3840c14069",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Kenya', 'Tanzania'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dictionary**  does not allow duplicate and uses key Value Pair and is ordered and is editable"
      ],
      "metadata": {
        "id": "n21SA3xMUAJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "employee ={\"Name\": \"Stephen\", \"Age\":40, \"City\": \"Nairobi\", \"Country\": \"Kenya\"}\n",
        "print(employee)\n",
        "print(type(employee))\n",
        "print(len(employee))"
      ],
      "metadata": {
        "id": "nS42mJLkUO6F",
        "outputId": "f5a22dc0-8e27-4527-c94d-0ae54a4f8b92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Name': 'Stephen', 'Age': 40, 'City': 'Nairobi', 'Country': 'Kenya'}\n",
            "<class 'dict'>\n",
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(employee['Name'])\n",
        "print(employee['Age'])\n",
        "print(employee['City'])\n",
        "print(employee['Country'])\n",
        "print(employee.keys())\n",
        "print(employee.values())"
      ],
      "metadata": {
        "id": "dyvIWlb6V0t4",
        "outputId": "fea440a5-b9df-4d8b-d79b-81ba3a4bb478",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stephen\n",
            "40\n",
            "Nairobi\n",
            "Kenya\n",
            "dict_keys(['Name', 'Age', 'City', 'Country'])\n",
            "dict_values(['Stephen', 40, 'Nairobi', 'Kenya'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(employee.keys())\n",
        "keys = employee.keys()\n",
        "employee[\"salary\"] = 100000\n",
        "\n",
        "for i in keys:\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "9STjtw8yXNR4",
        "outputId": "82ad7c91-32c3-49b1-bb37-2dec6fa12f9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['Name', 'Age', 'City', 'Country'])\n",
            "Name\n",
            "Age\n",
            "City\n",
            "Country\n",
            "salary\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "imagine that you have a to do list. Take the 5 items from the user to add to the todo list. Ask user which item is finished, then remove it from the list. print the remaining items in the todo list."
      ],
      "metadata": {
        "id": "w4ve7fw1BxTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# File to store the to-do list\n",
        "todo_file = \"todo_list.json\"\n",
        "\n",
        "# Default to-do list\n",
        "default_todo_list = [\n",
        "    \"update stocks\",\n",
        "    \"call account customers\",\n",
        "    \"submit daily standup report\",\n",
        "    \"forward pending deliveries\",\n",
        "    \"compile weekly casuals wages\"\n",
        "]\n",
        "\n",
        "def load_todo_list():\n",
        "    if os.path.exists(todo_file):\n",
        "        with open(todo_file, \"r\") as file:\n",
        "            return json.load(file)\n",
        "    else:\n",
        "        return default_todo_list\n",
        "\n",
        "def save_todo_list(todo_list):\n",
        "    with open(todo_file, \"w\") as file:\n",
        "        json.dump(todo_list, file)\n",
        "\n",
        "# Load the to-do list\n",
        "todo_list = load_todo_list()\n",
        "\n",
        "if todo_list:\n",
        "    # Display the to-do list\n",
        "    print(\"To-do list:\")\n",
        "    for index, item in enumerate(todo_list, start=1):\n",
        "        print(f\"{index}. {item}\")\n",
        "\n",
        "    # Ask user which item is finished\n",
        "    finished_item = int(input(\"\\nWhich item is finished (enter the number): \"))\n",
        "\n",
        "    # Remove the finished item from the list\n",
        "    if 1 <= finished_item <= len(todo_list):\n",
        "        finished_task = todo_list.pop(finished_item - 1)\n",
        "        print(f\"\\nRemoved finished item: {finished_task}\")\n",
        "        save_todo_list(todo_list)\n",
        "    else:\n",
        "        print(\"\\nInvalid item number.\")\n",
        "else:\n",
        "    print(\"\\nAll items done, please create a new to-do list.\")\n",
        "\n",
        "# Display the remaining to-do list\n",
        "if todo_list:\n",
        "    print(\"\\nRemaining to-do list:\")\n",
        "    for index, item in enumerate(todo_list, start=1):\n",
        "        print(f\"{index}. {item}\")\n",
        "else:\n",
        "    print(\"\\nAll items done, please create a new to-do list.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k69Nhh-WFerW",
        "outputId": "188cb756-cafa-4d58-cbe9-5e2cfb55c5b5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "All items done, please create a new to-do list.\n",
            "\n",
            "All items done, please create a new to-do list.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# File to store the to-do list\n",
        "todo_file = \"todo_list.json\"\n",
        "\n",
        "def load_todo_list():\n",
        "    if os.path.exists(todo_file):\n",
        "        with open(todo_file, \"r\") as file:\n",
        "            return json.load(file)\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "def save_todo_list(todo_list):\n",
        "    with open(todo_file, \"w\") as file:\n",
        "        json.dump(todo_list, file)\n",
        "\n",
        "def add_items_to_todo_list(todo_list):\n",
        "    print(\"Enter your to-do items (type 'done' when finished):\")\n",
        "    while len(todo_list) < 5:\n",
        "        item = input(\"Add item: \")\n",
        "        if item.lower() == 'done':\n",
        "            break\n",
        "        if len(todo_list) < 5:\n",
        "            todo_list.append(item)\n",
        "            print(f\"Item added. You can add {5 - len(todo_list)} more items.\")\n",
        "        else:\n",
        "            print(\"To-do list is full. You cannot add more than 5 items.\")\n",
        "    save_todo_list(todo_list)\n",
        "\n",
        "def main():\n",
        "    # Load the to-do list\n",
        "    todo_list = load_todo_list()\n",
        "\n",
        "    # If the list is empty, prompt the user to add items\n",
        "    if not todo_list:\n",
        "        add_items_to_todo_list(todo_list)\n",
        "\n",
        "    while todo_list:\n",
        "        # Display the to-do list\n",
        "        print(\"\\nTo-do list:\")\n",
        "        for index, item in enumerate(todo_list, start=1):\n",
        "            print(f\"{index}. {item}\")\n",
        "\n",
        "        # Ask user which item is finished\n",
        "        try:\n",
        "            finished_item = int(input(\"\\nWhich item is finished (enter the number): \"))\n",
        "            if 1 <= finished_item <= len(todo_list):\n",
        "                finished_task = todo_list.pop(finished_item - 1)\n",
        "                print(f\"\\nRemoved finished item: {finished_task}\")\n",
        "                save_todo_list(todo_list)\n",
        "            else:\n",
        "                print(\"\\nInvalid item number.\")\n",
        "        except ValueError:\n",
        "            print(\"\\nPlease enter a valid number.\")\n",
        "\n",
        "        # If all items are done, notify the user and ask for next action\n",
        "        if not todo_list:\n",
        "            print(\"\\nAll items done.\")\n",
        "            next_action = input(\"Do you want to create a new to-do list? (yes/no): \").strip().lower()\n",
        "            if next_action == 'yes':\n",
        "                add_items_to_todo_list(todo_list)\n",
        "            else:\n",
        "                print(\"Exiting the to-do list manager. Goodbye!\")\n",
        "                break\n",
        "\n",
        "# Final message if all items are done\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsLaOXYLJtU5",
        "outputId": "43ded5c9-e1ba-497a-d97a-38c310cda4c6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your to-do items (type 'done' when finished):\n",
            "Add item: create database\n",
            "Item added. You can add 4 more items.\n",
            "Add item: create table schema\n",
            "Item added. You can add 3 more items.\n",
            "Add item: import data into table\n",
            "Item added. You can add 2 more items.\n",
            "Add item: create indexes\n",
            "Item added. You can add 1 more items.\n",
            "Add item: backup database\n",
            "Item added. You can add 0 more items.\n",
            "\n",
            "To-do list:\n",
            "1. create database\n",
            "2. create table schema\n",
            "3. import data into table\n",
            "4. create indexes\n",
            "5. backup database\n",
            "\n",
            "Which item is finished (enter the number): 1\n",
            "\n",
            "Removed finished item: create database\n",
            "\n",
            "To-do list:\n",
            "1. create table schema\n",
            "2. import data into table\n",
            "3. create indexes\n",
            "4. backup database\n",
            "\n",
            "Which item is finished (enter the number): 4\n",
            "\n",
            "Removed finished item: backup database\n",
            "\n",
            "To-do list:\n",
            "1. create table schema\n",
            "2. import data into table\n",
            "3. create indexes\n",
            "\n",
            "Which item is finished (enter the number): 2\n",
            "\n",
            "Removed finished item: import data into table\n",
            "\n",
            "To-do list:\n",
            "1. create table schema\n",
            "2. create indexes\n",
            "\n",
            "Which item is finished (enter the number): 3\n",
            "\n",
            "Invalid item number.\n",
            "\n",
            "To-do list:\n",
            "1. create table schema\n",
            "2. create indexes\n",
            "\n",
            "Which item is finished (enter the number): 2\n",
            "\n",
            "Removed finished item: create indexes\n",
            "\n",
            "To-do list:\n",
            "1. create table schema\n",
            "\n",
            "Which item is finished (enter the number): 1\n",
            "\n",
            "Removed finished item: create table schema\n",
            "\n",
            "All items done.\n",
            "Do you want to create a new to-do list? (yes/no): no\n",
            "Exiting the to-do list manager. Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**user defined functions**"
      ],
      "metadata": {
        "id": "Oriy8T0UPJkM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display(message):\n",
        "  print(message)\n",
        "\n",
        "x=20\n",
        "y=35\n",
        "z=x+y\n",
        "display(z)\n",
        "display(\"Hello all\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfG8Hc5mPQZ_",
        "outputId": "182fd496-5f18-469d-f91b-ee789a48ce0d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "55\n",
            "Hello all\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add (x,y):\n",
        "  return x+y\n",
        "\n",
        "def substract(x,y):\n",
        "  return x-y\n",
        "\n",
        "def multiply(x,y):\n",
        "  return x*y\n",
        "\n",
        "def divide(x,y):\n",
        "  return x/y\n",
        "\n",
        "print(add(10,20))\n",
        "print(substract(10,20))\n",
        "print(multiply(10,20))\n",
        "print(divide(10,20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZV4JSPkQuzq",
        "outputId": "74a24d44-c763-47d8-bb37-e3bf9adca1d0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30\n",
            "-10\n",
            "200\n",
            "0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Module /library"
      ],
      "metadata": {
        "id": "oFIi7D4XSU6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "print(dir(math))\n",
        "def calculate_average_and_result(marks):\n",
        "    total_marks = sum(marks)\n",
        "    total_subjects = len(marks)\n",
        "    average_percentage = (total_marks / (total_subjects * 100)) * 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dleu-eohS9aK",
        "outputId": "aa9ec985-a162-474f-fd2a-8bd72ca412a0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['__doc__', '__loader__', '__name__', '__package__', '__spec__', 'acos', 'acosh', 'asin', 'asinh', 'atan', 'atan2', 'atanh', 'ceil', 'comb', 'copysign', 'cos', 'cosh', 'degrees', 'dist', 'e', 'erf', 'erfc', 'exp', 'expm1', 'fabs', 'factorial', 'floor', 'fmod', 'frexp', 'fsum', 'gamma', 'gcd', 'hypot', 'inf', 'isclose', 'isfinite', 'isinf', 'isnan', 'isqrt', 'lcm', 'ldexp', 'lgamma', 'log', 'log10', 'log1p', 'log2', 'modf', 'nan', 'nextafter', 'perm', 'pi', 'pow', 'prod', 'radians', 'remainder', 'sin', 'sinh', 'sqrt', 'tan', 'tanh', 'tau', 'trunc', 'ulp']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "class and objects"
      ],
      "metadata": {
        "id": "6WWPqbU01OWd"
      }
    },
    {
      "source": [
        "class Vehicle:\n",
        "  name = ''\n",
        "  color = ''\n",
        "\n",
        "# Create the instance outside the class definition\n",
        "car = Vehicle()\n",
        "car.name = 'BMW'\n",
        "car.color = 'red'\n",
        "print(car.name)\n",
        "print(car.color)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHhab-q51-9K",
        "outputId": "eab567bd-83cc-4458-c5ad-e880db83a458"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BMW\n",
            "red\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Car:\n",
        "  def __init__(self, name, color):\n",
        "    self.name = name\n",
        "    self.color = color"
      ],
      "metadata": {
        "id": "v18ZIvnc2iHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Employee:\n",
        "    def __init__(self, name, age, salary):\n",
        "        self.name = name\n",
        "        self.age = age\n",
        "        self.salary = salary\n",
        "        print(\"Inside the Employee init function:\")\n",
        "\n",
        "# Create instances of Employee\n",
        "c1 = Employee('Stephen', 40, 50000)\n",
        "c2 = Employee('Jacinta', 36, 45000)\n",
        "\n",
        "# Print the attributes for Stephen\n",
        "print(c1.name)\n",
        "print(c1.age)\n",
        "print(c1.salary)\n",
        "\n",
        "# Print the attributes for Jacinta\n",
        "print(c2.name)\n",
        "print(c2.age)\n",
        "print(c2.salary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmnMWUwe2xyM",
        "outputId": "d2f6abe2-be95-4c12-d6ec-0487c8490266"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inside the Employee init function:\n",
            "Inside the Employee init function:\n",
            "Stephen\n",
            "40\n",
            "50000\n",
            "Jacinta\n",
            "36\n",
            "45000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the attributes for Jacinta\n",
        "print(c2.name)\n",
        "print(c2.age)\n",
        "print(c2.salary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAVN_KXq6KDt",
        "outputId": "ebe8807e-168f-4b85-a94c-a2598d09c5a5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jacinta\n",
            "36\n",
            "45000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "exception handling"
      ],
      "metadata": {
        "id": "vk0-EnT87kYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CF-dQmR6OHeJ",
        "outputId": "1e9c15c8-dfc1-45cb-cfb9-429c8629828b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JgivKegR3SB",
        "outputId": "eb891db2-2d3e-464d-a0f6-a182b983e7c9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=528e428682a2fd9ef4e775c80692164843bda07bbb60b72afa317c09b4612a77\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "pyspark"
      ],
      "metadata": {
        "id": "flISvtVBdWrd"
      }
    },
    {
      "source": [
        "# Import the necessary PySpark module\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"MySparkApp\").getOrCreate()\n",
        "\n",
        "# Sample data\n",
        "data = [(\"Alice\", 25), (\"Bob\", 30)]\n",
        "# Define the schema\n",
        "schema = [\"name\", \"age\"]\n",
        "\n",
        "# Now you can use the spark object to create a DataFrame\n",
        "df = spark.createDataFrame(data, schema)\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRpCCkbbbfzc",
        "outputId": "ee42851d-8420-4414-e5cf-8c02e2ff6bed"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+\n",
            "| name|age|\n",
            "+-----+---+\n",
            "|Alice| 25|\n",
            "|  Bob| 30|\n",
            "+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField  # Import both StructType and StructField\n",
        "\n",
        "help(StructType)\n",
        "help(StructField)  # Now StructField is accessible\n",
        "\n",
        "data = [(1, 'Stephen'), (2, 'Ayden')]\n",
        "schema = [\"id\", \"name\"]\n",
        "df = spark.createDataFrame(data, schema)\n",
        "df.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_k-E02A2e4To",
        "outputId": "367d5201-8662-4fc3-a5d9-5639fb62f922"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class StructType in module pyspark.sql.types:\n",
            "\n",
            "class StructType(DataType)\n",
            " |  StructType(fields: Optional[List[pyspark.sql.types.StructField]] = None)\n",
            " |  \n",
            " |  Struct type, consisting of a list of :class:`StructField`.\n",
            " |  \n",
            " |  This is the data type representing a :class:`Row`.\n",
            " |  \n",
            " |  Iterating a :class:`StructType` will iterate over its :class:`StructField`\\s.\n",
            " |  A contained :class:`StructField` can be accessed by its name or position.\n",
            " |  \n",
            " |  Examples\n",
            " |  --------\n",
            " |  >>> from pyspark.sql.types import *\n",
            " |  >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n",
            " |  >>> struct1[\"f1\"]\n",
            " |  StructField('f1', StringType(), True)\n",
            " |  >>> struct1[0]\n",
            " |  StructField('f1', StringType(), True)\n",
            " |  \n",
            " |  >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n",
            " |  >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n",
            " |  >>> struct1 == struct2\n",
            " |  True\n",
            " |  >>> struct1 = StructType([StructField(\"f1\", CharType(10), True)])\n",
            " |  >>> struct2 = StructType([StructField(\"f1\", CharType(10), True)])\n",
            " |  >>> struct1 == struct2\n",
            " |  True\n",
            " |  >>> struct1 = StructType([StructField(\"f1\", VarcharType(10), True)])\n",
            " |  >>> struct2 = StructType([StructField(\"f1\", VarcharType(10), True)])\n",
            " |  >>> struct1 == struct2\n",
            " |  True\n",
            " |  >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n",
            " |  >>> struct2 = StructType([StructField(\"f1\", StringType(), True),\n",
            " |  ...     StructField(\"f2\", IntegerType(), False)])\n",
            " |  >>> struct1 == struct2\n",
            " |  False\n",
            " |  \n",
            " |  The below example demonstrates how to create a DataFrame based on a struct created\n",
            " |  using class:`StructType` and class:`StructField`:\n",
            " |  \n",
            " |  >>> data = [(\"Alice\", [\"Java\", \"Scala\"]), (\"Bob\", [\"Python\", \"Scala\"])]\n",
            " |  >>> schema = StructType([\n",
            " |  ...     StructField(\"name\", StringType()),\n",
            " |  ...     StructField(\"languagesSkills\", ArrayType(StringType())),\n",
            " |  ... ])\n",
            " |  >>> df = spark.createDataFrame(data=data, schema=schema)\n",
            " |  >>> df.printSchema()\n",
            " |  root\n",
            " |   |-- name: string (nullable = true)\n",
            " |   |-- languagesSkills: array (nullable = true)\n",
            " |   |    |-- element: string (containsNull = true)\n",
            " |  >>> df.show()\n",
            " |  +-----+---------------+\n",
            " |  | name|languagesSkills|\n",
            " |  +-----+---------------+\n",
            " |  |Alice|  [Java, Scala]|\n",
            " |  |  Bob|[Python, Scala]|\n",
            " |  +-----+---------------+\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      StructType\n",
            " |      DataType\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __getitem__(self, key: Union[str, int]) -> pyspark.sql.types.StructField\n",
            " |      Access fields by name or slice.\n",
            " |  \n",
            " |  __init__(self, fields: Optional[List[pyspark.sql.types.StructField]] = None)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  __iter__(self) -> Iterator[pyspark.sql.types.StructField]\n",
            " |      Iterate the fields\n",
            " |  \n",
            " |  __len__(self) -> int\n",
            " |      Return the number of fields.\n",
            " |  \n",
            " |  __repr__(self) -> str\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  add(self, field: Union[str, pyspark.sql.types.StructField], data_type: Union[str, pyspark.sql.types.DataType, NoneType] = None, nullable: bool = True, metadata: Optional[Dict[str, Any]] = None) -> 'StructType'\n",
            " |      Construct a :class:`StructType` by adding new elements to it, to define the schema.\n",
            " |      The method accepts either:\n",
            " |      \n",
            " |          a) A single parameter which is a :class:`StructField` object.\n",
            " |          b) Between 2 and 4 parameters as (name, data_type, nullable (optional),\n",
            " |             metadata(optional). The data_type parameter may be either a String or a\n",
            " |             :class:`DataType` object.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      field : str or :class:`StructField`\n",
            " |          Either the name of the field or a :class:`StructField` object\n",
            " |      data_type : :class:`DataType`, optional\n",
            " |          If present, the DataType of the :class:`StructField` to create\n",
            " |      nullable : bool, optional\n",
            " |          Whether the field to add should be nullable (default True)\n",
            " |      metadata : dict, optional\n",
            " |          Any additional metadata (default None)\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`StructType`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from pyspark.sql.types import IntegerType, StringType, StructField, StructType\n",
            " |      >>> struct1 = StructType().add(\"f1\", StringType(), True).add(\"f2\", StringType(), True, None)\n",
            " |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True),\n",
            " |      ...     StructField(\"f2\", StringType(), True, None)])\n",
            " |      >>> struct1 == struct2\n",
            " |      True\n",
            " |      >>> struct1 = StructType().add(StructField(\"f1\", StringType(), True))\n",
            " |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n",
            " |      >>> struct1 == struct2\n",
            " |      True\n",
            " |      >>> struct1 = StructType().add(\"f1\", \"string\", True)\n",
            " |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n",
            " |      >>> struct1 == struct2\n",
            " |      True\n",
            " |  \n",
            " |  fieldNames(self) -> List[str]\n",
            " |      Returns all field names in a list.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from pyspark.sql.types import StringType, StructField, StructType\n",
            " |      >>> struct = StructType([StructField(\"f1\", StringType(), True)])\n",
            " |      >>> struct.fieldNames()\n",
            " |      ['f1']\n",
            " |  \n",
            " |  fromInternal(self, obj: Tuple) -> 'Row'\n",
            " |      Converts an internal SQL object into a native Python object.\n",
            " |  \n",
            " |  jsonValue(self) -> Dict[str, Any]\n",
            " |  \n",
            " |  needConversion(self) -> bool\n",
            " |      Does this type needs conversion between Python object and internal SQL object.\n",
            " |      \n",
            " |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
            " |  \n",
            " |  simpleString(self) -> str\n",
            " |  \n",
            " |  toInternal(self, obj: Tuple) -> Tuple\n",
            " |      Converts a Python object into an internal SQL object.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods defined here:\n",
            " |  \n",
            " |  fromJson(json: Dict[str, Any]) -> 'StructType' from builtins.type\n",
            " |      Constructs :class:`StructType` from a schema defined in JSON format.\n",
            " |      \n",
            " |      Below is a JSON schema it must adhere to::\n",
            " |      \n",
            " |          {\n",
            " |            \"title\":\"StructType\",\n",
            " |            \"description\":\"Schema of StructType in json format\",\n",
            " |            \"type\":\"object\",\n",
            " |            \"properties\":{\n",
            " |               \"fields\":{\n",
            " |                  \"description\":\"Array of struct fields\",\n",
            " |                  \"type\":\"array\",\n",
            " |                  \"items\":{\n",
            " |                      \"type\":\"object\",\n",
            " |                      \"properties\":{\n",
            " |                         \"name\":{\n",
            " |                            \"description\":\"Name of the field\",\n",
            " |                            \"type\":\"string\"\n",
            " |                         },\n",
            " |                         \"type\":{\n",
            " |                            \"description\": \"Type of the field. Can either be\n",
            " |                                            another nested StructType or primitive type\",\n",
            " |                            \"type\":\"object/string\"\n",
            " |                         },\n",
            " |                         \"nullable\":{\n",
            " |                            \"description\":\"If nulls are allowed\",\n",
            " |                            \"type\":\"boolean\"\n",
            " |                         },\n",
            " |                         \"metadata\":{\n",
            " |                            \"description\":\"Additional metadata to supply\",\n",
            " |                            \"type\":\"object\"\n",
            " |                         },\n",
            " |                         \"required\":[\n",
            " |                            \"name\",\n",
            " |                            \"type\",\n",
            " |                            \"nullable\",\n",
            " |                            \"metadata\"\n",
            " |                         ]\n",
            " |                      }\n",
            " |                 }\n",
            " |              }\n",
            " |           }\n",
            " |         }\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      json : dict or a dict-like object e.g. JSON object\n",
            " |          This \"dict\" must have \"fields\" key that returns an array of fields\n",
            " |          each of which must have specific keys (name, type, nullable, metadata).\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`StructType`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> json_str = '''\n",
            " |      ...  {\n",
            " |      ...      \"fields\": [\n",
            " |      ...          {\n",
            " |      ...              \"metadata\": {},\n",
            " |      ...              \"name\": \"Person\",\n",
            " |      ...              \"nullable\": true,\n",
            " |      ...              \"type\": {\n",
            " |      ...                  \"fields\": [\n",
            " |      ...                      {\n",
            " |      ...                          \"metadata\": {},\n",
            " |      ...                          \"name\": \"name\",\n",
            " |      ...                          \"nullable\": false,\n",
            " |      ...                          \"type\": \"string\"\n",
            " |      ...                      },\n",
            " |      ...                      {\n",
            " |      ...                          \"metadata\": {},\n",
            " |      ...                          \"name\": \"surname\",\n",
            " |      ...                          \"nullable\": false,\n",
            " |      ...                          \"type\": \"string\"\n",
            " |      ...                      }\n",
            " |      ...                  ],\n",
            " |      ...                  \"type\": \"struct\"\n",
            " |      ...              }\n",
            " |      ...          }\n",
            " |      ...      ],\n",
            " |      ...      \"type\": \"struct\"\n",
            " |      ...  }\n",
            " |      ...  '''\n",
            " |      >>> import json\n",
            " |      >>> scheme = StructType.fromJson(json.loads(json_str))\n",
            " |      >>> scheme.simpleString()\n",
            " |      'struct<Person:struct<name:string,surname:string>>'\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from DataType:\n",
            " |  \n",
            " |  __eq__(self, other: Any) -> bool\n",
            " |      Return self==value.\n",
            " |  \n",
            " |  __hash__(self) -> int\n",
            " |      Return hash(self).\n",
            " |  \n",
            " |  __ne__(self, other: Any) -> bool\n",
            " |      Return self!=value.\n",
            " |  \n",
            " |  json(self) -> str\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from DataType:\n",
            " |  \n",
            " |  typeName() -> str from builtins.type\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from DataType:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n",
            "Help on class StructField in module pyspark.sql.types:\n",
            "\n",
            "class StructField(DataType)\n",
            " |  StructField(name: str, dataType: pyspark.sql.types.DataType, nullable: bool = True, metadata: Optional[Dict[str, Any]] = None)\n",
            " |  \n",
            " |  A field in :class:`StructType`.\n",
            " |  \n",
            " |  Parameters\n",
            " |  ----------\n",
            " |  name : str\n",
            " |      name of the field.\n",
            " |  dataType : :class:`DataType`\n",
            " |      :class:`DataType` of the field.\n",
            " |  nullable : bool, optional\n",
            " |      whether the field can be null (None) or not.\n",
            " |  metadata : dict, optional\n",
            " |      a dict from string to simple type that can be toInternald to JSON automatically\n",
            " |  \n",
            " |  Examples\n",
            " |  --------\n",
            " |  >>> from pyspark.sql.types import StringType, StructField\n",
            " |  >>> (StructField(\"f1\", StringType(), True)\n",
            " |  ...      == StructField(\"f1\", StringType(), True))\n",
            " |  True\n",
            " |  >>> (StructField(\"f1\", StringType(), True)\n",
            " |  ...      == StructField(\"f2\", StringType(), True))\n",
            " |  False\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      StructField\n",
            " |      DataType\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, name: str, dataType: pyspark.sql.types.DataType, nullable: bool = True, metadata: Optional[Dict[str, Any]] = None)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  __repr__(self) -> str\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  fromInternal(self, obj: ~T) -> ~T\n",
            " |      Converts an internal SQL object into a native Python object.\n",
            " |  \n",
            " |  jsonValue(self) -> Dict[str, Any]\n",
            " |  \n",
            " |  needConversion(self) -> bool\n",
            " |      Does this type needs conversion between Python object and internal SQL object.\n",
            " |      \n",
            " |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
            " |  \n",
            " |  simpleString(self) -> str\n",
            " |  \n",
            " |  toInternal(self, obj: ~T) -> ~T\n",
            " |      Converts a Python object into an internal SQL object.\n",
            " |  \n",
            " |  typeName(self) -> str\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods defined here:\n",
            " |  \n",
            " |  fromJson(json: Dict[str, Any]) -> 'StructField' from builtins.type\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from DataType:\n",
            " |  \n",
            " |  __eq__(self, other: Any) -> bool\n",
            " |      Return self==value.\n",
            " |  \n",
            " |  __hash__(self) -> int\n",
            " |      Return hash(self).\n",
            " |  \n",
            " |  __ne__(self, other: Any) -> bool\n",
            " |      Return self!=value.\n",
            " |  \n",
            " |  json(self) -> str\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from DataType:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n",
            "+---+-------+\n",
            "| id|   name|\n",
            "+---+-------+\n",
            "|  1|Stephen|\n",
            "|  2|  Ayden|\n",
            "+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "data = [(1, 'Stephen'), (2, 'Ayden')]\n",
        "schema = StructType([StructField(name=\"id\", dataType=IntegerType(), nullable=True),\n",
        "    StructField(name=\"name\", dataType=StringType(), nullable=True)\n",
        "])\n",
        "df = spark.createDataFrame(data, schema)\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1z72t5Rf0Ot",
        "outputId": "8492a6e1-a789-46ac-9410-86d8151636d4"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "| id|   name|\n",
            "+---+-------+\n",
            "|  1|Stephen|\n",
            "|  2|  Ayden|\n",
            "+---+-------+\n",
            "\n",
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [{'id': 1, \"name\": 'Stephen'},\n",
        "        {'id': 2, \"name\": 'Ayden'}]\n",
        "\n",
        "df = spark.createDataFrame(data)\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRYv0lDchSg6",
        "outputId": "9f7a8947-e6cc-4598-aabb-e8fef6289e0e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "| id|   name|\n",
            "+---+-------+\n",
            "|  1|Stephen|\n",
            "|  2|  Ayden|\n",
            "+---+-------+\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(spark.read)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYLwD_9wkH1s",
        "outputId": "5339ef0c-a8a3-4c3a-bafe-acb91c402d34"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on DataFrameReader in module pyspark.sql.readwriter object:\n",
            "\n",
            "class DataFrameReader(OptionUtils)\n",
            " |  DataFrameReader(spark: 'SparkSession')\n",
            " |  \n",
            " |  Interface used to load a :class:`DataFrame` from external storage systems\n",
            " |  (e.g. file systems, key-value stores, etc). Use :attr:`SparkSession.read`\n",
            " |  to access this.\n",
            " |  \n",
            " |  .. versionadded:: 1.4.0\n",
            " |  \n",
            " |  .. versionchanged:: 3.4.0\n",
            " |      Supports Spark Connect.\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      DataFrameReader\n",
            " |      OptionUtils\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, spark: 'SparkSession')\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  csv(self, path: Union[str, List[str]], schema: Union[pyspark.sql.types.StructType, str, NoneType] = None, sep: Optional[str] = None, encoding: Optional[str] = None, quote: Optional[str] = None, escape: Optional[str] = None, comment: Optional[str] = None, header: Union[bool, str, NoneType] = None, inferSchema: Union[bool, str, NoneType] = None, ignoreLeadingWhiteSpace: Union[bool, str, NoneType] = None, ignoreTrailingWhiteSpace: Union[bool, str, NoneType] = None, nullValue: Optional[str] = None, nanValue: Optional[str] = None, positiveInf: Optional[str] = None, negativeInf: Optional[str] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, maxColumns: Union[str, int, NoneType] = None, maxCharsPerColumn: Union[str, int, NoneType] = None, maxMalformedLogPerPartition: Union[str, int, NoneType] = None, mode: Optional[str] = None, columnNameOfCorruptRecord: Optional[str] = None, multiLine: Union[bool, str, NoneType] = None, charToEscapeQuoteEscaping: Optional[str] = None, samplingRatio: Union[str, float, NoneType] = None, enforceSchema: Union[bool, str, NoneType] = None, emptyValue: Optional[str] = None, locale: Optional[str] = None, lineSep: Optional[str] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None, unescapedQuoteHandling: Optional[str] = None) -> 'DataFrame'\n",
            " |      Loads a CSV file and returns the result as a  :class:`DataFrame`.\n",
            " |      \n",
            " |      This function will go through the input once to determine the input schema if\n",
            " |      ``inferSchema`` is enabled. To avoid going through the entire data once, disable\n",
            " |      ``inferSchema`` option or specify the schema explicitly using ``schema``.\n",
            " |      \n",
            " |      .. versionadded:: 2.0.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      path : str or list\n",
            " |          string, or list of strings, for input path(s),\n",
            " |          or RDD of Strings storing CSV rows.\n",
            " |      schema : :class:`pyspark.sql.types.StructType` or str, optional\n",
            " |          an optional :class:`pyspark.sql.types.StructType` for the input schema\n",
            " |          or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
            " |      \n",
            " |      Other Parameters\n",
            " |      ----------------\n",
            " |      Extra options\n",
            " |          For the extra options, refer to\n",
            " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_\n",
            " |          for the version you use.\n",
            " |      \n",
            " |          .. # noqa\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      Write a DataFrame into a CSV file and read it back.\n",
            " |      \n",
            " |      >>> import tempfile\n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     # Write a DataFrame into a CSV file\n",
            " |      ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n",
            " |      ...     df.write.mode(\"overwrite\").format(\"csv\").save(d)\n",
            " |      ...\n",
            " |      ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.\n",
            " |      ...     spark.read.csv(d, schema=df.schema, nullValue=\"Hyukjin Kwon\").show()\n",
            " |      +---+----+\n",
            " |      |age|name|\n",
            " |      +---+----+\n",
            " |      |100|NULL|\n",
            " |      +---+----+\n",
            " |  \n",
            " |  format(self, source: str) -> 'DataFrameReader'\n",
            " |      Specifies the input data source format.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      source : str\n",
            " |          string, name of the data source, e.g. 'json', 'parquet'.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> spark.read.format('json')\n",
            " |      <...readwriter.DataFrameReader object ...>\n",
            " |      \n",
            " |      Write a DataFrame into a JSON file and read it back.\n",
            " |      \n",
            " |      >>> import tempfile\n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     # Write a DataFrame into a JSON file\n",
            " |      ...     spark.createDataFrame(\n",
            " |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
            " |      ...     ).write.mode(\"overwrite\").format(\"json\").save(d)\n",
            " |      ...\n",
            " |      ...     # Read the JSON file as a DataFrame.\n",
            " |      ...     spark.read.format('json').load(d).show()\n",
            " |      +---+------------+\n",
            " |      |age|        name|\n",
            " |      +---+------------+\n",
            " |      |100|Hyukjin Kwon|\n",
            " |      +---+------------+\n",
            " |  \n",
            " |  jdbc(self, url: str, table: str, column: Optional[str] = None, lowerBound: Union[str, int, NoneType] = None, upperBound: Union[str, int, NoneType] = None, numPartitions: Optional[int] = None, predicates: Optional[List[str]] = None, properties: Optional[Dict[str, str]] = None) -> 'DataFrame'\n",
            " |      Construct a :class:`DataFrame` representing the database table named ``table``\n",
            " |      accessible via JDBC URL ``url`` and connection ``properties``.\n",
            " |      \n",
            " |      Partitions of the table will be retrieved in parallel if either ``column`` or\n",
            " |      ``predicates`` is specified. ``lowerBound``, ``upperBound`` and ``numPartitions``\n",
            " |      is needed when ``column`` is specified.\n",
            " |      \n",
            " |      If both ``column`` and ``predicates`` are specified, ``column`` will be used.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      table : str\n",
            " |          the name of the table\n",
            " |      column : str, optional\n",
            " |          alias of ``partitionColumn`` option. Refer to ``partitionColumn`` in\n",
            " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#data-source-option>`_\n",
            " |          for the version you use.\n",
            " |      predicates : list, optional\n",
            " |          a list of expressions suitable for inclusion in WHERE clauses;\n",
            " |          each one defines one partition of the :class:`DataFrame`\n",
            " |      properties : dict, optional\n",
            " |          a dictionary of JDBC database connection arguments. Normally at\n",
            " |          least properties \"user\" and \"password\" with their corresponding values.\n",
            " |          For example { 'user' : 'SYSTEM', 'password' : 'mypassword' }\n",
            " |      \n",
            " |      Other Parameters\n",
            " |      ----------------\n",
            " |      Extra options\n",
            " |          For the extra options, refer to\n",
            " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#data-source-option>`_\n",
            " |          for the version you use.\n",
            " |      \n",
            " |          .. # noqa\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      Don't create too many partitions in parallel on a large cluster;\n",
            " |      otherwise Spark might crash your external database systems.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |  \n",
            " |  json(self, path: Union[str, List[str], pyspark.rdd.RDD[str]], schema: Union[pyspark.sql.types.StructType, str, NoneType] = None, primitivesAsString: Union[bool, str, NoneType] = None, prefersDecimal: Union[bool, str, NoneType] = None, allowComments: Union[bool, str, NoneType] = None, allowUnquotedFieldNames: Union[bool, str, NoneType] = None, allowSingleQuotes: Union[bool, str, NoneType] = None, allowNumericLeadingZero: Union[bool, str, NoneType] = None, allowBackslashEscapingAnyCharacter: Union[bool, str, NoneType] = None, mode: Optional[str] = None, columnNameOfCorruptRecord: Optional[str] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, multiLine: Union[bool, str, NoneType] = None, allowUnquotedControlChars: Union[bool, str, NoneType] = None, lineSep: Optional[str] = None, samplingRatio: Union[str, float, NoneType] = None, dropFieldIfAllNull: Union[bool, str, NoneType] = None, encoding: Optional[str] = None, locale: Optional[str] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None, allowNonNumericNumbers: Union[bool, str, NoneType] = None) -> 'DataFrame'\n",
            " |      Loads JSON files and returns the results as a :class:`DataFrame`.\n",
            " |      \n",
            " |      `JSON Lines <http://jsonlines.org/>`_ (newline-delimited JSON) is supported by default.\n",
            " |      For JSON (one record per file), set the ``multiLine`` parameter to ``true``.\n",
            " |      \n",
            " |      If the ``schema`` parameter is not specified, this function goes\n",
            " |      through the input once to determine the input schema.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      path : str, list or :class:`RDD`\n",
            " |          string represents path to the JSON dataset, or a list of paths,\n",
            " |          or RDD of Strings storing JSON objects.\n",
            " |      schema : :class:`pyspark.sql.types.StructType` or str, optional\n",
            " |          an optional :class:`pyspark.sql.types.StructType` for the input schema or\n",
            " |          a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
            " |      \n",
            " |      Other Parameters\n",
            " |      ----------------\n",
            " |      Extra options\n",
            " |          For the extra options, refer to\n",
            " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option>`_\n",
            " |          for the version you use.\n",
            " |      \n",
            " |          .. # noqa\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      Write a DataFrame into a JSON file and read it back.\n",
            " |      \n",
            " |      >>> import tempfile\n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     # Write a DataFrame into a JSON file\n",
            " |      ...     spark.createDataFrame(\n",
            " |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
            " |      ...     ).write.mode(\"overwrite\").format(\"json\").save(d)\n",
            " |      ...\n",
            " |      ...     # Read the JSON file as a DataFrame.\n",
            " |      ...     spark.read.json(d).show()\n",
            " |      +---+------------+\n",
            " |      |age|        name|\n",
            " |      +---+------------+\n",
            " |      |100|Hyukjin Kwon|\n",
            " |      +---+------------+\n",
            " |  \n",
            " |  load(self, path: Union[str, List[str], NoneType] = None, format: Optional[str] = None, schema: Union[pyspark.sql.types.StructType, str, NoneType] = None, **options: 'OptionalPrimitiveType') -> 'DataFrame'\n",
            " |      Loads data from a data source and returns it as a :class:`DataFrame`.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      path : str or list, optional\n",
            " |          optional string or a list of string for file-system backed data sources.\n",
            " |      format : str, optional\n",
            " |          optional string for format of the data source. Default to 'parquet'.\n",
            " |      schema : :class:`pyspark.sql.types.StructType` or str, optional\n",
            " |          optional :class:`pyspark.sql.types.StructType` for the input schema\n",
            " |          or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
            " |      **options : dict\n",
            " |          all other string options\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      Load a CSV file with format, schema and options specified.\n",
            " |      \n",
            " |      >>> import tempfile\n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     # Write a DataFrame into a CSV file with a header\n",
            " |      ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n",
            " |      ...     df.write.option(\"header\", True).mode(\"overwrite\").format(\"csv\").save(d)\n",
            " |      ...\n",
            " |      ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon',\n",
            " |      ...     # and 'header' option set to `True`.\n",
            " |      ...     df = spark.read.load(\n",
            " |      ...         d, schema=df.schema, format=\"csv\", nullValue=\"Hyukjin Kwon\", header=True)\n",
            " |      ...     df.printSchema()\n",
            " |      ...     df.show()\n",
            " |      root\n",
            " |       |-- age: long (nullable = true)\n",
            " |       |-- name: string (nullable = true)\n",
            " |      +---+----+\n",
            " |      |age|name|\n",
            " |      +---+----+\n",
            " |      |100|NULL|\n",
            " |      +---+----+\n",
            " |  \n",
            " |  option(self, key: str, value: 'OptionalPrimitiveType') -> 'DataFrameReader'\n",
            " |      Adds an input option for the underlying data source.\n",
            " |      \n",
            " |      .. versionadded:: 1.5.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      key : str\n",
            " |          The key for the option to set.\n",
            " |      value\n",
            " |          The value for the option to set.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> spark.read.option(\"key\", \"value\")\n",
            " |      <...readwriter.DataFrameReader object ...>\n",
            " |      \n",
            " |      Specify the option 'nullValue' with reading a CSV file.\n",
            " |      \n",
            " |      >>> import tempfile\n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     # Write a DataFrame into a CSV file\n",
            " |      ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n",
            " |      ...     df.write.mode(\"overwrite\").format(\"csv\").save(d)\n",
            " |      ...\n",
            " |      ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.\n",
            " |      ...     spark.read.schema(df.schema).option(\n",
            " |      ...         \"nullValue\", \"Hyukjin Kwon\").format('csv').load(d).show()\n",
            " |      +---+----+\n",
            " |      |age|name|\n",
            " |      +---+----+\n",
            " |      |100|NULL|\n",
            " |      +---+----+\n",
            " |  \n",
            " |  options(self, **options: 'OptionalPrimitiveType') -> 'DataFrameReader'\n",
            " |      Adds input options for the underlying data source.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      **options : dict\n",
            " |          The dictionary of string keys and prmitive-type values.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> spark.read.option(\"key\", \"value\")\n",
            " |      <...readwriter.DataFrameReader object ...>\n",
            " |      \n",
            " |      Specify the option 'nullValue' and 'header' with reading a CSV file.\n",
            " |      \n",
            " |      >>> import tempfile\n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     # Write a DataFrame into a CSV file with a header.\n",
            " |      ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n",
            " |      ...     df.write.option(\"header\", True).mode(\"overwrite\").format(\"csv\").save(d)\n",
            " |      ...\n",
            " |      ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon',\n",
            " |      ...     # and 'header' option set to `True`.\n",
            " |      ...     spark.read.options(\n",
            " |      ...         nullValue=\"Hyukjin Kwon\",\n",
            " |      ...         header=True\n",
            " |      ...     ).format('csv').load(d).show()\n",
            " |      +---+----+\n",
            " |      |age|name|\n",
            " |      +---+----+\n",
            " |      |100|NULL|\n",
            " |      +---+----+\n",
            " |  \n",
            " |  orc(self, path: Union[str, List[str]], mergeSchema: Optional[bool] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None) -> 'DataFrame'\n",
            " |      Loads ORC files, returning the result as a :class:`DataFrame`.\n",
            " |      \n",
            " |      .. versionadded:: 1.5.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      path : str or list\n",
            " |      \n",
            " |      Other Parameters\n",
            " |      ----------------\n",
            " |      Extra options\n",
            " |          For the extra options, refer to\n",
            " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-orc.html#data-source-option>`_\n",
            " |          for the version you use.\n",
            " |      \n",
            " |          .. # noqa\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      Write a DataFrame into a ORC file and read it back.\n",
            " |      \n",
            " |      >>> import tempfile\n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     # Write a DataFrame into a ORC file\n",
            " |      ...     spark.createDataFrame(\n",
            " |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
            " |      ...     ).write.mode(\"overwrite\").format(\"orc\").save(d)\n",
            " |      ...\n",
            " |      ...     # Read the Parquet file as a DataFrame.\n",
            " |      ...     spark.read.orc(d).show()\n",
            " |      +---+------------+\n",
            " |      |age|        name|\n",
            " |      +---+------------+\n",
            " |      |100|Hyukjin Kwon|\n",
            " |      +---+------------+\n",
            " |  \n",
            " |  parquet(self, *paths: str, **options: 'OptionalPrimitiveType') -> 'DataFrame'\n",
            " |      Loads Parquet files, returning the result as a :class:`DataFrame`.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      paths : str\n",
            " |      \n",
            " |      Other Parameters\n",
            " |      ----------------\n",
            " |      **options\n",
            " |          For the extra options, refer to\n",
            " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#data-source-option>`_\n",
            " |          for the version you use.\n",
            " |      \n",
            " |          .. # noqa\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      Write a DataFrame into a Parquet file and read it back.\n",
            " |      \n",
            " |      >>> import tempfile\n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     # Write a DataFrame into a Parquet file\n",
            " |      ...     spark.createDataFrame(\n",
            " |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
            " |      ...     ).write.mode(\"overwrite\").format(\"parquet\").save(d)\n",
            " |      ...\n",
            " |      ...     # Read the Parquet file as a DataFrame.\n",
            " |      ...     spark.read.parquet(d).show()\n",
            " |      +---+------------+\n",
            " |      |age|        name|\n",
            " |      +---+------------+\n",
            " |      |100|Hyukjin Kwon|\n",
            " |      +---+------------+\n",
            " |  \n",
            " |  schema(self, schema: Union[pyspark.sql.types.StructType, str]) -> 'DataFrameReader'\n",
            " |      Specifies the input schema.\n",
            " |      \n",
            " |      Some data sources (e.g. JSON) can infer the input schema automatically from data.\n",
            " |      By specifying the schema here, the underlying data source can skip the schema\n",
            " |      inference step, and thus speed up data loading.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      schema : :class:`pyspark.sql.types.StructType` or str\n",
            " |          a :class:`pyspark.sql.types.StructType` object or a DDL-formatted string\n",
            " |          (For example ``col0 INT, col1 DOUBLE``).\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> spark.read.schema(\"col0 INT, col1 DOUBLE\")\n",
            " |      <...readwriter.DataFrameReader object ...>\n",
            " |      \n",
            " |      Specify the schema with reading a CSV file.\n",
            " |      \n",
            " |      >>> import tempfile\n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     spark.read.schema(\"col0 INT, col1 DOUBLE\").format(\"csv\").load(d).printSchema()\n",
            " |      root\n",
            " |       |-- col0: integer (nullable = true)\n",
            " |       |-- col1: double (nullable = true)\n",
            " |  \n",
            " |  table(self, tableName: str) -> 'DataFrame'\n",
            " |      Returns the specified table as a :class:`DataFrame`.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      tableName : str\n",
            " |          string, name of the table.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.range(10)\n",
            " |      >>> df.createOrReplaceTempView('tblA')\n",
            " |      >>> spark.read.table('tblA').show()\n",
            " |      +---+\n",
            " |      | id|\n",
            " |      +---+\n",
            " |      |  0|\n",
            " |      |  1|\n",
            " |      |  2|\n",
            " |      |  3|\n",
            " |      |  4|\n",
            " |      |  5|\n",
            " |      |  6|\n",
            " |      |  7|\n",
            " |      |  8|\n",
            " |      |  9|\n",
            " |      +---+\n",
            " |      >>> _ = spark.sql(\"DROP TABLE tblA\")\n",
            " |  \n",
            " |  text(self, paths: Union[str, List[str]], wholetext: bool = False, lineSep: Optional[str] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None) -> 'DataFrame'\n",
            " |      Loads text files and returns a :class:`DataFrame` whose schema starts with a\n",
            " |      string column named \"value\", and followed by partitioned columns if there\n",
            " |      are any.\n",
            " |      The text files must be encoded as UTF-8.\n",
            " |      \n",
            " |      By default, each line in the text file is a new row in the resulting DataFrame.\n",
            " |      \n",
            " |      .. versionadded:: 1.6.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      paths : str or list\n",
            " |          string, or list of strings, for input path(s).\n",
            " |      \n",
            " |      Other Parameters\n",
            " |      ----------------\n",
            " |      Extra options\n",
            " |          For the extra options, refer to\n",
            " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-text.html#data-source-option>`_\n",
            " |          for the version you use.\n",
            " |      \n",
            " |          .. # noqa\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      Write a DataFrame into a text file and read it back.\n",
            " |      \n",
            " |      >>> import tempfile\n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     # Write a DataFrame into a text file\n",
            " |      ...     df = spark.createDataFrame([(\"a\",), (\"b\",), (\"c\",)], schema=[\"alphabets\"])\n",
            " |      ...     df.write.mode(\"overwrite\").format(\"text\").save(d)\n",
            " |      ...\n",
            " |      ...     # Read the text file as a DataFrame.\n",
            " |      ...     spark.read.schema(df.schema).text(d).sort(\"alphabets\").show()\n",
            " |      +---------+\n",
            " |      |alphabets|\n",
            " |      +---------+\n",
            " |      |        a|\n",
            " |      |        b|\n",
            " |      |        c|\n",
            " |      +---------+\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from OptionUtils:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "source": [
        "df = pd.read_csv('/content/results of dk 777 on bundle inputs  1.csv')"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "7K5adsCHotL1",
        "outputId": "555a298a-2498-49d8-af09-77c76b9d67f5"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-11f944ec08f5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/results of dk 777 on bundle inputs  1.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install pandas==1.5.3\n",
        "import pandas as pd"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "Ych6qvVmo0bP",
        "outputId": "aae70415-6ef8-4a67-c2ef-6f534e56e1eb"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas==1.5.3\n",
            "  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3) (2023.4)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas==1.5.3) (1.16.0)\n",
            "Installing collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.0.3\n",
            "    Uninstalling pandas-2.0.3:\n",
            "      Successfully uninstalled pandas-2.0.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pandas<2.2.2dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.0.3, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-1.5.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              },
              "id": "d6bbcb3710e5457e867e48f98644e0d7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"BostonHousePrices\").getOrCreate()\n",
        "\n",
        "# Read the CSV file using the correct path for Colab\n",
        "file_path = \"/content/results of dk 777 on bundle inputs  1.csv\"\n",
        "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5sCCydjtiT6",
        "outputId": "6cdde1a2-42d9-4271-cc76-0e23dec22bb7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+-----+-----+------+----+------+---------+-----+------+-------+------+------+------+------+\n",
            "| 1834|-21680|-7866|40431|DK 777|  kg|0.0000|7085.0000|0.000|1.0009|1.00010|NULL11|NULL12|NULL13|NULL14|\n",
            "+-----+------+-----+-----+------+----+------+---------+-----+------+-------+------+------+------+------+\n",
            "| 1834|-17092|-5863|40431|DK 777|  kg|   0.0|  13200.0|  0.0|   2.0|    1.0|  NULL|  NULL|  NULL|  NULL|\n",
            "| 2404|-11326|-2675|40431|DK 777|unit|   0.0|    510.0|  0.0|   2.0|    1.0|  NULL|  NULL|  NULL|  NULL|\n",
            "| 2834|-21680|-7866|40431|DK 777|  kg|   0.0|   7085.0|  0.0|   1.0|    1.0|  NULL|  NULL|  NULL|  NULL|\n",
            "| 2834|-17092|-5863|40431|DK 777|  kg|   0.0|  13200.0|  0.0|   2.0|    1.0|  NULL|  NULL|  NULL|  NULL|\n",
            "| 3834|-21680|-7866|40431|DK 777|  kg|   0.0|   7085.0|  0.0|   1.0|    1.0|  NULL|  NULL|  NULL|  NULL|\n",
            "| 3834|-17092|-5863|40431|DK 777|  kg|   0.0|  13200.0|  0.0|   2.0|    1.0|  NULL|  NULL|  NULL|  NULL|\n",
            "| 4834|-21680|-7866|40431|DK 777|  kg|   0.0|   7085.0|  0.0|   1.0|    1.0|  NULL|  NULL|  NULL|  NULL|\n",
            "| 4834|-17092|-5863|40431|DK 777|  kg|   0.0|  13200.0|  0.0|   2.0|    1.0|  NULL|  NULL|  NULL|  NULL|\n",
            "| 5834|-21680|-7866|40431|DK 777|  kg|   0.0|   7085.0|  0.0|   1.0|    1.0|  NULL|  NULL|  NULL|  NULL|\n",
            "| 5834|-17092|-5863|40431|DK 777|  kg|   0.0|  13200.0|  0.0|   2.0|    1.0|  NULL|  NULL|  NULL|  NULL|\n",
            "| 6834|-21680|-7866|40431|DK 777|  kg|   0.0|   7085.0|  0.0|   1.0|    1.0|  NULL|  NULL|  NULL|  NULL|\n",
            "| 6834|-17092|-5863|40431|DK 777|  kg|   0.0|  13200.0|  0.0|   2.0|    1.0|  NULL|  NULL|  NULL|  NULL|\n",
            "| 7834|-21680|-7866|40431|DK 777|  kg|   0.0|   7085.0|  0.0|   1.0|    1.0|  NULL|  NULL|  NULL|  NULL|\n",
            "| 7834|-17092|-5863|40431|DK 777|  kg|   0.0|  13200.0|  0.0|   2.0|    1.0|  NULL|  NULL|  NULL|  NULL|\n",
            "| 8834|-21680|-7866|40431|DK 777|  kg|   0.0|   7085.0|  0.0|   1.0|    1.0|  NULL|  NULL|  NULL|  NULL|\n",
            "| 8834|-17092|-5863|40431|DK 777|  kg|   0.0|  13200.0|  0.0|   2.0|    1.0|  NULL|  NULL|  NULL|  NULL|\n",
            "| 9834|-21680|-7866|40431|DK 777|  kg|   0.0|   6540.0|  0.0|   1.0|    1.0|  NULL|  NULL|  NULL|  NULL|\n",
            "| 9834|-17092|-5863|40431|DK 777|  kg|   0.0|  13200.0|  0.0|   2.0|    1.0|  NULL|  NULL|  NULL|  NULL|\n",
            "|10834|-21680|-7866|40431|DK 777|  kg|   0.0|   6540.0|  0.0|   1.0|    1.0|  NULL|  NULL|  NULL|  NULL|\n",
            "|10834|-17092|-5863|40431|DK 777|  kg|   0.0|  13200.0|  0.0|   2.0|    1.0|  NULL|  NULL|  NULL|  NULL|\n",
            "+-----+------+-----+-----+------+----+------+---------+-----+------+-------+------+------+------+------+\n",
            "only showing top 20 rows\n",
            "\n",
            "root\n",
            " |-- 1834: integer (nullable = true)\n",
            " |-- -21680: integer (nullable = true)\n",
            " |-- -7866: integer (nullable = true)\n",
            " |-- 40431: integer (nullable = true)\n",
            " |-- DK 777: string (nullable = true)\n",
            " |-- kg: string (nullable = true)\n",
            " |-- 0.0000: double (nullable = true)\n",
            " |-- 7085.0000: double (nullable = true)\n",
            " |-- 0.000: double (nullable = true)\n",
            " |-- 1.0009: double (nullable = true)\n",
            " |-- 1.00010: double (nullable = true)\n",
            " |-- NULL11: string (nullable = true)\n",
            " |-- NULL12: string (nullable = true)\n",
            " |-- NULL13: string (nullable = true)\n",
            " |-- NULL14: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "source": [
        "df = spark.read.csv(path='/content/population_total.csv', header=True, inferSchema=True)\n",
        "display(df)\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "CVxkh0RSpJQ1",
        "outputId": "8af6016b-804e-49be-c3a8-cef88e52616c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[country: string, year: int, population: int]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+----------+\n",
            "|country|year|population|\n",
            "+-------+----+----------+\n",
            "|  China|2020|1439323776|\n",
            "|  China|2019|1433783686|\n",
            "|  China|2018|1427647786|\n",
            "|  China|2017|1421021791|\n",
            "|  China|2016|1414049351|\n",
            "|  China|2015|1406847870|\n",
            "|  China|2010|1368810615|\n",
            "|  China|2005|1330776380|\n",
            "|  China|2000|1290550765|\n",
            "|  China|1995|1240920535|\n",
            "|  China|1990|1176883674|\n",
            "|  China|1985|1075589361|\n",
            "|  China|1980|1000089235|\n",
            "|  China|1975| 926240885|\n",
            "|  China|1970| 827601394|\n",
            "|  China|1965| 724218968|\n",
            "|  China|1960| 660408056|\n",
            "|  China|1955| 612241554|\n",
            "| France|2020|  65273511|\n",
            "| France|2019|  65129728|\n",
            "+-------+----+----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "root\n",
            " |-- country: string (nullable = true)\n",
            " |-- year: integer (nullable = true)\n",
            " |-- population: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"/gdp.csv\").getOrCreate()\n",
        "\n",
        "# Read the CSV file using the correct path for Colab\n",
        "file_path = \"/gdp.csv\"\n",
        "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "metadata": {
        "id": "HVnAzaxZxg1l",
        "outputId": "01e1c05d-8a8f-4d9a-abb7-1dcaf3d2786f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pyspark'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-225e3fe9a528>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Initialize SparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/gdp.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "source": [
        "!pip install pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"/gdp.csv\").getOrCreate()\n",
        "\n",
        "# Read the CSV file using the correct path for Colab\n",
        "file_path = \"/gdp.csv\"\n",
        "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "a9pcj28Ax-YY",
        "outputId": "b53b305f-00bc-4bef-d477-e95d017c6863",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=5230cc555a81c6d0eeff0f8ecf7e4c4066dc1c987a16471d7d51fa24222d1215\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n",
            "+----+----+-----------+----+---------+---------+-----------+------------+\n",
            "|unid|wbid|    country|year|      SES|    gdppc|    yrseduc|    popshare|\n",
            "+----+----+-----------+----+---------+---------+-----------+------------+\n",
            "|   4| AFG|Afghanistan|1970|3.4742124|    709.0|       NULL|0.0030974303|\n",
            "|   4| AFG|Afghanistan|1920|26.968016|731.75677|       NULL|0.0032446014|\n",
            "|   4| AFG|Afghanistan|1990|1.2695296|    604.0|       NULL| 0.002346751|\n",
            "|   4| AFG|Afghanistan|1960|15.763076|    739.0|       NULL| 0.003039381|\n",
            "|   4| AFG|Afghanistan|2000|2.0611143|    565.0|       NULL|0.0033088236|\n",
            "|   4| AFG|Afghanistan|2010|5.6763997|1662.8035|       NULL|0.0041496116|\n",
            "|   4| AFG|Afghanistan|1880|37.957447|585.46509|       NULL|0.0032711735|\n",
            "|   4| AFG|Afghanistan|1940|14.320977|673.91895|       NULL|0.0032264683|\n",
            "|   4| AFG|Afghanistan|1890|29.591391|635.93024|       NULL|0.0032554974|\n",
            "|   4| AFG|Afghanistan|1980|3.4650476|    690.0|       NULL|0.0030573066|\n",
            "|   4| AFG|Afghanistan|1930|15.306766|702.83783|       NULL|0.0032587289|\n",
            "|   4| AFG|Afghanistan|1950|23.424145|    645.0|       NULL|0.0033020985|\n",
            "|   4| AFG|Afghanistan|1900|28.104797|686.39532|       NULL|0.0032445015|\n",
            "|   4| AFG|Afghanistan|1910|  26.9876|736.86047|       NULL|0.0031784344|\n",
            "|  24| AGO|     Angola|2010|21.247763|6492.1768|       2.79|0.0031490561|\n",
            "|  24| AGO|     Angola|1930|24.175644|747.81482|0.079999998| 0.001587122|\n",
            "|  24| AGO|     Angola|1920|24.223469|682.62964|0.079999998|0.0015237019|\n",
            "|  24| AGO|     Angola|1970|30.052431|   1768.0| 0.28999999|0.0017549359|\n",
            "|  24| AGO|     Angola|1960|27.918711|   1253.0|       0.11|0.0017810419|\n",
            "|  24| AGO|     Angola|1950| 28.05554|   1052.0|0.090000004| 0.001668241|\n",
            "+----+----+-----------+----+---------+---------+-----------+------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "root\n",
            " |-- unid: integer (nullable = true)\n",
            " |-- wbid: string (nullable = true)\n",
            " |-- country: string (nullable = true)\n",
            " |-- year: integer (nullable = true)\n",
            " |-- SES: double (nullable = true)\n",
            " |-- gdppc: double (nullable = true)\n",
            " |-- yrseduc: double (nullable = true)\n",
            " |-- popshare: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}